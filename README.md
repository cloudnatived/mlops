


1. å®‰è£…æ“ä½œç³»ç»Ÿ
2. å®‰è£…GPUé©±åŠ¨ï¼ŒV100çš„GPUé©±åŠ¨ã€CUDA ToolKit
3. å®‰è£…dockerã€containerdã€nvidia-container-toolkit
4. é…ç½® DCGM + Prometheus + Grafana çš„ GPU ç›‘æ§æ–¹æ¡ˆ
5. å¼€ç®±å³ç”¨çš„æ¨ç†æœåŠ¡:Ollamaä¸Dify
6. åœ¨å•æœº2ä¸ª V100-PCIE-16GB çš„ GPUæœåŠ¡å™¨ä¸Š ï¼Œéƒ¨ç½²æ¨¡å‹è¿›è¡Œæ¨ç†
7. åœ¨å¤–ç½‘è®¿é—®æ‰€éƒ¨ç½²çš„æ¨¡å‹æ¨ç†æœåŠ¡
8. åˆ†å¸ƒå¼è®¡ç®—é›†ç¾¤ Ray
9. é«˜æ€§èƒ½æ¨ç†å¼•æ“ vLLM å•æœºéƒ¨ç½²
10. ä½¿ç”¨ vllm + ray é›†ç¾¤ï¼Œè¿›è¡Œå¤šæœºå¤šå¡çš„éƒ¨ç½²æµ‹è¯•
11. åˆ†å¸ƒå¼è®­ç»ƒä¸é›†ç¾¤é€šä¿¡ï¼Œæ£€æŸ¥å’Œæµ‹è¯• GPU çš„ nccl é€šä¿¡
11.1 ä½¿ç”¨ nccl-tests é¡¹ç›®æµ‹è¯• NCCL åŸºç¡€åŠŸèƒ½
11.2 ä½¿ç”¨ python çš„ torch.distributed åº“æµ‹è¯• NCCL åŸºç¡€åŠŸèƒ½
12. V100-PCIE-16GBä¸Šï¼ŒResNet-152å¯¹CIFAR-10æ•°æ®é›†è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ
13. V100-PCIE-16GBä¸Šï¼Œä½¿ç”¨ BERT æ¨¡å‹å¯¹ bert-base-uncased æ•°æ®é›†è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ
14. triton-inference-serveréƒ¨ç½²
15. Milvus
16. Weaviate
17. NVIDIA Nsight Compute å’Œ NVIDIA Nsight Systems



```


1.å®‰è£…æ“ä½œç³»ç»Ÿ
åœ¨äº‘ä¸»æœºä¸Šé…ç½®V100GPU

åä¸ºçš„å®˜æ–¹æ–‡æ¡£æç¤ºï¼Œåœ¨FushComputerä¸Šï¼Œå®‰è£…å‡†å¤‡ç”¨äºGPUçš„äº‘ä¸»æœºæ—¶éœ€è¦æ³¨æ„ã€‚3ç‚¹ã€‚
å¿…å¤‡äº‹é¡¹
å‰ææ¡ä»¶
1. è™šæ‹Ÿæœºçš„å†…å­˜å…¨éƒ¨é¢„ç•™ã€‚
2. è™šæ‹Ÿæœºå·²ç»‘å®šPCIè®¾å¤‡æ‰€åœ¨çš„ä¸»æœºã€‚
3. è™šæ‹Ÿæœºçš„çŠ¶æ€æ˜¾ç¤ºâ€œå·²åœæ­¢â€ã€‚

# å› ä¸ºç›®å‰è¿™å¥—åä¸ºç§æœ‰äº‘ç³»ç»ŸCPUå’Œå†…å­˜èµ„æºç´§å¼ ï¼Œç›®å‰é€‰æ‹©çš„äº‘ä¸»æœºçš„é…ç½®:
CPU:8C
MEMORY:32G
DISK:500G

é›†ç¾¤ç¡¬ä»¶é…ç½®æ¦‚è§ˆ:
æœåŠ¡å™¨èŠ‚ç‚¹: 3å°
IPåœ°å€: 172.18.8.208, 172.18.8.209, 172.18.8.210
GPU: æ¯å°2 x V100-PCIE-16GB (ç®—åŠ› 7.0)
CPU/å†…å­˜: 8æ ¸ / 32GB
ç½‘ç»œ: 10G

GPUé›†ç¾¤ï¼Œé›†ç¾¤çš„èµ„æºä¸ºï¼Œ3å°GPUæœåŠ¡å™¨ã€‚æ¯ä¸ªGPUæœåŠ¡å™¨é…ç½®ä¸º2ä¸ªV100-PCIE-16GBï¼ŒæœåŠ¡å™¨ä¹‹é—´çš„ç½‘ç»œä¸º10Gã€‚
é›†ç¾¤èµ„æºé…ç½®åˆ†æ:
ç¡¬ä»¶é…ç½®:
    3å°æœåŠ¡å™¨ Ã— 2ä¸ªV100-PCIE-16GB = 6ä¸ªGPU
    æ¯ä¸ªGPU 16GBæ˜¾å­˜
    æœåŠ¡å™¨é—´10Gbç½‘ç»œè¿æ¥
ç†è®ºè®¡ç®—èƒ½åŠ›:
    æ€»æ˜¾å­˜:6 Ã— 16GB = 96GB
    æ€»è®¡ç®—å•å…ƒ:6 Ã— V100
    ç½‘ç»œå¸¦å®½:10Gbpsï¼ˆçº¦1.25GB/sï¼‰

V100 16GBæ˜¾å­˜é™åˆ¶äº†å•å¡å¯è®­ç»ƒæ¨¡å‹å¤§å°
10Gç½‘ç»œå¸¦å®½åœ¨å¤§è§„æ¨¡åˆ†å¸ƒå¼è®­ç»ƒä¸­å¯èƒ½æˆä¸ºç“¶é¢ˆ
æ€»è®¡6å—GPUçš„è§„æ¨¡é€‚ä¸­ï¼Œä¸é€‚åˆè¶…å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒ


æ¯ä¸ªäº‘ä¸»æœºä¸Šæœ‰2ä¸ªV100-PCIE-16GBçš„GPU
Tesla V100ï¼ˆç®—åŠ› 7.0ï¼‰

# ç”¨äºGPUçš„äº‘ä¸»æœºå®‰è£…çš„æ“ä½œç³»ç»Ÿã€‚
ubuntu-22.04.5-live-server-amd64.iso

ä¸ºä»€ä¹ˆè¦ä½¿ç”¨è¿™ä¸ªç‰ˆæœ¬çš„æ“ä½œç³»ç»Ÿï¼Ÿ
å› ä¸ºï¼ŒV100-PCIE-16GBï¼ŒTesla V100ï¼ˆç®—åŠ› 7.0ï¼‰,å¯¹åº”çš„é©±åŠ¨å’ŒCUDAçš„ç‰ˆæœ¬æ˜¯:cuda_11.8.0_520.61.05_linux.run
https://developer.nvidia.com/cuda-11-8-0-download-archive
wget https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda_11.8.0_520.61.05_linux.run

# å®‰è£…å®Œæ“ä½œç³»ç»Ÿåï¼Œé…ç½® IP ã€‚
ip link set enp4s1 up
ip addr add 172.18.8.209/24 dev enp4s1
ip route add default via 172.18.8.1

# å†™å…¥ç½‘å¡é…ç½®æ–‡ä»¶ã€‚
cat > /etc/netplan/50-cloud-init.yaml <<EOF
# network: {config: disabled}
network:
  ethernets:
    enp4s1:
      dhcp4: false
      addresses:
        - 172.18.8.208/24
      routes:
        - to: default
          via: 172.18.8.1
      nameservers:
         addresses: [8.8.8.8, 168.95.1.1]
  version: 2
EOF

# ä½¿ç”¨ netplan åº”ç”¨ç½‘ç»œé…ç½®ã€‚
netplan apply

# ä½¿ç”¨æ¸…åå¤§å­¦çš„ ubuntu-22.04.5-live-server-amd64.iso çš„æº
cp /etc/apt/sources.list /etc/apt/sources.list.original;
cat > /etc/apt/sources.list <<EOF
# é»˜è®¤æ³¨é‡Šäº†æºç é•œåƒä»¥æé«˜ apt update é€Ÿåº¦ï¼Œå¦‚æœ‰éœ€è¦å¯è‡ªè¡Œå–æ¶ˆæ³¨é‡Š
deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy main restricted universe multiverse
# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy main restricted universe multiverse
deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-updates main restricted universe multiverse
# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-updates main restricted universe multiverse
deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-backports main restricted universe multiverse
# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-backports main restricted universe multiverse
deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-security main restricted universe multiverse
# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-security main restricted universe multiverse

# é¢„å‘å¸ƒè½¯ä»¶æºï¼Œä¸å»ºè®®å¯ç”¨
# deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-proposed main restricted universe multiverse
# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-proposed main restricted universe multiverse
EOF

+++++++++++++++++++++++++++++++++++   æ³¨æ„ï¼Œè¿™æ®µæ˜¯è®¾ç½®ubuntu-24.04.3çš„æº
# ä½¿ç”¨æ¸…åå¤§å­¦çš„ ubuntu-24.04.3 çš„æº
cat > /etc/apt/sources.list.d/ubuntu.sources <<EOF
Types: deb
URIs: https://mirrors.tuna.tsinghua.edu.cn/ubuntu/
Suites: noble noble-updates noble-security
Components: main restricted universe multiverse
Signed-By: /usr/share/keyrings/ubuntu-archive-keyring.gpg
EOF
+++++++++++++++++++++++++++++++++++

# ç›®å‰å†…æ ¸ç‰ˆæœ¬å·
root@x:~# uname -a
Linux x 6.8.0-71-generic #71-Ubuntu SMP PREEMPT_DYNAMIC Tue Jul 22 16:52:38 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux

# å‡çº§ä¹‹åå°†å˜æˆçš„ç‰ˆæœ¬å·ã€‚
root@x:~# uname -a
Linux x 6.8.0-78-generic #78-Ubuntu SMP PREEMPT_DYNAMIC Tue Aug 12 11:34:18 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux

# ubuntu-22.04.5-live-server-amd64.iso é»˜è®¤æ“ä½œç³»ç»Ÿç‰ˆæœ¬ä¿¡æ¯ã€‚
root@x:/etc/netplan# cat /etc/*releas*
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=22.04
DISTRIB_CODENAME=jammy
DISTRIB_DESCRIPTION="Ubuntu 22.04.5 LTS"
PRETTY_NAME="Ubuntu 22.04.5 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.5 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy

# å†™å…¥å›ºå®šé…ç½®çš„ resolv é…ç½®æ–‡ä»¶
rm -rf /etc/resolv.conf

cat > /etc/resolv.conf <<EOF
nameserver 8.8.8.8
nameserver 168.95.1.1
EOF

# æ›´æ–°æº
# å¯èƒ½ä¼šæ›´æ–°å†…æ ¸ã€‚æ³¨æ„ï¼Œå®‰è£…å®Œ nvidia çš„ GPU é©±åŠ¨ä¹‹åï¼Œä¸è¦å‡çº§å†…æ ¸ï¼Œå¦åˆ™éœ€è¦é‡æ–°å®‰è£… nvidia çš„ GPU é©±åŠ¨ã€‚
apt update -y;
apt list --upgradable;
apt upgrade -y;

# è¿è¡Œåœ¨init 3
systemctl isolate multi-user.target;
systemctl isolate runlevel3.target;
ln -sf /lib/systemd/system/multi-user.target /etc/systemd/system/default.target;
systemctl set-default multi-user.target;

#å…³é—­ä¸éœ€è¦çš„æœåŠ¡:
systemctl list-unit-files |awk '{ print $1,$2 }'|grep enable|egrep -v "ssh|multi|systemd-resolved|wpa_" |awk '{ print $1}'|xargs -i systemctl disable {};

#ç¡®è®¤æœåŠ¡å·²å…³é—­:
systemctl list-unit-files |awk '{print $1,$2}'|grep enabled;

uname -r         # æŸ¥çœ‹å†…æ ¸ç‰ˆæœ¬
lsb_release -a   # æŸ¥çœ‹å‘è¡Œç‰ˆä¿¡æ¯ï¼ˆUbuntu/Debian/CentOSç­‰ï¼‰

# ç¡®è®¤å®‰è£…äº† gcc make
apt install -y gcc make g++ net-tools

# é‡å¯ä¸€æ¬¡ã€‚
reboot

# ç¦ç”¨å¼€æº Nouveau é©±åŠ¨ï¼ˆé‡è¦ï¼Œå¦åˆ™å®‰è£…é©±åŠ¨å’Œcudaçš„è¿‡ç¨‹ä¼šå¤±è´¥ï¼‰
sudo bash -c 'echo -e "blacklist nouveau\noptions nouveau modeset=0" > /etc/modprobe.d/blacklist-nouveau.conf'

# æ›´æ–°å†…æ ¸ initramfs:ï¼ˆé‡è¦ï¼Œå¦åˆ™å®‰è£…é©±åŠ¨å’Œcudaçš„è¿‡ç¨‹ä¼šå¤±è´¥ï¼‰
sudo update-initramfs -u      # Ubuntu / Debian
# æˆ–
sudo dracut --force           # CentOS / RHEL

# å¦‚æœæ›´æ–°äº†å†…æ ¸å°±è¦æ›´æ–°å†…æ ¸å¤´æ–‡ä»¶ã€‚ï¼ˆé‡è¦ï¼Œå¦åˆ™å®‰è£…é©±åŠ¨å’Œ cuda çš„è¿‡ç¨‹ä¼šå¤±è´¥ï¼‰
apt-get install -y dkms build-essential linux-headers-$(uname -r)

# å†é‡å¯ä¸€æ¬¡ã€‚
reboot # é‡å¯

# é‡å¯åç¡®è®¤ Nouveau ä¸å†è¢«åŠ è½½:
lsmod | grep nouveau   # æ— è¾“å‡ºåˆ™æˆåŠŸ

# æŸ¥çœ‹pciè®¾å¤‡
lspci | grep -i -E "vga|nvidia"
lspci | grep -i -E "vga|nvidia"|awk '{ print $1}'|xargs -i lspci -v -s {}

# ä»å¯åŠ¨ä¿¡æ¯é‡ŒæŸ¥çœ‹
dmesg |grep -i -E "vga|nvidia"

# lspci æ˜¯ä¸€ä¸ªåˆ—å‡ºæ‰€æœ‰ PCI è®¾å¤‡çš„å·¥å…·ï¼ŒåŒ…æ‹¬æ˜¾å¡ã€‚
lspci -k | grep -A 2 -E "(VGA|3D)"

# lshwï¼ˆList Hardwareï¼‰å¯ä»¥åˆ—å‡ºç³»ç»Ÿçš„æ‰€æœ‰ç¡¬ä»¶é…ç½®ã€‚
lshw -C display

# ä½¿ç”¨ ubuntu-drivers devices æŸ¥çœ‹æ¨èé©±åŠ¨ï¼ŒæŸ¥çœ‹ç³»ç»Ÿæ¨èçš„æ˜¾å¡é©±åŠ¨:
ubuntu-drivers devices

# ä½¿ç”¨ modinfo æŸ¥çœ‹åŠ è½½çš„é©±åŠ¨æ¨¡å—ï¼ŒæŸ¥çœ‹å½“å‰åŠ è½½çš„æ˜¾å¡é©±åŠ¨æ¨¡å—ã€‚ï¼ˆå®‰è£…å®ŒGPUé©±åŠ¨åæ‰å¯æŸ¥çœ‹ï¼‰
modinfo nvidia







2.å®‰è£…GPUé©±åŠ¨ï¼ŒV100çš„GPUé©±åŠ¨ã€CUDA ToolKit

# Ubuntu 22.04 è‡ªå¸¦ä»“åº“å·²åŒ…å« 525/535 ç­‰æ–°ç‰ˆæœ¬
# 525+ å·²é€‚é… 5.15 å†…æ ¸ï¼Œä¸å†ä½¿ç”¨ GPL-only ç¬¦å·ï¼Œæ— éœ€è‡ªå·±ç¼–è¯‘ã€‚
sudo apt update
sudo apt install -y nvidia-driver-525   # æˆ– 535/545

# å®‰è£…å®Œé©±åŠ¨ä¹‹åçš„æç¤º:
root@x:/Data/IMAGES# ./cuda_11.8.0_520.61.05_linux.run 
===========
= Summary =
===========

Driver:   Not Selected
Toolkit:  Installed in /usr/local/cuda-11.8/

Please make sure that
 -   PATH includes /usr/local/cuda-11.8/bin
 -   LD_LIBRARY_PATH includes /usr/local/cuda-11.8/lib64, or, add /usr/local/cuda-11.8/lib64 to /etc/ld.so.conf and run ldconfig as root

To uninstall the CUDA Toolkit, run cuda-uninstaller in /usr/local/cuda-11.8/bin
***WARNING: Incomplete installation! This installation did not install the CUDA Driver. A driver of version at least 520.00 is required for CUDA 11.8 functionality to work.
To install the driver using this installer, run the following command, replacing <CudaInstaller> with the name of this run file:
    sudo <CudaInstaller>.run --silent --driver

Logfile is /var/log/cuda-installer.log


# å¦‚æœéœ€è¦å¸è½½é©±åŠ¨ CUDA Toolkit 11.8 
nvidia-uninstall 


# é€‰æ‹©runæ–‡ä»¶è¿›è¡Œå®‰è£…
# https://developer.nvidia.com/cuda-toolkit-archive

cuda_11.8.0_520.61.05_linux.run

â”Œâ”€â”
â”‚ CUDA Installer se Agreement                                                  â”‚
â”‚ - [ ] Driver                                                                 â”‚
â”‚      [ ] 520.61.05                                                           â”‚
â”‚ + [X] CUDA Toolkit 11.8                                                      â”‚
â”‚   [X] CUDA Demo Suite 11.8                                                   â”‚
â”‚   [X] CUDA Documentation 11.8                                                â”‚
â”‚ - [ ] Kernel Objects                                                         â”‚
â”‚      [ ] nvidia-fs                                                           â”‚
â”‚   Options                                                                    â”‚
â”‚   Install                                                                    â”‚
â”‚                                                                              â”‚
â”‚                                                                              â”‚
â”‚                                                                              â”‚
â”‚                                                                              â”‚
â”‚                                                                              â”‚
â”‚                                                                              â”‚
â”‚                                                                              â”‚
â”‚   reface                                                                     â”‚
â”‚                                                                              â”‚
â”‚                                                                              â”‚

# ç¯å¢ƒå˜é‡ã€‚ç›®å‰æ·»åŠ çš„ç¯å¢ƒå˜é‡ã€‚ä¹‹åå¯èƒ½éœ€è¦æ·»åŠ  nccl ï¼Œä»¥åŠå…¶å®ƒçš„å˜é‡
cat >> /etc/profile <<EOF
export PATH=/usr/local/cuda/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}
export LIBRARY_PATH=$LIBRARY_PATH:/usr/local/cuda/lib64
EOF

source /etc/profile

# æ£€æŸ¥ DKMS çŠ¶æ€
# DKMSï¼ˆDynamic Kernel Module Supportï¼‰ç”¨äºè‡ªåŠ¨ç¼–è¯‘å’Œå®‰è£…å†…æ ¸æ¨¡å—ã€‚æ£€æŸ¥ DKMS æ˜¯å¦å·²æ­£ç¡®é…ç½®:
dkms status

# æ£€æŸ¥å†…æ ¸æ¨¡å—åŠ è½½æƒ…å†µ
# é‡å¯åï¼Œå†æ¬¡æ£€æŸ¥å†…æ ¸æ¨¡å—æ˜¯å¦å·²åŠ è½½:
lsmod | grep -i nvidia
lsof | grep -i nvidia

# æµ‹è¯•é‡æ–°åŠ è½½nvidia æ¨¡å—ï¼Œæ˜¯å¦æŠ¥é”™ã€‚ 
modprobe nvidia

# æŸ¥çœ‹æ˜¾å¡é©±åŠ¨ç‰ˆæœ¬
cat /proc/driver/nvidia/version

# æŸ¥çœ‹ CUDAã€cuDNN ç‰ˆæœ¬
cat /usr/local/cuda/version.json

# æŸ¥çœ‹nvccç‰ˆæœ¬
/usr/local/cuda/bin/nvcc -V

# æŸ¥çœ‹å·²å®‰è£…çš„cuda-toolkitä¿¡æ¯ã€‚
apt list |grep cuda-toolkit

NVIDIA Nsight Computeï¼Œcuda-toolkit é™„å¸¦çš„å·¥å…·ã€‚
# æŸ¥çœ‹Nsight-Computeæ”¯æŒçš„sections
ncu --list-sections

NVIDIA Nsight Systemsï¼Œcuda-toolkit é™„å¸¦çš„å·¥å…·ã€‚
# åˆ—å‡ºå½“å‰æ‰€æœ‰æ´»åŠ¨çš„æ€§èƒ½åˆ†æä¼šè¯ã€‚
nsys sessions list

å¼€å§‹ä½¿ç”¨ nvidia ç³»ç»Ÿå·¥å…·
# ä½¿ç”¨ nvidia-smi
nvidia-smi
nvidia-smi -L # åˆ—å‡ºç³»ç»Ÿä¸­æ‰€æœ‰å¯ç”¨çš„ GPU è®¾å¤‡åŠå…¶ UUIDã€‚

nvidia-smi topo --matrix  # æŸ¥çœ‹ GPU ä¸ç³»ç»Ÿå…¶ä»–è®¾å¤‡çš„è¿æ¥æ‹“æ‰‘ã€‚

# æ¯éš”2ç§’åˆ·æ–°ä¸€æ¬¡ï¼Œæ¯æ¬¡åªåœ¨å›ºå®šä½ç½®åˆ·æ–°
watch -n 5 -d nvidia-smi

# å®šæ—¶æŸ¥è¯¢
nvidia-smi -l 5

# éªŒè¯GPUè®¡ç®—èƒ½åŠ›â€‹
# åœ¨å®¿ä¸»æœºæ‰§è¡Œï¼ŒTesla V100ï¼ˆç®—åŠ› 7.0ï¼‰
nvidia-smi --query-gpu=compute_cap --format=csv
# è¾“å‡ºä¸º:
compute_cap
7.0
7.0

# éªŒè¯ GPU P2P æ”¯æŒâ€‹
# åœ¨å®¿ä¸»æœºæ‰§è¡Œ
nvidia-smi topo -m

# è®¾å¤‡ç›‘æ§å‘½ä»¤ï¼Œä»¥æ»šåŠ¨æ¡å½¢å¼æ˜¾ç¤º GPU è®¾å¤‡ç»Ÿè®¡ä¿¡æ¯:
nvidia-smi pmon

# é…ç½® GPU åŠŸè€—ä¸ç®—åŠ›æ¨¡å¼ï¼š
# æŸ¥çœ‹å½“å‰æ¨¡å¼
nvidia-smi -q -d CLOCK,POWER | grep -E "GPU Current Clock|Power Limit"

# æ£€æŸ¥ GPU å¥åº·çŠ¶æ€
nvidia-smi -q -d MEMORY,UTILIZATION,POWER,CLOCK,COMPUTE

# è¿è¡Œç®€å•çš„ CUDA æµ‹è¯•
nvidia-smi --query-gpu=index,name,temperature.gpu,utilization.gpu,memory.used,memory.total --format=csv

# è°ƒæ•´åŠŸè€—ä¸Šé™ï¼ˆéœ€rootï¼Œå•ä½ç“¦ç‰¹ï¼‰
nvidia-smi -pl 250

# é”å®šæœ€é«˜ç®—åŠ›ï¼ˆé’ˆå¯¹V100çš„ç®—åŠ›7.0ï¼‰
nvidia-smi -ac 877,1530  # å†…å­˜é¢‘ç‡,æ ¸å¿ƒé¢‘ç‡

# NVLink å¸¦å®½æµ‹è¯•ä¸ä¼˜åŒ–ï¼šä½¿ç”¨nvidia-smi nvlink --statusæ£€æŸ¥é“¾è·¯çŠ¶æ€ï¼Œé€šè¿‡nccl-testsçš„all_reduce_perféªŒè¯è·¨å¡é€šä¿¡æ•ˆç‡ã€‚
Tesla V100-PCIE-16GB ä¸æ”¯æŒ nvlink

# GPU èµ„æºéš”ç¦»
# ä½¿ç”¨ cgroups é™åˆ¶å®¹å™¨ GPU ä½¿ç”¨ç‡ï¼š
# åˆ›å»º cgroup é™åˆ¶ GPU åˆ©ç”¨ç‡ä¸è¶…è¿‡80%
# é¦–å…ˆè®¾ç½®è®¡ç®—æ¨¡å¼
nvidia-smi -c 0

# ç„¶åè®¾ç½®å†…å­˜æ—¶é’Ÿ
nvidia-smi -lmc 80

# è®¾ç½®æ—¶é’Ÿé¢‘ç‡ï¼ˆéœ€è¦åŒæ—¶æŒ‡å®šæ ¸å¿ƒå’Œå†…å­˜ï¼‰
nvidia-smi -ac 1590,870  # æ ¸å¿ƒ1590MHzï¼Œå†…å­˜870MHz

# åªè®¾ç½®å†…å­˜æ—¶é’Ÿï¼ˆå¦‚æœæ”¯æŒï¼‰
nvidia-smi -lmc 870


# æ­¤æ—¶è¿˜å¹¶æœªå®‰è£… nccl ç›¸å…³å·¥å…·
dpkg -l|grep nccl
hi  libnccl2                        2.15.5-1+cuda11.8                       amd64        NVIDIA Collective Communication Library (NCCL) Runtime

# æ·»åŠ aptæº:
# æ·»åŠ  NVIDIA å®˜æ–¹ä»“åº“
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb
dpkg -i cuda-keyring_1.1-1_all.deb
apt-get update

# ç„¶åæœç´¢æ‰€éœ€è¦çš„è½¯ä»¶åŒ…
apt search nvtx

# Ubuntu/Debian 
apt install -y libnccl-dev libnccl2   
# è§£å†³:../verifiable/verifiable.cu:4:10: fatal error: nccl.h: No such file or directory
    4 | #include <nccl.h>


# æ£€æŸ¥ NCCL åº“æ˜¯å¦å­˜åœ¨
ls /usr/lib/x86_64-linux-gnu/libnccl*  # Ubuntu
ls /usr/local/cuda/lib64/libnccl*      # CUDA ç›®å½•
strings /usr/lib/x86_64-linux-gnu/libnccl.so | grep "NCCL" 

# æµ‹è¯• NCCL åŸºç¡€åŠŸèƒ½ï¼Œè¿™ä¸ªæµ‹è¯•æœ€å¥½ä½¿ç”¨ pytorch:23.10-py3 è¿™ç±»é•œåƒæ¥éƒ¨ç½²ï¼Œå¦åˆ™å®‰è£…åº“ä¾èµ–å¾ˆéº»çƒ¦
git clone https://github.com/NVIDIA/nccl-tests.git
cd nccl-tests
make
./build/all_reduce_perf -b 8 -e 128M -f 2

# è¿è¡Œç®€å•çš„CUDAæµ‹è¯•
python3 -c "
import torch
print('CUDAå¯ç”¨:', torch.cuda.is_available())
if torch.cuda.is_available():
    print('GPUæ•°é‡:', torch.cuda.device_count())
    print('å½“å‰GPU:', torch.cuda.current_device())
    print('GPUåç§°:', torch.cuda.get_device_name(0))
    
    # ç®€å•çš„CUDAæ“ä½œæµ‹è¯•
    x = torch.randn(1000, 1000).cuda()
    y = torch.randn(1000, 1000).cuda()
    z = torch.mm(x, y)
    print('CUDAçŸ©é˜µä¹˜æ³•æµ‹è¯•æˆåŠŸ')
"
CUDAå¯ç”¨: True
GPUæ•°é‡: 2
å½“å‰GPU: 0
GPUåç§°: Tesla V100-PCIE-16GB
CUDAçŸ©é˜µä¹˜æ³•æµ‹è¯•æˆåŠŸ


# æŸ¥çœ‹ nccl-tests æ˜¯ç”¨å“ªä¸ª CUDA ç¼–è¯‘çš„ï¼ˆå¦‚æœçŸ¥é“è·¯å¾„ï¼‰
ldd ./build/all_reduce_perf | grep cuda

# ç”±äºå®˜ç½‘çš„æ”¾åœ¨githubä¸Šï¼Œè®¿é—®å¾ˆæ…¢æ‰€ä»¥è¿™é‡Œä½¿ç”¨å›½å†…çš„å­˜å‚¨åº“ï¼Œä¸­ç§‘å¤§çš„æºï¼Œä½†æ˜¯ä¸­ç§‘å¤§çš„æºï¼Œåªæœ‰ nvidia-container-toolkit çš„åŒ…ï¼Œå¹¶æ²¡æœ‰å…¶å®ƒæ›´å¤šçš„å®‰è£…åŒ…ã€‚ï¼ˆå¦‚æœæ‰§è¡Œäº†ä¸Šä¸€æ­¥ï¼Œå®‰è£…äº†è‹±ä¼Ÿè¾¾çš„æºï¼Œå°±ä¸éœ€è¦å†è£…è¿™ä¸ªæºï¼‰
# è¿™ä¸€æ®µä¸éœ€è¦æ‰§è¡Œã€‚
curl -fsSL https://mirrors.ustc.edu.cn/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
&& curl -s -L https://mirrors.ustc.edu.cn/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
sed 's#deb https://nvidia.github.io#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://mirrors.ustc.edu.cn#g' | \
sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list

# å®‰è£…nvidia-container-toolkitã€‚
# å®‰è£…nvidia-container-toolkitï¼Œæ˜¯éœ€è¦åœ¨å®‰è£…å®Œdockeræˆ–è€…containerdä¹‹åå†æ‰§è¡Œçš„å®‰è£…æ­¥éª¤ã€‚
apt update
apt-get install -y nvidia-container-toolkit

# éªŒè¯å®‰è£…
apt list --installed *nvidia*
nvidia-container-cli  --version

# æ£€æŸ¥ PyTorch å’Œ CUDA ç‰ˆæœ¬åŒ¹é…
# ç¡®ä¿ PyTorch å’Œ CUDA ç‰ˆæœ¬åŒ¹é…ã€‚ä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤æ£€æŸ¥ PyTorch æ˜¯å¦èƒ½æ£€æµ‹åˆ° GPU:
python3 -c "
import torch
print(f'PyTorch version: {torch.__version__}')
print(f'CUDA available: {torch.cuda.is_available()}')
print(f'CUDA version: {torch.version.cuda}')
"

# å®‰è£…python
apt install -y python3-pip python3 python3-netaddr wget git;
apt install -y python3-dev;
pip install --upgrade pip;

# å¦‚ä½•è®¾ç½®é˜¿é‡Œæº
pip config set global.index-url https://mirrors.aliyun.com/pypi/simple
pip config set install.trusted-host mirrors.aliyun.com

# æˆ–è€…è®¾ç½®æ¸…åå¤§å­¦çš„
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple/
pip config set install.trusted-host pypi.tuna.tsinghua.edu.cn

# è§£å†³ubuntu24.04 ä½¿ç”¨pipæ—¶çš„ä¿¡æ¯æé†’:error: externally-managed-environment
mv /usr/lib/python3.12/EXTERNALLY-MANAGED /usr/lib/python3.12/EXTERNALLY-MANAGED.bk

pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple;
mkdir -p /root/.config/pip;
cat > /root/.config/pip/pip.conf <<EOF
[global]
index-url = https://pypi.tuna.tsinghua.edu.cn/simple
EOF

æ‰©å±•çŸ¥è¯†:
----------------------------------------------------
æ‰¹é‡å®‰è£…é©±åŠ¨ã€CUDAã€dockerã€NCCL æµ‹è¯•å·¥å…·ã€‚

----------------------------------------------------





3. å®‰è£… dockerã€containerdã€nvidia-container-toolkit
dockerã€containerdã€nvidia-container-toolkitï¼Œç”¨äºåœ¨GPUæœåŠ¡å™¨ä¸Šå¯åŠ¨å®¹å™¨æ—¶ï¼Œé™„å¸¦--gpus allè¿™ä¸ªå‚æ•°ã€‚
2025-08-24ï¼Œå¯¹äºå®‰è£…dockeræˆ–è€…containerdçš„è¯´æ˜ã€‚ç›®å‰å¤§éƒ¨åˆ†å¼€å‘æ–‡æ¡£ï¼Œæˆ–è€…ä¸€äº›ç®€æ˜“å·¥å…·çš„å‘å¸ƒï¼Œéƒ½ç”¨çš„æ˜¯cdockerè¿™ä¸ªå‘½ä»¤è¡Œå·¥å…·æ¥æ“ä½œã€‚ä½†æ˜¯åœ¨ä¸€äº›ä¼ä¸šçº§éƒ¨ç½²çš„æ—¶å€™ï¼Œç”¨çš„æ˜¯nerdctlè¿™ä¸ªå‘½ä»¤è¡Œå·¥å…·æ“ä½œã€‚
å¤§éƒ¨åˆ†docker composeéƒ¨ç½²ï¼Œéƒ½æ˜¯ç”¨docker composeæ“ä½œçš„ã€‚nerdctlå·¥å…·ï¼Œåœ¨æ“ä½œnerdctl composeæ—¶ï¼Œä¼šç¼ºä¹å‚æ•°ï¼Œæ— æ³•æ‰§è¡ŒæŸäº›æ“ä½œã€‚
æ¯”å¦‚:dify-1.7.2ç‰ˆæœ¬ï¼Œä½¿ç”¨docker composeå¯ä»¥æ“ä½œï¼Œä½†æ˜¯ç”¨nerdctl composeï¼Œæ— æ³•æ“ä½œã€‚

dockerè¿™ä¸ªå‰ç«¯å·¥å…·ï¼Œæ“ä½œcontainerdè¿™ä¸ªåå°æœåŠ¡ã€‚

# å®‰è£…ä¸€äº›å¸¸ç”¨çš„æ”¯æŒã€‚
sudo apt install apt-transport-https ca-certificates curl gnupg2 software-properties-common

# ä½¿ç”¨æ¸…åçš„docker-ceçš„æºï¼Œdocker-ceçš„æºä¸åœ¨å¸¸ç”¨è½¯ä»¶åŒ…çš„æºé‡Œ:
curl -fsSL https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/ubuntu/gpg | sudo apt-key add -
sudo add-apt-repository -y "deb [arch=amd64] https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/ubuntu $(lsb_release -cs) stable"

sudo apt install -y docker-ce docker-ce-cli containerd.io # dockerçš„åŸºç¡€ç¯å¢ƒï¼Œå®‰è£…è¿™äº›

# å®‰è£… nvidia-container-toolkit ã€‚
sudo apt install -y nvidia-container-toolkit nvidia-container-toolkit-base libnvidia-container-tools libnvidia-container1

# æ£€æŸ¥æœåŠ¡å¯åŠ¨æƒ…å†µã€‚
systemctl list-unit-files |grep -E "docker|contain"

root@x:/Data# systemctl list-unit-files |grep -E "docker|contain"
container-getty@.service                     static          -
containerd.service                           enabled         enabled
docker.service                               enabled         enabled
docker.socket                                enabled         enabled

# ä½¿ç”¨ containerd ç”Ÿæˆé…ç½®æ–‡ä»¶ã€‚è¿™ä¸€æ­¥æ˜¯å¿…é¡»çš„
containerd config default > /etc/containerd/config.toml

# ä¸éœ€è¦é…ç½®/etc/containerd/daemon.jsonæ–‡ä»¶ 

# æ£€æŸ¥ runtime_type çš„æŒ‡å‘
root@x:/etc/docker# docker info |grep run
  /var/run/cdi
 Runtimes: runc io.containerd.runc.v2 nvidia
 Default Runtime: runc
 runc version: v1.2.5-0-g59923ef

# æµ‹è¯•è¿è¡Œ nvidia-container-toolkit æ˜¯å¦å·²ç»ç”Ÿæ•ˆã€‚å¦‚æœæ­£å¸¸è¿è¡Œï¼Œåˆ™å·²é…ç½®å¥½ nvidia-container-toolkit ã€‚
docker run --rm --gpus all nvcr.io/nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04 nvidia-smi

æ‰©å±•çŸ¥è¯†:
----------------------------------------------------
K8s + GPU Operator æˆ– Slurm ç®¡ç†å¤šæœº GPU ä»»åŠ¡ã€‚

----------------------------------------------------





4. é…ç½® DCGM + Prometheus + Grafana çš„ GPU ç›‘æ§æ–¹æ¡ˆ

ç³»ç»Ÿæ¶æ„:
äº‘ä¸»æœº:172.18.8.208 2ä¸ªV100-PCIE-16GB
äº‘ä¸»æœº:172.18.8.209 2ä¸ªV100-PCIE-16GB
äº‘ä¸»æœº:172.18.8.210 2ä¸ªV100-PCIE-16GB
æœåŠ¡å™¨:172.18.6.64  æœåŠ¡å™¨ä¸Šæ— GPUï¼Œè¿è¡Œä¸€ä¸ªnvcr.io/nvidia/k8s/dcgm-exporter:3.3.9-3.6.1-ubuntu22.04çš„å®¹å™¨ï¼Œå®¹å™¨å†…è¿è¡Œ:3ä¸ªdcgm-exporterè¿›ç¨‹ï¼Œä»3ä¸ªGPUäº‘æœåŠ¡å™¨è·å–ä¿¡æ¯ã€‚æœåŠ¡ä¸Šï¼Œè¿è¡Œ1ä¸ªprometheuè¿›ç¨‹ï¼Œè¿è¡Œ1ä¸ªgrafanaè¿›ç¨‹ã€‚

dcgm-exporter 

 # ç›®å‰ dcgm çš„ç‰ˆæœ¬
root@x:~# dcgmi --version
dcgmi  version: 3.3.9

# å®‰è£…å®Œæˆä¹‹åï¼Œdcgm.service å…¶å®å¹¶ä¸èƒ½é¡ºåˆ©æ‰§è¡Œï¼Œnv-hostengine --service-account nvidia-dcgm -b ALL è¿™æ¡å‘½ä»¤ã€‚
#systemctl enable dcgm.service 
#systemctl restart dcgm.service 

root@x:/Data# systemctl list-unit-files |grep dcgm
dcgm.service                                 disabled        enabled
nvidia-dcgm.service                          disabled        enabled

# é…ç½® rc-localï¼Œæ¥æ‰§è¡Œå¼€æœºå¯åŠ¨ 'nv-hostengine --service-account nvidia-dcgm -b ALL'
vim /etc/systemd/system/rc-local.service  # åˆ›å»ºè¿™ä¸ªæ–‡ä»¶

touch /etc/systemd/system/rc-local.service
touch /etc/rc.local
chmod 775 /etc/rc.local
cat >> /etc/systemd/system/rc-local.service << EOF
[Unit]
Description=/etc/rc.local Compatibility
ConditionPathExists=/etc/rc.local
[Service]
Type=forking
ExecStart=/etc/rc.local start
TimeoutSec=0
StandardOutput=tty
RemainAfterExit=yes
#SysVStartPriority=99
[Install]
WantedBy=multi-user.target
EOF

cat >> /etc/rc.local << EOF
#!/bin/bash
nv-hostengine --service-account nvidia-dcgm -b ALL
EOF

systemctl enable rc-local.service
systemctl restart rc-local.service

# æµ‹è¯•dcgmi
dcgmi discovery --host 172.18.8.208 -l
dcgmi discovery --host 172.18.8.209 -l
dcgmi discovery --host 172.18.8.210 -l

# å¸¸ç”¨å‘½ä»¤ç¤ºä¾‹
# å®æ—¶ç›‘æ§GPUæŒ‡æ ‡ï¼ˆæ¯2ç§’åˆ·æ–°ï¼‰:
# -e æŒ‡å®šæŒ‡æ ‡IDï¼ˆ203=GPUåˆ©ç”¨ç‡ï¼Œ252=æ˜¾å­˜ä½¿ç”¨ç‡ï¼‰
# -i æŒ‡å®šGPUç´¢å¼•ï¼ˆ0è¡¨ç¤ºç¬¬ä¸€å—GPUï¼‰
dcgmi discovery --host 172.18.8.210 dmon -i 0 -e 203,252 -c 5
dcgmi dmon -i 0 -e 203,252 -c 5

# æŸ¥çœ‹ GPU å¥åº·çŠ¶æ€:
# æ£€æŸ¥GPU 0çš„å¥åº·çŠ¶æ€ï¼ˆ-c è¡¨ç¤ºå…¨é¢æ£€æµ‹ï¼‰
dcgmi health -g 0 -c

# ç»Ÿè®¡ NVLink å¸¦å®½:
# æ˜¾ç¤ºGPU 0çš„NVLinkçŠ¶æ€åŠå¸¦å®½
dcgmi nvlink -i 0 -s

# ä¸‹è½½ dcgm çš„é•œåƒã€‚
docker pull nvcr.io/nvidia/k8s/dcgm-exporter:3.3.9-3.6.1-ubuntu22.04  # å®¹å™¨é‡Œçš„dcgmä¸º3.3.9
docker pull nvidia/dcgm-exporter:4.4.0-4.5.0-ubuntu22.04
docker pull nvcr.io/nvidia/k8s/dcgm-exporter:4.4.0-4.5.0-ubuntu22.04

# å¦‚æœä½¿ç”¨å¸¦ GPU çš„æœåŠ¡å™¨æ¥è¿è¡Œ dcgm-exporter å®¹å™¨ã€‚
docker run -d --gpus all --cap-add SYS_ADMIN --name dcgm -p 9400:9400 -p 9401:9401 -p 9403:9403 -p 9405:9405 nvcr.io/nvidia/k8s/dcgm-exporter:3.3.9-3.6.1-ubuntu22.04
docker run -d --gpus all --cap-add SYS_ADMIN --name dcgm -p 9400:9400 -p 9401:9401 -p 9403:9403 -p 9405:9405 nvcr.io/nvidia/k8s/dcgm-exporter:4.4.0-4.5.0-ubuntu22.04

# æ‰¾äº†ä¸€ä¸ª cpu æœåŠ¡å™¨æ¥è¿è¡Œ
docker run -d --name dcgm -p 9400:9400 -p 9401:9401 -p 9403:9403 -p 9405:9405 nvcr.io/nvidia/k8s/dcgm-exporter:3.3.9-3.6.1-ubuntu22.04
docker run -d --name dcgm -p 9400:9400 -p 9401:9401 -p 9403:9403 -p 9405:9405 nvcr.io/nvidia/k8s/dcgm-exporter:4.4.0-4.5.0-ubuntu22.04

docker exec -it dcgm  /bin/bash     # è¿›å…¥ dcgm-exporter å®¹å™¨
dcgm-exporter -a :9401 -r "172.18.8.208:5555" &  # è®©å‘½ä»¤åœ¨åå°æŒç»­è¿è¡Œã€‚
dcgm-exporter -a :9403 -r "172.18.8.209:5555" &  # è®©å‘½ä»¤åœ¨åå°æŒç»­è¿è¡Œã€‚
dcgm-exporter -a :9405 -r "172.18.8.210:5555" &  # è®©å‘½ä»¤åœ¨åå°æŒç»­è¿è¡Œã€‚

curl 172.18.8.210:5555

# curl æµ‹è¯•
curl 172.18.6.64:9401
curl 172.18.6.64:9403
curl 172.18.6.64:9405

# åœ¨172.18.6.64ï¼Œä¸‹è½½ prometheus
https://github.com/prometheus/prometheus/releases/download/v3.6.0-rc.0/prometheus-3.6.0-rc.0.linux-amd64.tar.gz

# ä¿®æ”¹é…ç½®ï¼Œ
./prometheus.yml
++++++++++++++++++++++++++++++++++++++++++++++++
    static_configs:
      - targets: ["172.18.8.208:9090"]
       # The label name is added as a label `label_name=<label_value>` to any timeseries scraped from this config.
        labels:
          app: "prometheus"

  - job_name: "DCGM_exporter"
    static_configs:
      #- targets: ["172.18.8.208:9400", "172.18.8.208:9403", "172.18.8.208:9405"]
      - targets: ["172.18.8.208:9401", "172.18.8.208:9403", "172.18.8.208:9405"]
        labels:
          app: "DCGM_exporter"
++++++++++++++++++++++++++++++++++++++++++++++++

# è¿è¡Œ prometheus
./prometheus --config.file=./prometheus.yml

# æŸ¥çœ‹prometheusåˆšæ‰çš„é…ç½®æ˜¯å¦ç”Ÿæ•ˆ:
http://172.18.6.64:9090/targets
curl http://172.18.6.64:9090/targets

# å®‰è£… grafana
sudo apt-get install -y adduser libfontconfig1 musl
wget https://dl.grafana.com/grafana-enterprise/release/12.1.1/grafana-enterprise_12.1.1_16903967602_linux_amd64.deb
sudo dpkg -i grafana-enterprise_12.1.1_16903967602_linux_amd64.deb

systemctl enable grafana-server.service
systemctl restart grafana-server.service

# åœ¨ grafana çš„ç½‘ç«™æŸ¥æ‰¾ NVIDIA DCGM Exporterã€‚
https://grafana.com/search/ 
12239
22515







5. å¼€ç®±å³ç”¨çš„æ¨ç†æœåŠ¡:Ollamaä¸Dify
ä»‹ç»å¦‚ä½•å¿«é€Ÿéƒ¨ç½²å¯¹ç¤¾åŒºå‹å¥½çš„æ¨¡å‹æœåŠ¡ï¼ŒéªŒè¯GPUæ¨ç†èƒ½åŠ›ã€‚åœ¨ä»»ä¸€GPUèŠ‚ç‚¹ä¸Šï¼Œä½¿ç”¨Dockerè¿è¡ŒOllamaå®¹å™¨ï¼Œå¹¶æ˜ å°„æ¨¡å‹å­˜å‚¨å·å’Œç«¯å£ã€‚
åœ¨å¸¦æœ‰GPUå¡çš„äº‘ä¸»æœºä¸Šè¿è¡Œollama
# åœ¨å¸¦æœ‰GPUå¡çš„äº‘ä¸»æœºä¸Šï¼Œå¯åŠ¨ollamaçš„å®¹å™¨
root@x:/etc/docker# docker info |grep run
  /var/run/cdi
 Runtimes: runc io.containerd.runc.v2 nvidia
 Default Runtime: runc
 runc version: v1.2.5-0-g59923ef

# ä½¿ç”¨ ollama ä½œä¸ºæ¨¡å‹æ¨ç†å¼•æ“ã€‚
# è‡ªè¡Œéƒ¨ç½²çš„docker
docker run -d --gpus=all --ipc=host --network host -v ollama:/root/.ollama --name ollama hub.rat.dev/ollama/ollama:0.11.6

curl http://127.0.0.1:11434/api/tags

# è¿›å…¥ ollama å®¹å™¨ï¼Œä¸‹è½½æ¨¡å‹ã€‚
nerdctl exec -it da58a9c60029 /bin/bash

ollama pull qwen3:1.7b
ollama pull qwen3:8b
ollama pull qwen3:14b
ollama pull nomic-embed-text

ollama list
root@da58a9c60029:/# ollama list
NAME                       ID              SIZE      MODIFIED
nomic-embed-text:latest    0a109f422b47    274 MB    34 hours ago
qwen3:14b                  bdbd181c33f2    9.3 GB    36 hours ago
qwen3:8b                   500a1f067a9f    5.2 GB    36 hours ago
qwen3:1.7b                 8f68893c685c    1.4 GB    37 hours ago

DIFY
dify 
# å¦å¤–æ‰¾1ä¸ª CPU æœºå™¨ï¼Œå®¹å™¨æ–¹å¼è¿è¡Œ dify
# dify çš„åœ°å€:https://github.com/langgenius/dify/releases/tag/1.8.0
wget https://github.com/langgenius/dify/archive/refs/tags/1.8.0.tar.gz
tar zxf 1.8.0.tar.gz
cd dify-1.8.0;
cd docker
cp .env.example .env

nerdctl pull docker.io/langgenius/dify-sandbox:0.2.12   # æµ‹è¯•æ‹‰å–å®¹å™¨é•œåƒã€‚
nerdctl pull hub.rat.dev/docker.io/langgenius/dify-sandbox:0.2.12  # é€šè¿‡hub.rat.devä»£ç†æ— æ³•ä¸‹è½½ã€‚
nerdctl pull m.daocloud.io/docker.io/langgenius/dify-sandbox:0.2.12  # é€šè¿‡m.daocloud.ioä»£ç†å¯ä»¥ä¸‹è½½ã€‚

cp docker-compose.yaml docker-compose.yaml.20250823  # å¤‡ä»½docker-compose.yamlæ–‡ä»¶ã€‚
sed -i 's#image: #image: m.daocloud.io/docker.io/#g' docker-compose.yaml #æŠŠdocker-compose.yamlæ‰€æœ‰è¦ä¸‹è½½çš„å®¹å™¨é•œåƒå‰ï¼ŒåŠ ä¸Šä»£ç†ã€‚
sed -i 's#image: #image: hub.rat.dev/docker.io/#g' docker-compose.yaml #æŠŠdocker-compose.yamlæ‰€æœ‰è¦ä¸‹è½½çš„å®¹å™¨é•œåƒå‰ï¼ŒåŠ ä¸Šä»£ç†ã€‚

# å¯åŠ¨ didy
cp .env.example .env
docker compose up -d    # å®˜ç½‘å‘½ä»¤ã€‚
nerdctl compose up -d   # æˆ‘æ”¹æˆäº†nerdctl
nerdctl compose -f docker-compose.yaml up -d # æœ¬æ¬¡éƒ¨ç½²ä½¿ç”¨çš„å¯åŠ¨å‘½ä»¤ã€‚
nerdctl compose -f docker-compose.yaml down  # æœ¬æ¬¡éƒ¨ç½²ä½¿ç”¨çš„åœæ­¢å‘½ä»¤ã€‚

# difi-1.18.0ï¼Œå¤–ç½‘è®¿é—®åœ°å€:
http://121.40.245.182:7533/apps
wangzhen2@trustfar.cn
wangzhen2@trustfar.cn
TTyy@1234

ç™»å½•ä¹‹åï¼Œé€‰åœ¨è®¾ç½®ï¼Œé€‰æ‹©æ¨¡å‹ä¾›åº”å•†ï¼Œæ·»åŠ  ollama ï¼Œç„¶åæ·»åŠ æ¨¡å‹ã€‚
æ¨¡å‹ç±»å‹:LLM
æ¨¡å‹åç§°:qwen3:14b
åŸºç¡€url:http://172.18.8.209:11434


æ¨¡å‹ç±»å‹:Text Embedding
æ¨¡å‹åç§°:nomic-embed-text
åŸºç¡€url:http://172.18.8.209:11434

# å¢å¤§å…³äºæ–‡ä»¶ä¸Šä¼ çš„é™åˆ¶
vim .env
UPLOAD_FILE_SIZE_LIMIT=15
UPLOAD_FILE_BATCH_LIMIT=5

# Upload image file size limit, default 10M.
UPLOAD_IMAGE_FILE_SIZE_LIMIT=10
# Upload video file size limit, default 100M.
UPLOAD_VIDEO_FILE_SIZE_LIMIT=100
# Upload audio file size limit, default 50M.
UPLOAD_AUDIO_FILE_SIZE_LIMIT=50

WORKFLOW_FILE_UPLOAD_LIMIT=10

docker compose down
docker compose up -d







6. åœ¨å•æœº2ä¸ª V100-PCIE-16GB çš„ GPUæœåŠ¡å™¨ä¸Š ï¼Œéƒ¨ç½²æ¨¡å‹è¿›è¡Œæ¨ç†
åŒ…å«6ä¸ªå¼€æºæ¨¡å‹çš„éƒ¨ç½²ï¼Œéœ€è¦ä¸‹è½½æ¨¡å‹ï¼Œå¹¶ä¸ºæ¯ä¸ªéƒ¨ç½²åˆ›å»ºä¸€ä¸ª python è™šæ‹Ÿç¯å¢ƒã€‚
å•ä¸ªGPUèŠ‚ç‚¹ï¼ˆä¾‹å¦‚172.18.8.209ï¼‰çš„å®¹å™¨å†…ï¼Œä¸ºä¸åŒç±»å‹çš„AIä»»åŠ¡éƒ¨ç½²ç‹¬ç«‹çš„æ¨ç†æœåŠ¡ã€‚
å‡†å¤‡å·¥ä½œ: å¯åŠ¨ä¸€ä¸ªCUDAå®¹å™¨ï¼Œå¹¶å°†å®¿ä¸»æœºæ•°æ®ç›®å½•æŒ‚è½½è¿›å»ã€‚ä¸ºæ¯ä¸ªæ¨¡å‹åˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„Pythonè™šæ‹Ÿç¯å¢ƒä»¥éš”ç¦»ä¾èµ–ã€‚
æ¨¡å‹ä¸‹è½½: ä½¿ç”¨huggingface-cliå¹¶è®¾ç½®é•œåƒç«¯ç‚¹HF_ENDPOINT=https://hf-mirror.comæ¥ä¸‹è½½æ¨¡å‹æƒé‡ã€‚

6.1 å›¾åƒåˆ†ç±» (ViT): éƒ¨ç½²google/vit-base-patch16-224æ¨¡å‹ï¼Œæä¾›WebæœåŠ¡ã€‚
6.2 æ–‡ç”Ÿå›¾ (Stable Diffusion): éƒ¨ç½²stable-diffusion-v1-5æ¨¡å‹ã€‚
6.3 æ–‡å­—è¯†åˆ« (OCR): éƒ¨ç½²microsoft/trocr-base-printedæ¨¡å‹ã€‚
6.4 è¯­éŸ³è¯†åˆ« (Whisper): éƒ¨ç½²openai/whisper-large-v3æ¨¡å‹ã€‚
6.5 æ–‡å­—è½¬è¯­éŸ³ (TTS): éƒ¨ç½²coqui/XTTS-v2æ¨¡å‹ã€‚
6.6 ç›®æ ‡æ£€æµ‹ (YOLO): éƒ¨ç½²YOLOv13æ¨¡å‹ã€‚

ä»¥ä¸‹æ˜¯åœ¨1ä¸ª GPU çš„äº‘ä¸»æœºä¸Šè¿è¡Œ1ä¸ª nvcr.io/nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04 çš„å®¹å™¨ï¼Œåœ¨å®¹å™¨å†…éƒ¨ç½²å‡ ä¸ªæ¨¡å‹ï¼Œå¹¶æä¾›æ¨ç†æœåŠ¡ã€‚
ç›®å‰ä¸‹é¢çš„3ä¸ªæ¨¡å‹ï¼Œéƒ½éƒ¨ç½²åœ¨172.18.8.209è¿™ä¸ªäº‘ä¸»æœºé‡Œçš„å®¹å™¨ä¸­ã€‚172.18.8.209è¿™ä¸ªäº‘ä¸»æœºçš„é…ç½®ä¸º8Cï¼Œ32G memoryï¼Œ1000G diskã€‚
# è¿è¡Œè¿™ä¸ªå‘½ä»¤ï¼Œè¿™æ ·æŠŠå®¿ä¸»æœºçš„/Dataç›®å½•å…±äº«ç»™å®¹å™¨çš„/Dataç›®å½•ã€‚
docker run -it -d --shm-size=4G --gpus all --network host --cap-add SYS_ADMIN -v /Data:/Data nvcr.io/nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04
docker run -it -d --shm-size=4G --gpus all --network host --cap-add SYS_ADMIN -v /Data:/Data nvcr.io/nvidia/pytorch:23.10-py3     # æ¨èä½¿ç”¨è¿™ä¸ªé•œåƒ


# è¿›å…¥å®¹å™¨ã€‚
docker exec -it 3cb448814bfc9392db74341d1e4f5e27ecfac107600d76ca21c0919164fcc972 /bin/bash

# ä½¿ç”¨æ¸…åçš„ apt æºï¼Œæ›´æ–°ï¼Œå®‰è£…æ‰€éœ€çš„æ“ä½œç³»ç»ŸåŒ…
# ä½¿ç”¨æ¸…åçš„ python æºï¼Œå¹¶å®‰è£…æ‰€éœ€çš„ python åº“


6.1. éƒ¨ç½²å›¾åƒåˆ†ç±»æ¨¡å‹æ¨ç†ã€‚å›¾åƒåˆ†ç±»ï¼ˆè¯†åˆ«æ•´ä½“å†…å®¹ç±»åˆ«ï¼‰,google/vit-base-patch16-224
å›¾åƒåˆ†ç±»ï¼ˆè¯†åˆ«æ•´ä½“å†…å®¹ç±»åˆ«ï¼‰,google/vit-base-patch16-224
# è¿™é‡Œæœ‰æ¯”è¾ƒè¯¦ç»†çš„ä»‹ç»: 
https://zhuanlan.zhihu.com/p/713616890
åŸºäº Vision Transformer çš„å›¾åƒåˆ†ç±»æ¨¡å‹ï¼Œåœ¨ ImageNet ä¸Šé¢„è®­ç»ƒï¼Œæ”¯æŒ 1000 ç±»ç‰©ä½“åˆ†ç±»ï¼ˆå¦‚çŒ«ã€ç‹—ã€æ±½è½¦ç­‰ï¼‰ã€‚
python3 -m venv /Data/DEMO/CODE/VIT/VIT
source /Data/DEMO/CODE/VIT/VIT/bin/activate
pip download -r vit_0.py.requirements.txt --dest /Data/IMAGES/whl -v   # æŠŠrequirements.txtæ–‡ä»¶ä¸­åˆ—å‡ºçš„åŒ…å…¨éƒ½å…ˆä¸‹è½½åˆ°/Data/IMAGES/whlç›®å½•ï¼Œä¸å®‰è£…ã€‚
pip install -r vit_0.py.requirements.txt --find-links=/Data/IMAGES/whl --no-index #ä»/Data/IMAGES/whlå¯»æ‰¾å®‰è£…åŒ…å®‰è£…ï¼Œä¸è¦é€£æ¥åˆ° PyPIã€‚
python3 /Data/DEMO/CODE/VIT/vit_0.py
deactivate

cat > vit_0.py.requirements.txt <<EOF
gradio==5.44.1
numpy==2.3.2
pandas==2.3.2
Pillow==11.3.0
torch==2.8.0
transformers==4.56.0
opencv-python
opencv-contrib-python 
opencv-python-headless
EOF

# ç¨‹åºæŠ¥é”™ï¼ŒåŠéœ€è¦å®‰è£…çš„åŒ…
ModuleNotFoundError: No module named 'cv2'   # pip install opencv-python opencv-contrib-python opencv-python-headless
ImportError: libGL.so.1: cannot open shared object file: No such file or directory # apt install -y libgl1
ImportError: libgthread-2.0.so.0: cannot open shared object file: No such file or directory # apt install -y libglib2.0-0
ModuleNotFoundError: No module named 'PIL'  # pip install Pillow

curl -I https://huggingface.co/google/vit-base-patch16-224
curl -I https://hf-mirror.com/google/vit-base-patch16-224

# åŸºæœ¬ç”¨æ³•-ä¸‹è½½æ¨¡å‹
huggingface-cli download bigscience/bloom-560m --local-dir bloom-560m
# åŸºæœ¬ç”¨æ³•-ä¸‹è½½æ•°æ®é›†
huggingface-cli download --repo-type dataset lavita/medical-qa-shared-task-v1-toy

# å›¾åƒåˆ†ç±»æ¨¡å‹ã€‚google/vit-base-patch16-224
export HF_ENDPOINT=https://hf-mirror.com
huggingface-cli download google/vit-base-patch16-224 --local-dir /Data/DEMO/MODEL/google/vit-base-patch16-224  # å›¾åƒåˆ†ç±»
python3 /Data/DEMO/CODE/VIT/image_2.py
port:7860
# å¤–ç½‘ç›´æ¥è®¿é—®:
http://121.40.245.182:7503/


6.2. éƒ¨ç½²æ–‡ç”Ÿå›¾æ¨¡å‹æ¨ç†ã€‚stable-diffusion-v1-5/stable-diffusion-v1-5
å½“å‰ä½¿ç”¨çš„stable-diffusion-v1-5/stable-diffusion-v1-5æ¨¡å‹ï¼Œæ˜¯è‹±æ–‡çš„ï¼Œæ‰€ä»¥ç›®å‰åªæ”¯æŒè‹±æ–‡çš„æç¤ºè¯ã€‚

7861
python3 -m venv /Data/DEMO/CODE/TXT2IMG/TXT2IMG
source /Data/DEMO/CODE/TXT2IMG/TXT2IMG/bin/activate
pip download -r text2img_server.py.requirements.txt --dest /Data/IMAGES/whl -v   # æŠŠrequirements.txtæ–‡ä»¶ä¸­åˆ—å‡ºçš„åŒ…å…¨éƒ½å…ˆä¸‹è½½åˆ°/Data/IMAGES/whlç›®å½•ï¼Œä¸å®‰è£…ã€‚
pip install -r text2img_server.py.requirements.txt --find-links=/Data/IMAGES/whl --no-index #ä»/Data/IMAGES/whlå¯»æ‰¾å®‰è£…åŒ…å®‰è£…ï¼Œä¸è¦é€£æ¥åˆ° PyPIã€‚
python3 /Data/DEMO/CODE/TXT2IMG/text2img_server.py
deactivate

export HF_ENDPOINT=https://hf-mirror.com
huggingface-cli download stable-diffusion-v1-5/stable-diffusion-v1-5 --local-dir /Data/DEMO/MODEL/stable-diffusion-v1-5/stable-diffusion-v1-5  # æ–‡ç”Ÿåœ–
python3 /Data/DEMO/CODE/TXT2IMG/text2img_server.py 7861
port:7861
# æ–‡ç”Ÿå›¾æ¨¡å‹å¤–ç½‘ç›´æ¥è®¿é—®å¤–ç½‘ç›´æ¥è®¿é—®:
http://121.40.245.182:7504/

cat > requirements.txt.text2img_server <<EOF
gradio==5.44.1
numpy==2.3.2
pandas==2.3.2
Pillow==11.3.0
torch==2.8.0
diffusers
accelerate
transformers
EOF

# è¿è¡Œä»¥ä¸‹å‘½ä»¤æ£€æŸ¥å…³é”®ä¾èµ–:
pip list | grep -E "torch|diffusers|transformers|xformers"

# å¦‚æœéœ€è¦çš„è¯ï¼Œå¼ºåˆ¶å‡çº§æ‰€æœ‰ä¾èµ–:
pip install --upgrade torch diffusers transformers accelerate



6.3. éƒ¨ç½²OCRæ¨¡å‹æ¨ç†ã€‚OCRï¼ˆæ–‡å­—è¯†åˆ«ï¼‰,microsoft/trocr-base-printed
OCRï¼ˆæ–‡å­—è¯†åˆ«ï¼‰
microsoft/trocr-base-printedï¼Œç›®å‰è¿™ä¸ªæ¨¡å‹å¥½åƒæœ‰é—®é¢˜ï¼Œæ— æ³•å‡†ç¡®è¯†åˆ«ï¼Œæ­£åœ¨æµ‹è¯•å…¶å®ƒçš„OCRæ¨¡å‹ã€‚
ç«¯åˆ°ç«¯ OCR æ¨¡å‹ï¼Œæ”¯æŒå°åˆ·ä½“æ–‡å­—è¯†åˆ«ï¼Œå¯ç›´æ¥è¾“å‡ºå›¾åƒä¸­çš„æ–‡æœ¬å†…å®¹ã€‚

export HF_ENDPOINT=https://hf-mirror.com
huggingface-cli download microsoft/trocr-base-printed --local-dir /Data/DEMO/MODEL/microsoft/trocr-base-printed  # å°åˆ·æ–‡æœ¬è¯†åˆ«
python3 /Data/DEMO/CODE/OCR/ocr_0.py
port:7862
# å¤–ç½‘ç›´æ¥è®¿é—®:
http://121.40.245.182:7505/

python3 -m venv /Data/DEMO/CODE/OCR/OCR
source /Data/DEMO/CODE/OCR/OCR/bin/activate
pip download -r ocr_0.py.requirements.txt --dest /Data/IMAGES/whl -v   # æŠŠocr_0.py.requirements.txtæ–‡ä»¶ä¸­åˆ—å‡ºçš„åŒ…å…¨éƒ½å…ˆä¸‹è½½åˆ°/Data/IMAGES/whlç›®å½•ï¼Œä¸å®‰è£…ã€‚
pip install -r ocr_0.py.requirements.txt --find-links=/Data/IMAGES/whl --no-index #ä»/Data/IMAGES/whlå¯»æ‰¾å®‰è£…åŒ…å®‰è£…ï¼Œä¸è¦é€£æ¥åˆ° PyPIã€‚
python3 /Data/DEMO/CODE/OCR/ocr_0.py
deactivate

cat > ocr_0.py.requirements.txt <<EOF
transformers==4.44.2
torch==2.3.1
opencv-python-headless>=4.5.0
pandas>=2.0.0
pillow>=10.0.0
gradio>=4.0.0
opencv-python
opencv-contrib-python 
opencv-python-headless
EOF

pip freeze > ocr_0.py.requirements.txt  # ä¿å­˜å…¨éƒ¨çš„whlåŒ…åç§°


6.4. éƒ¨ç½²å¼€æºè¯­éŸ³è¯†åˆ«æ¨¡å‹ã€‚openai/whisper-large-v3
# è¿™ä¸ªä»‹ç»æ¯”è¾ƒè¯¦ç»†:
https://www.cnblogs.com/liupiaos/p/18465221    pythonç³»åˆ—&deep_studyç³»åˆ—:Whisper OpenAIå¼€æºè¯­éŸ³è¯†åˆ«æ¨¡å‹
https://zhuanlan.zhihu.com/p/662906303         OpenAI Whisper æ–°ä¸€ä»£è¯­éŸ³æŠ€æœ¯(æ›´æ–°è‡³v3-turbo)
  
export HF_ENDPOINT=https://hf-mirror.com
huggingface-cli download openai/whisper-large-v3 --local-dir /Data/DEMO/MODEL/openai/whisper-large-v3  # å¼€æºè¯­éŸ³è¯†åˆ«æ¨¡å‹
pythone3 /Data/DEMO/CODE/WHISPER/whisper_gradio_asr_8.py
port:7863
# å¤–ç½‘ç›´æ¥è®¿é—®:
http://121.40.245.182:7506/

7863
python3 -m venv /Data/DEMO/CODE/WHISPER/WHISPER
source /Data/DEMO/CODE/WHISPER/WHISPER/bin/activate
pip download -r whisper_gradio_asr_8.py.requirements.txt --dest /Data/IMAGES/whl -v   # æŠŠrequirements.txtæ–‡ä»¶ä¸­åˆ—å‡ºçš„åŒ…å…¨éƒ½å…ˆä¸‹è½½åˆ°/Data/IMAGES/whlç›®å½•ï¼Œä¸å®‰è£…ã€‚
pip install -r whisper_gradio_asr_8.py.requirements.txt --find-links=/Data/IMAGES/whl --no-index #ä»/Data/IMAGES/whlå¯»æ‰¾å®‰è£…åŒ…å®‰è£…ï¼Œä¸è¦é€£æ¥åˆ° PyPIã€‚
python3 /Data/DEMO/CODE/WHISPER/whisper_gradio_asr_8.py
deactivate


6.5. éƒ¨ç½²å¼€æºè¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼Œæ–‡å­—è½¬è¯­éŸ³ã€‚coqui/XTTS-v2
export HF_ENDPOINT=https://hf-mirror.com
huggingface-cli download coqui/XTTS-v2 --local-dir /Data/DEMO/MODEL/coqui/XTTS-v2  # å¼€æºè¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼Œæ–‡å­—è½¬è¯­éŸ³
python3 /Data/DEMO/CODE/TTS/xtts_tts_server_5.py
port:7864
# å¼€æºè¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼Œæ–‡å­—è½¬è¯­éŸ³ã€‚
http://121.40.245.182:7507/

cat > requirements.txt <<EOF
gradio==5.45.0
gradio_client==1.13.0
torch==2.3.1
torchaudio==2.3.1
TTS==0.22.0
numpy
pandas
EOF

7864
python3 -m venv /Data/DEMO/CODE/TTS/TTS
source /Data/DEMO/CODE/TTS/TTS/bin/activate
pip download -r xtts_tts_server_5.py.requirements.txt --dest /Data/IMAGES/whl -v   # æŠŠrequirements.txtæ–‡ä»¶ä¸­åˆ—å‡ºçš„åŒ…å…¨éƒ½å…ˆä¸‹è½½åˆ°/Data/IMAGES/whlç›®å½•ï¼Œä¸å®‰è£…ã€‚
pip install -r xtts_tts_server_5.py.requirements.txt --find-links=/Data/IMAGES/whl --no-index #ä»/Data/IMAGES/whlå¯»æ‰¾å®‰è£…åŒ…å®‰è£…ï¼Œä¸è¦é€£æ¥åˆ° PyPIã€‚
python3 /Data/DEMO/CODE/TTS/xtts_tts_server_5.py
deactivate


6.6. å¼€æºè¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼Œæ–‡å­—è½¬è¯­éŸ³ã€‚YOLO
# å‚è€ƒ:
https://blog.csdn.net/youcans/article/details/142510400    ã€è·Ÿæˆ‘å­¦YOLOã€‘YOLO13ï¼ˆ2ï¼‰æ¨¡å‹ä¸‹è½½ã€ç¯å¢ƒé…ç½®ä¸æ£€æµ‹
https://zhuanlan.zhihu.com/p/1920777603847021007

https://docs.ultralytics.com/zh/
https://github.com/iMoonLab/yolov13/
https://github.com/iMoonLab/yolov13/releases/tag/yolov13    # ä¸‹è½½YOLOV13æ¨¡å‹æƒé‡æ–‡ä»¶ã€‚
port:7865
# å¤–ç½‘ç›´æ¥è®¿é—®:
http://121.40.245.182:7508/


# ä¼˜å…ˆå®‰è£…æŒ‡å®šç‰ˆæœ¬çš„Gradioï¼ˆé¿å…æœ€æ–°ç‰ˆæœ¬Schemaè§£æé—®é¢˜ï¼‰yolov13ï¼Œä¸€å®šè¦ä½¿ç”¨è¿™äº›åº“ã€‚
pip install gradio==3.48.0 ultralytics==8.2.28 opencv-python==4.9.0.80 pandas==2.2.1 torch==2.2.1
pip install gradio==3.48.0 ultralytics==8.2.28 opencv-python==4.9.0.80 pandas==2.2.1 torch==2.2.1+cu121 -f https://download.pytorch.org/whl/torch_stable.html

apt install python3.10-venv  # éœ€è¦å…ˆå®‰è£… venv
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple package_name

7865
python3 -m venv /Data/DEMO/CODE/YOLO/YOLO
source /Data/DEMO/CODE/YOLO/YOLO/bin/activate
pip install ./flash_attn-2.7.3+cu11torch2.2cxx11abiFALSE-cp310-cp310-linux_x86_64.whl  --no-dependencies
pip download -r yolo_1.py.requirement.txt --dest /Data/IMAGES/whl -v   # æŠŠrequirements.txtæ–‡ä»¶ä¸­åˆ—å‡ºçš„åŒ…å…¨éƒ½å…ˆä¸‹è½½åˆ°/Data/IMAGES/whlç›®å½•ï¼Œä¸å®‰è£…ã€‚
pip install -r yolo_1.py.requirement.txt --find-links=/Data/IMAGES/whl --no-index #ä»/Data/IMAGES/whlå¯»æ‰¾å®‰è£…åŒ…å®‰è£…ï¼Œä¸è¦é€£æ¥åˆ° PyPIã€‚

python3 /Data/DEMO/CODE/YOLO/yolo_1.py
deactivate

# é”™è¯¯æç¤ºå’Œè§£å†³æ–¹æ³•:
é”™è¯¯:ImportError: libGL.so.1: cannot open shared object file: No such file or directory
apt-get install -y libgl1-mesa-glx

é”™è¯¯:ImportError: libgthread-2.0.so.0: cannot open shared object file: No such file or directory
apt-get install -y libglib2.0-0

# è¿™ä¸ªç‰ˆæœ¬ä¸å¯¹
(YOLO) root@anhua209:/Data/DEMO/CODE/YOLO/yolov13# pip list |grep thop
thop                     0.1.1.post2209072238

é”™è¯¯:ModuleNotFoundError: No module named 'thop'
pip install thop

# è¿è¡Œä»¥ä¸‹å‘½ä»¤æ£€æŸ¥å…³é”®ä¾èµ–:
pip list | grep -E "torch|diffusers|transformers|xformers|ultralytics|opencv-python|pandas|onnx|flash|gradio|huggingface|thop "


(YOLO) root@anhua209:/Data/DEMO/CODE/YOLO/yolov13# cat requirements.txt 
torch==2.2.2 
torchvision==0.17.2
#flash_attn-2.7.3+cu11torch2.2cxx11abiFALSE-cp311-cp311-linux_x86_64.whl
timm==1.0.14
albumentations==2.0.4
onnx==1.14.0
onnxruntime==1.15.1
pycocotools==2.0.7
PyYAML==6.0.1
scipy==1.13.0
onnxslim==0.1.31
onnxruntime-gpu==1.18.0
#gradio==4.44.1
gradio==3.50.2
opencv-python==4.9.0.80
psutil==5.9.8
py-cpuinfo==9.0.0
huggingface-hub==0.23.2
safetensors==0.4.3
numpy==1.26.4
supervision==0.22.0







7. åœ¨å¤–ç½‘è®¿é—®æ‰€éƒ¨ç½²çš„æ¨¡å‹æ¨ç†æœåŠ¡

7.1. # å›¾åƒåˆ†ç±»æ¨¡å‹ã€‚google/vit-base-patch16-224
export HF_ENDPOINT=https://hf-mirror.com
huggingface-cli download google/vit-base-patch16-224 --local-dir /Data/google/vit-base-patch16-224  # å›¾åƒåˆ†ç±»
python3 /Data/DEMO/CODE/VIT/image_2.py
port:7860
# å¤–ç½‘ç›´æ¥è®¿é—®:
http://121.40.245.182:7503/


7.2. # æ–‡ç”Ÿå›¾æ¨¡å‹ã€‚stable-diffusion-v1-5/stable-diffusion-v1-5
export HF_ENDPOINT=https://hf-mirror.com
huggingface-cli download stable-diffusion-v1-5/stable-diffusion-v1-5 --local-dir /Data/stable-diffusion-v1-5/stable-diffusion-v1-5  # æ–‡ç”Ÿåœ–
python3 /Data/DEMO/CODE/TXT2IMG/text2img_server.py 7861
port:7861
# å¤–ç½‘ç›´æ¥è®¿é—®:
http://121.40.245.182:7504/


7.3. # OCRæ¨¡å‹ã€‚microsoft/trocr-base-printed
export HF_ENDPOINT=https://hf-mirror.com
huggingface-cli download microsoft/trocr-base-printed --local-dir /Data/microsoft/trocr-base-printed  # å°åˆ·æ–‡æœ¬è¯†åˆ«
python3 /Data/DEMO/CODE/OCR/ocr_0.py
port:7862
# å¤–ç½‘ç›´æ¥è®¿é—®:
http://121.40.245.182:7505/


7.4. # å¼€æºè¯­éŸ³è¯†åˆ«æ¨¡å‹ã€‚openai/whisper-large-v3
export HF_ENDPOINT=https://hf-mirror.com
huggingface-cli download openai/whisper-large-v3 --local-dir /Data/DEMO/MODEL/openai/whisper-large-v3  # å¼€æºè¯­éŸ³è¯†åˆ«æ¨¡å‹
pythone3 /Data/DEMO/CODE/WHISPER/whisper_gradio_asr_8.py
port:7863
# å¤–ç½‘ç›´æ¥è®¿é—®:
http://121.40.245.182:7506/


7.5. # å¼€æºè¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼Œæ–‡å­—è½¬è¯­éŸ³ã€‚coqui/XTTS-v2
export HF_ENDPOINT=https://hf-mirror.com
huggingface-cli download coqui/XTTS-v2 --local-dir /Data/DEMO/MODEL/coqui/XTTS-v2  # å¼€æºè¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼Œæ–‡å­—è½¬è¯­éŸ³
python3  /Data/DEMO/CODE/TTS/xtts_tts_server_5.py
port:7864
# å¼€æºè¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼Œæ–‡å­—è½¬è¯­éŸ³ã€‚
http://121.40.245.182:7507/


7.6. # å¼€æºè¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼Œæ–‡å­—è½¬è¯­éŸ³ã€‚YOLO
https://docs.ultralytics.com/zh/
https://github.com/iMoonLab/yolov13/
https://github.com/iMoonLab/yolov13/releases/tag/yolov13  # æƒé‡ä¸‹è½½
python3 /Data/DEMO/CODE/YOLO/yolo_1.py
port:7865
# å¤–ç½‘ç›´æ¥è®¿é—®:
http://121.40.245.182:7508/







8. åˆ†å¸ƒå¼è®¡ç®—é›†ç¾¤Ray
RAY
docker æ–¹å¼éƒ¨ç½²rayé›†ç¾¤ï¼Œæœªéƒ¨ç½²æˆåŠŸï¼Œç‰ˆæœ¬å¤ªé«˜ã€‚:
docker pull hub.rat.dev/rayproject/ray-ml:nightly-py39-gpu # é•œåƒé‡Œçš„cudaç‰ˆæœ¬æ˜¯:CUDA Version 12.1.1ï¼Œæœªéƒ¨ç½²æˆåŠŸï¼Œç‰ˆæœ¬å¤ªé«˜ã€‚
docker pull hub.rat.dev/rayproject/ray:nightly-py39-cu128  # é•œåƒé‡Œçš„cudaç‰ˆæœ¬æ˜¯:CUDA Version 12.8.1ï¼Œæœªéƒ¨ç½²æˆåŠŸï¼Œç‰ˆæœ¬å¤ªé«˜ã€‚
docker pull hub.rat.dev/rayproject/ray:2.37.0

# å¯ä»¥ä¸‹è½½ Ray çš„ CPU ç‰ˆæœ¬ï¼Œåœ¨ cpu çš„è®¡ç®—èŠ‚ç‚¹ä¸Šåšæµ‹è¯•:
docker pull hub.rat.dev/rayproject/ray:nightly-py39-cpu # å¯ä»¥ä¸‹è½½ã€‚
docker pull m.daocloud.io/rayproject/ray:nightly-py39-cpu

# å¯åŠ¨ Head èŠ‚ç‚¹
# åœ¨GPUçš„ä¸»æœºä¸Šï¼Œè¿è¡Œä¸€ä¸ªcuda:11.8.0-cudnn8-runtime-ubuntu22.04å®¹å™¨ï¼Œåœ¨è¿™ä¸ªå®¹å™¨é‡Œå®‰è£…vllmï¼Œrayï¼Œå¹¶åšä¸ºrayçš„HeadèŠ‚ç‚¹ã€‚
# å› ä¸ºéœ€è¦pythonä½äºPython 3.11ï¼Œubuntu22.04çš„pythonç‰ˆæœ¬æ˜¯3.10.12ï¼Œåœ¨CPUèŠ‚ç‚¹ä¸Šå¯åŠ¨ä¸€ä¸ªubuntu22.04ç”¨æ¥å®‰è£…rayã€‚
docker run -it -d --cap-add SYS_ADMIN --net=host --pid=host --ipc=host --privileged nvcr.io/nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04

# æ¯ä¸ªèŠ‚ç‚¹ä¸Šå¯åŠ¨ä¸€ä¸ªé•œåƒï¼Œç”¨æ¥éƒ¨ç½² ray ï¼Œå¦‚æœéœ€è¦æŒ‚è½½ vllm æ¨¡å‹ç›®å½•ï¼Œæ¯ä¸ªå‡†å¤‡ä½œä¸º ray çš„èŠ‚ç‚¹çš„å®¹å™¨ï¼Œéƒ½éœ€è¦é¢„å…ˆå°±æŒ‚è½½ä¸Š
docker run -it -d --gpus all --cap-add SYS_ADMIN --net=host --pid=host --ipc=host --privileged -v /Data/Qwen/Qwen2.5-1.5B:/model nvcr.io/nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04 
docker run -it -d --gpus all --cap-add SYS_ADMIN --net=host --pid=host --ipc=host --privileged -v /Data/Qwen/Qwen2.5-14B:/model nvcr.io/nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04

docker run --rm --cap-add SYS_ADMIN --net=host --pid=host --ipc=host --privileged hub.rat.dev/rayproject/ray:2.37.0


# ä¸‹è½½äº†54ä¸ª python åŒ…
# æŠŠå®‰è£…åŒ…éƒ½ä¸‹è½½åˆ°äº†/Data/IMAGES/whl/ç›®å½•ä¸­ï¼Œå¤§æ¦‚äº†è§£ä¸€ä¸‹rayæ‰€éœ€çš„pythonåŒ…ã€‚
-rw-r--r-- 1 root root 212986727 Sep  9 02:23 xformers-0.0.23.post1-cp310-cp310-manylinux2014_x86_64.whl

# åœ¨ python å®‰è£…æ–‡ä»¶çš„ç›®å½•é‡Œæ‰§è¡Œ
ls |grep whl|xargs -i pip install {} --no-dependencies

# å®‰è£… ray
pip install ray[default] --no-dependencies --find-links=/Data/IMAGES/whl
pip install "numpy<2" "transformers<4.40" "torch==2.1.2"
pip install "numpy<2" "transformers<4.40" "torch==2.1.2" --no-dependencies --find-links=/Data/IMAGES/whl

# ray çš„ç‰ˆæœ¬
ray --version
ray, version 2.49.1

# å¯åŠ¨ Head èŠ‚ç‚¹ï¼Œåœ¨172.18.8.210èŠ‚ç‚¹ä¸Šå…ˆå¯åŠ¨rayï¼Œä½œä¸º Head èŠ‚ç‚¹å¯åŠ¨ ray
ray stop # å…ˆæ‰§è¡Œstopï¼Œç¡®ä¿rayå¤„åœ¨åœæ­¢çŠ¶æ€
ray stop --force
ray start --head --node-ip-address=172.18.8.208 --port=6379 --dashboard-host=0.0.0.0 --include-dashboard=true --dashboard-port=8265 --num-cpus=4 --num-gpus=2

# å¯åŠ¨ Worker èŠ‚ç‚¹ï¼Œåœ¨172.18.8.208å’Œ172.18.8.209è¿™ä¸¤ä¸ªGPUäº‘ä¸»æœºä¸Šå¯åŠ¨rayï¼Œä½œä¸ºworkèŠ‚ç‚¹
ray start --address='172.18.8.210:6379' --node-ip-address=172.18.8.208 --num-cpus=4 --num-gpus=2
ray start --address='172.18.8.210:6379' --node-ip-address=172.18.8.209 --num-cpus=4 --num-gpus=2

# éªŒè¯é›†ç¾¤çŠ¶æ€â€‹
# Head èŠ‚ç‚¹ä¸Šï¼ŒæŸ¥çœ‹é›†ç¾¤èŠ‚ç‚¹
ray list nodes
ray status
# é¢„æœŸè¾“å‡ºåº”åŒ…å«æ‰€æœ‰èŠ‚ç‚¹å’Œèµ„æº

# RAY ï¼Œæµ‹è¯•APIè¿é€šæ€§ï¼Œåœ¨ä»»æ„èŠ‚ç‚¹ä¸Šæ‰§è¡Œ
curl -v http://172.18.8.210:8265/api/v0/nodes

æ‰©å±•çŸ¥è¯†:
----------------------------------------------------
è¨“ç·´ä»»å‹™æ˜¯ç›´æ¥å¤±æ•—ï¼Œé‚„æ˜¯èƒ½å¤ ï¼ˆå¦‚æœä½¿ç”¨äº† Ray æˆ–å…¶ä»–å½ˆæ€§æ¡†æ¶ï¼‰æª¢æ¸¬åˆ°ç¯€é»ä¸Ÿå¤±ä¸¦å˜—è©¦æ¢å¾©æˆ–ç¹¼çºŒé‹è¡Œ

åœ¨CPUæœåŠ¡å™¨ä¸Šä½¿ç”¨hub.rat.dev/rayproject/ray:nightly-py39-cpuï¼Œåšæµ‹è¯•:
docker run -it -d --shm-size=4G --network host -v /Data:/Data hub.rat.dev/rayproject/ray:nightly-py39-cpu # è¿™ä¸ªå®¹å™¨é‡Œçš„Rayç‰ˆæœ¬ä¸º:ray, version 3.0.0.dev0
----------------------------------------------------


# å¯åŠ¨ Head èŠ‚ç‚¹ï¼Œ
ray start --head --node-ip-address=172.18.6.60 --port=6379 --dashboard-host=0.0.0.0 --include-dashboard=true --dashboard-port=8265 --num-cpus=4

# å¯åŠ¨ Worker èŠ‚ç‚¹ï¼Œ
ray start --address='172.18.6.60:6379' --node-ip-address=172.18.6.61 --num-cpus=4
ray start --address='172.18.6.60:6379' --node-ip-address=172.18.6.62 --num-cpus=4


# å½“æµ‹è¯•å†™ python ç¨‹åºå»æµ‹è¯• Ray é›†ç¾¤ï¼Œå¹¶è¿›è¡Œæ“ä½œçš„æ—¶å€™ï¼Œä¼šæœ‰ç‰ˆæœ¬é—®é¢˜:
2025-10-16 16:04:07 - ERROR - è¿æ¥Rayé›†ç¾¤å¤±è´¥: Version mismatch: The cluster was started with:
    Ray: 3.0.0.dev0
    Python: 3.9.23
This process on Ray Client was started with:
    Ray: 2.50.0
    Python: 3.12.3

docker pull rayproject/ray:2.50.0-cpu
docker pull hub.rat.dev/rayproject/ray:2.50.0-cpu

docker run -it -d --shm-size=4G --network host -v /Data:/Data hub.rat.dev/rayproject/ray:2.5.0-cpu
docker run -it -d --shm-size=4G --network host -v /Data:/Data hub.rat.dev/rayproject/ray:2.50.0-cpu

# è¿›å…¥åˆ°ä»»æ„çš„ Ray èŠ‚ç‚¹çš„å®¹å™¨ï¼Œè¿è¡Œ:
pthone3 ray_cluster_test.py

# æç¤º:
ğŸŸ¡ å…³äºâ€‹â€‹ä»»åŠ¡å¤±è´¥è­¦å‘Šï¼ˆä»»åŠ¡ 1 å’Œ 6ï¼‰â€‹â€‹:
ğŸ”’ â€‹â€‹è¿™æ˜¯æ­£å¸¸çš„ï¼Œæ˜¯æ‚¨è‡ªå·±ä»£ç ä¸­æ•…æ„æŠ›å‡ºå¼‚å¸¸æ¥æµ‹è¯•å®¹é”™æ€§çš„ï¼â€‹

# âœ… æœ€ç»ˆç»“è®º:
 â€‹â€‹Ray é›†ç¾¤åŠŸèƒ½æµ‹è¯•æ•´ä½“è¿è¡ŒæˆåŠŸâ€‹â€‹ï¼Œå…·ä½“åŒ…æ‹¬:
åŠŸèƒ½                 çŠ¶æ€                 è¯´æ˜
Ray é›†ç¾¤è¿æ¥         âœ… æˆåŠŸ              è§£å†³äº†ç‰ˆæœ¬åŒ¹é…é—®é¢˜åè¿æ¥æ­£å¸¸
åŸºæœ¬ä»»åŠ¡è°ƒåº¦ & å¹¶å‘   âœ… æˆåŠŸ              å¤šä»»åŠ¡å¹¶è¡Œæ‰§è¡Œ
å®¹é”™èƒ½åŠ›æµ‹è¯•          âœ… æˆåŠŸ              æœ‰ä»»åŠ¡æ•…æ„å¤±è´¥ï¼ŒRay æ­£å¸¸å¤„ç†
åˆ†å¸ƒå¼ä»»åŠ¡é€šä¿¡        âœ… æˆåŠŸ              ä»»åŠ¡é—´å¯äº’ç›¸é€šä¿¡
å¤§æ•°æ®ä¼ è¾“æµ‹è¯•        âœ… æˆåŠŸï¼ˆä½†æœ‰è­¦å‘Šï¼‰    10MB æ•°æ®ä¼ è¾“æˆåŠŸï¼Œä½†å»ºè®®ä¼˜åŒ–æ–¹å¼





9. é«˜æ€§èƒ½æ¨ç†å¼•æ“ vLLM å•æœºéƒ¨ç½²
vLLMå¸¸è§é—®é¢˜
1. CUDA Out of Memory: æ¨¡å‹æƒé‡æˆ– KV Cache è¶…å‡ºæ˜¾å­˜ã€‚è§£å†³æ–¹æ³•æ˜¯å‡å° batch sizeã€åºåˆ—é•¿åº¦ï¼Œæˆ–ä½¿ç”¨æ›´å¼ºçš„é‡åŒ–ã€‚
2. æ¨¡å‹æ¶æ„ä¸è¯†åˆ«: Transformersåº“ç‰ˆæœ¬è¿‡ä½ï¼Œéœ€è¦å‡çº§ã€‚
3. P2Pèƒ½åŠ›è­¦å‘Š: V100 PCIEç‰ˆæœ¬å¯èƒ½ä¸æ”¯æŒP2Pï¼Œæ·»åŠ --disable-custom-all-reduceå‚æ•°å¯è§„é¿æ­¤é—®é¢˜ã€‚

https://zhuanlan.zhihu.com/p/1916898243423500022    vLLMå‚æ•°è¯¦ç»†è¯´æ˜
https://blog.csdn.net/baiyipiao/article/details/141930442    vllmå¸¸ç”¨å‚æ•°æ€»ç»“
https://www.studywithgpt.com/zh-cn/tutorial/zl0s7e    ä½¿ç”¨Dockeréƒ¨ç½²vLLM

ç¡®ä¿PyTorchç‰ˆæœ¬â‰¤2.1.2ï¼ŒCUDAç‰ˆæœ¬ä¸º11.8ï¼ŒV100-PCIE-16GB
transformers>=4.40.0  # Qwen3éœ€è¦â‰¥4.40

å¯¹äº â€‹â€‹Tesla V100-PCIE-16GBâ€‹â€‹ï¼Œä»¥ä¸‹æ˜¯ç»è¿‡éªŒè¯çš„å…¼å®¹CUDAå®¹å™¨ç‰ˆæœ¬:
â€‹â€‹1. æ¨èç‰ˆæœ¬çŸ©é˜µâ€‹â€‹
å®¹å™¨ç‰ˆæœ¬	                               CUDAç‰ˆæœ¬	PyTorch	   TensorRT	  å…¼å®¹æ€§	 æ¨èåº¦
nvcr.io/nvidia/pytorch:23.10-py3	      11.8	   2.1.2	    8.6.1	    âœ… æœ€ä½³	â­â­â­â­â­
nvcr.io/nvidia/tensorrt:23.09-py3	      11.8	   -	        8.6.      âœ… ä¼˜ç§€	â­â­â­â­
nvidia/cuda:11.8.0-runtime-ubuntu20.04	11.8	   éœ€å®‰è£…	     -	       âœ… ç¨³å®š	 â­â­â­
nvcr.io/nvidia/pytorch:22.12-py3	      11.7	   1.14	      8.5.3   	âœ… è‰¯å¥½	â­â­â­

ä¸å…¼å®¹ç‰ˆæœ¬è­¦å‘Šâ€‹â€‹
â€‹â€‹é¿å…ä»¥ä¸‹ç‰ˆæœ¬â€‹â€‹:
å®¹å™¨ç‰ˆæœ¬	                         é—®é¢˜
nvcr.io/nvidia/pytorch:24.0*	    CUDA 12.4ï¼ŒV100æ”¯æŒä¸å®Œå–„
nvidia/cuda:12.*	                éœ€è¦é©±åŠ¨535+ï¼Œå¯èƒ½ä¸å…¼å®¹æ—§ç³»ç»Ÿ
ä»»ä½•åŒ…å«PyTorch                      2.2+çš„ç‰ˆæœ¬	å·²å¼ƒç”¨è®¡ç®—èƒ½åŠ›7.0

# ç¡¬ä»¶æ˜¯ Tesla V100ï¼ˆç®—åŠ› 7.0ï¼‰ï¼Œä½†æ—¥å¿—ä¸­ PyTorch æ˜ç¡®æç¤º:The minimum cuda capability supported by this library is 7.5ï¼ˆæœ€ä½æ”¯æŒç®—åŠ› 7.5ï¼Œå¦‚ Tesla T4ï¼‰ã€‚
# å½“å‰ vLLM é•œåƒï¼ˆv0.10.1.1ï¼‰å†…ç½®çš„ PyTorch ç‰ˆæœ¬è¿‡é«˜ï¼ˆæ¨æµ‹ â‰¥2.1ï¼‰ï¼Œå·²ç§»é™¤å¯¹ç®—åŠ› 7.0 çš„æ”¯æŒã€‚
# v100å»ºè®®ä½¿ç”¨ vllm/vllm-openai:v0.3.0 é•œåƒ
# vllm/vllm-openai:v0.3.0	æ ¸å¿ƒ:æ—§ç‰ˆé•œåƒå†…ç½® PyTorch 2.0ï¼Œæ”¯æŒç®—åŠ› 7.0 çš„ V100 GPU
# --disable-custom-all-reduce	ç¦ç”¨ vLLM è‡ªå®šä¹‰çš„åˆ†å¸ƒå¼é€šä¿¡ä¼˜åŒ–ï¼ˆéƒ¨åˆ†æ–°ç‰ˆä¼˜åŒ–ä¸å…¼å®¹ V100ï¼‰ï¼Œç¡®ä¿ NCCL é€šä¿¡æ­£å¸¸

# é€‚ç”¨ vllm/vllm-openai:v0.3.0 é•œåƒï¼Œä½¿ç”¨ Qwen2.5-1.5B çš„æ¨¡å‹ï¼Œè¿›è¡Œè°ƒè¯•

é€‚åˆ V100 16GB çš„æ¨¡å‹
# æ¨èæ¨¡å‹å¤§å°
MODELS=(
    "Qwen2-7B-Instruct"     # 7Bå‚æ•°ï¼Œé€‚åˆ2ä¸ªV100
    "Llama-2-7b-chat-hf"    # 7Bå‚æ•°
    "Mistral-7B-Instruct"   # 7Bå‚æ•°
    "Baichuan2-7B-Chat"     # 7Bå‚æ•°
)

# å¯åŠ¨å‚æ•°ä¼˜åŒ–
OPTIMAL_CONFIG="--tensor-parallel-size 2 \
    --gpu-memory-utilization 0.85 \
    --max-model-len 4096 \
    --dtype float16 \
    --enforce-eager \
    --max-num-batched-tokens 2048"


hub.rat.dev/vllm/vllm-openai:v0.3.0                     # é€‚ç”¨ V100-PCIE-16GB å‹å·
hub.rat.dev/lmsysorg/sglang:v0.3.6.post3-cu124          # é€‚ç”¨ V100-PCIE-16GB å‹å·ï¼Œä½†æ˜¯ sglang çš„è¿™ä¸ªç‰ˆæœ¬å·²ç§»é™¤å¯¹ç®—åŠ›7.0çš„æ”¯æŒã€‚

hub.rat.dev/vllm/vllm-openai:v0.10.1.1                  # å·²ç§»é™¤å¯¹ç®—åŠ›7.0çš„æ”¯æŒã€‚
m.daocloud.io/vllm/vllm-openai:v0.10.1.1                # å·²ç§»é™¤å¯¹ç®—åŠ›7.0çš„æ”¯æŒã€‚
nvcr.io/nvidia/pytorch:24.05-py3
nvcr.io/nvidia/pytorch:24.02-py3
nvcr.io/nvidia/pytorch:23.10-py3
nvcr.io/nvidia/tensorrt-llm/release:0.21.0rc0
lmsysorg/sglang:v0.4.6.post4-cu124
nvcr.io/nvidia/tritonserver:25.02-trtllm-python-py3     # è¿™äº›é•œåƒå¯åŠ¨ï¼Œéœ€è¦å¸¦æ¨¡å‹ç›®å½•æ‰èƒ½å®Œæˆå¯åŠ¨
nvcr.io/nvidia/tritonserver:25.08-pyt-python-py3
nvcr.io/nvidia/tritonserver:25.08-trtllm-python-py3
m.daocoud.io/lmsysorg/sglang:v0.5.0rc2-cu129-gb200

# å¦‚æœä½¿ç”¨é vllm-openai å®¹å™¨é•œåƒéƒ¨ç½²çš„è¯ï¼Œéœ€è¦åœ¨å®¹å™¨é•œåƒå†…å®‰è£… vllm
pip install vllm==0.3.0
pip install "numpy<2" "transformers<4.40"
pip install "numpy<2" "transformers<4.40" "torch==2.1.2"  # åœ¨ä¸»èŠ‚ç‚¹ï¼Œä»èŠ‚ç‚¹ä¸Šéƒ½æ‰§è¡Œ
å¯¹åº”çš„torchç‰ˆæœ¬:torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl
å¯¹åº”çš„vllmç‰ˆæœ¬:vllm-0.3.0-cp310-cp310-manylinux1_x86_64.whl
éƒ¨ç½²vllmçš„æ—¶å€™ä¼šéœ€è¦tritonè¿™ä¸ªä¾èµ–ï¼Œç‰ˆæœ¬æ˜¯:triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl


Qwen/Qwen3-32B 
åœ¨V100-PCIE-16GBä¸Šä½¿ç”¨ollamaå¯ä»¥éƒ¨ç½²Qwen/Qwen3-32Bè¿›è¡Œæ¨ç†ï¼Œä½†æ˜¯æ— æ³•ä½¿ç”¨æ–°ç‰ˆçš„vllméƒ¨ç½²Qwen/Qwen3-32Bã€‚
1. â€‹â€‹FP16ç²¾åº¦éƒ¨ç½²â€‹
æ¨¡å‹è§„æ¨¡	å‚æ•°é‡	æ‰€éœ€æ˜¾å­˜	ä½ çš„é…ç½®æ”¯æŒæƒ…å†µ
Qwen2-32Bâ€‹â€‹	32B	~64GB	âœ… â€‹â€‹æœ€ä½³é€‰æ‹©â€‹â€‹ï¼ˆ4å¡å¹¶è¡Œï¼‰

Qwen2-32Bï¼ˆFP16ï¼‰ - æœ€ä½³å¹³è¡¡â€‹â€‹
# ä½¿ç”¨4å¼  GPU è¿›è¡Œå¼ é‡å¹¶è¡Œ
python -m torch.distributed.run --nproc_per_node=4 --nnodes=3 your_deploy_script.py --model-name Qwen/Qwen2-32B  #ä½¿ç”¨torchåˆ†å¸ƒå¼éƒ¨ç½²Qwen2-32Bçš„å‘½ä»¤å‚è€ƒã€‚

â€‹â€‹æ‰€éœ€æ˜¾å­˜â€‹â€‹:~64GB
â€‹â€‹ä½¿ç”¨GPUâ€‹â€‹:4å¼ V100
â€‹â€‹å‰©ä½™èµ„æºâ€‹â€‹:2å¼ GPUå¤‡ç”¨æˆ–éƒ¨ç½²å…¶ä»–æœåŠ¡

export HF_ENDPOINT=https://hf-mirror.com
huggingface-cli download Qwen/Qwen2.5-14B --local-dir /Data/Qwen/Qwen2.5-14B    # ä¸‹è½½æ¨¡å‹

export HF_ENDPOINT=https://hf-mirror.com
huggingface-cli download Qwen/Qwen2.5-7B --local-dir /Data/Qwen/Qwen2.5-7B      # ä¸‹è½½æ¨¡å‹

export HF_ENDPOINT=https://hf-mirror.com
huggingface-cli download Qwen/Qwen2.5-1.5B --local-dir /Data/Qwen/Qwen2.5-1.5B    # ä½¿ç”¨è¾ƒå°æ¨¡å‹ï¼Œè¿›è¡Œå¤šæœºå¤šå¡çš„åˆ†å¸ƒå¼éƒ¨ç½²

export HF_ENDPOINT=https://hf-mirror.com
huggingface-cli download Qwen/Qwen3-14B --local-dir /Data/Qwen/Qwen3-14B


# å‚æ•°å°çš„æ¨¡å‹èƒ½æ‰§è¡Œå®Œæˆï¼Œå¤§çº¦5åˆ†é’Ÿå®Œæˆï¼Œä½¿ç”¨é gated æ›¿ä»£æ¨¡å‹<200b>ï¼ŒFalcon-7BB (Apache 2.0è®¸å¯)ï¼Œ
# ä»£æ›¿ meta-llama/Llama-2-7bï¼Œå› ä¸º meta-llama/Llama-2-7b åœ¨ huggingface éœ€è¦ç™»å½•ï¼Œè€Œä¸”ç™»å½•ä¹‹åï¼Œä»æ— æ³•ä¸‹è½½ï¼Œé€šè¿‡ huggingface é•œåƒç½‘ç«™ä¹Ÿæ— æ³•ä¸‹è½½
# åˆ›å»ºè¾“å‡ºç›®å½•
export HF_ENDPOINT=https://hf-mirror.com
huggingface-cli download meta-llama/Llama-2-7b --local-dir /Data/meta-llama/Llama-2-7b      # ä¸‹è½½ huggingface ä¸Šçš„ meta-llama æ¨¡å‹æ–‡ä»¶ï¼Œéœ€è¦ç™»å½•ã€‚
huggingface-cli download mistralai/Mistral-7B-v0.1 --local-dir /Data/mistralai/mmistral-7b   # ä½¿ç”¨é gated æ›¿ä»£æ¨¡å‹<200b>ï¼ŒMistral-7B (æ€§èƒ½ä¼˜äºLlama2-7B)
huggingface-cli download tiiuae/falcon-7b --local-dir /Data/tiiuae/falcon-7b                 # ä½¿ç”¨é gated æ›¿ä»£æ¨¡å‹<200b>ï¼ŒFalcon-7B (Apache 2.0è®¸å¯)

export HF_ENDPOINT=https://hf-mirror.com
huggingface-cli download daryl149/llama-2-7b-chat-hf --local-dir /Data/meta-llama/Llama-2-7b
huggingface-cli download daryl149/llama-2-7b-chat-hf --local-dir /Data/llama-2-7b

https://www.modelscope.cn/models/LLM-Research/llama-2-7b  #  ä¸‹è½½ï¼Œllama-2-7b
git clone LLM-Research/llama-2-7b
git clone https://www.modelscope.cn/LLM-Research/llama-2-7b.git


VLLM
# å°è¯•ï¼Œå‚æ•°å‚è€ƒ
# ä½¿ç”¨nvcr.io/nvidia/cuda:12.9.1-cudnn-runtime-ubuntu24.04ï¼Œåœ¨ä¸€ä¸ªcpuèŠ‚ç‚¹ä¸Šè¿è¡ŒvllmæœåŠ¡ã€‚--network hostå’‹ä¸ªæ¨¡å¼ä¸‹ï¼Œä¸éœ€è¦è¦è®¾ç½®-på‚æ•°ï¼Œå‚æ•°å‚è€ƒ
docker run -it -d --network host --cap-add SYS_ADMIN -v /Data/Qwen/Qwen3-32B:/Data/Qwen/Qwen3-32B nvcr.io/nvidia/cuda:12.9.1-cudnn-runtime-ubuntu24.04
a2e72019f4d4
pip install vllm

docker pull hub.rat.dev/vllm/vllm-openai:v0.10.1.1    # V100-PCIE-16GBä¸Šï¼Œç®—åŠ›ä¸åŒ¹é…ï¼Œtorchç‰ˆæœ¬ä¸åŒ¹é…ï¼Œåœ¨æœ¬æ¬¡éƒ¨ç½²ä¸­æ— ç”¨
docker pull m.daocloud.io/vllm/vllm-openai:v0.10.1.1  # V100-PCIE-16GBä¸Šï¼Œç®—åŠ›ä¸åŒ¹é…ï¼Œtorchç‰ˆæœ¬ä¸åŒ¹é…ï¼Œåœ¨æœ¬æ¬¡éƒ¨ç½²ä¸­æ— ç”¨
docker pull m.daocloud.io/vllm/vllm-openai:v0.3.0     # ç®—åŠ› 7.0 çš„æ”¯æŒï¼ŒV100-PCIE-16GBä¸Šç®—åŠ›å’Œtorchç‰ˆæœ¬åŒ¹é…
docker pull hub.rat.dev/vllm/vllm-openai:v0.3.0       # ç®—åŠ› 7.0 çš„æ”¯æŒï¼ŒV100-PCIE-16GBä¸Šç®—åŠ›å’Œtorchç‰ˆæœ¬åŒ¹é…

# è¿è¡Œ vllm æœåŠ¡çš„å‚æ•°å‚è€ƒ
["python3" "-m" "vllm.entrypoints.openai.api_server"]
python3 -m vllm.entrypoints.openai.api_server --model /Data/Qwen/Qwen3-32B \
  --served-model-name Qwen3_32B --host 0.0.0.0 --port 6800 --block-size 16 \
  --pipeline-parallel-size 2 --trust-remote-code --enforce-eager \
  --distributed-executor-backend ray --ray-cluster-address 172.18.6.64:6379

# hhub.rat.dev/vllm/vllm-openai:v0.3.0ï¼Œåœ¨å•ä¸ªèŠ‚ç‚¹ä¸Šï¼Œä½¿ç”¨2ä¸ª V100-PCIE-16GB ï¼Œéƒ¨ç½²
# è¿™ä¸ªæ˜¯åœ¨ GPU èŠ‚ç‚¹ä¸Šå¯ä»¥å¯åŠ¨çš„ï¼Œåœ¨172.18.8.210ä¸Šå¯åŠ¨äº†ï¼Œäº‘ä¸»æœºé…ç½®æ˜¯8Cï¼Œ32Gï¼Œ2ä¸ª V100-PCIE-16GB

# å•æœºåŒå¡ V100-PCIE-16GB éƒ¨ç½²ï¼Œå¯ä»¥æˆåŠŸå¯åŠ¨ Qwen2.5-1.5B ï¼ŒæˆåŠŸå¯åŠ¨:
docker run --rm \
  --gpus all \
  --net=host --pid=host --ipc=host --privileged --shm-size=4G \
  -v /Data/Qwen/Qwen2.5-1.5B:/model \
  -e VLLM_HOST_IP=172.18.8.210 \
  -e PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True \
  -e VLLM_SWAP_SPACE=1 \
  hub.rat.dev/vllm/vllm-openai:v0.3.0 \
  --model /model \
  --tensor-parallel-size 2 \
  --host 0.0.0.0 \
  --dtype half \
  --swap-space 1 \
  --gpu-memory-utilization 0.8 \
  --max-num-batched-tokens 131072 \
  --trust-remote-code


# å•æœºåŒå¡ V100-PCIE-16GB éƒ¨ç½²ï¼ŒåŠ è½½ Qwen2.5-14B æ—¶ï¼ŒæˆåŠŸå¯åŠ¨:
docker run --rm \
  --gpus all \
  --net=host --pid=host --ipc=host --privileged --shm-size=4G \
  -v /Data/Qwen/Qwen2.5-14B:/model \
  -e PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True \
  -e RAY_memory_usage_threshold=0.99 \
  -e RAY_memory_monitor_refresh_ms=0 \
  hub.rat.dev/vllm/vllm-openai:v0.3.0 \
  --model /model \
  --tensor-parallel-size 2 \
  --host 0.0.0.0 \
  --dtype half \
  --swap-space 4 \
  --gpu-memory-utilization 0.995 \
  --max-model-len 400 \
  --max-num-batched-tokens 400 \
  --max-num-seqs 16 \
  --max-paddings 16 \
  --trust-remote-code \
  --disable-custom-all-reduce \
  --enforce-eager


# å•æœºåŒå¡ V100-PCIE-16GB éƒ¨ç½²ï¼ŒåŠ è½½ Qwen2.5-7B æ—¶ï¼ŒæˆåŠŸå¯åŠ¨:
docker run --rm \
  --net=host --pid=host --ipc=host --privileged --shm-size=4G \
  -v /Data/Qwen/Qwen2.5-7B:/model \
  -e PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True \
  -e RAY_memory_usage_threshold=0.99 \
  -e RAY_memory_monitor_refresh_ms=0 \
  hub.rat.dev/vllm/vllm-openai:v0.3.0 \
  --model /model \
  --tensor-parallel-size 2 \
  --host 0.0.0.0 \
  --dtype half \
  --swap-space 4 \
  --gpu-memory-utilization 0.995 \
  --max-model-len 400 \
  --max-num-batched-tokens 400 \
  --max-num-seqs 16 \
  --max-paddings 16 \
  --trust-remote-code \
  --disable-custom-all-reduce


# å•æœºåŒå¡ V100-PCIE-16GB éƒ¨ç½²ï¼ŒåŠ è½½ llama-2-7b æ—¶ï¼ŒæˆåŠŸå¯åŠ¨:
docker run --rm \
  --gpus all \
  --net=host --pid=host --ipc=host --privileged --shm-size=4G \
  -v /Data/llama-2-7b:/model \
  -e VLLM_HOST_IP=172.18.8.208 \
  -e PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True \
  -e VLLM_SWAP_SPACE=1 \
  hub.rat.dev/vllm/vllm-openai:v0.3.0 \
  --model /model \
  --tensor-parallel-size 2 \
  --host 0.0.0.0 \
  --dtype half \
  --swap-space 1 \
  --gpu-memory-utilization 0.8 \
  --max-num-batched-tokens 4096







1. æœåŠ¡å¥åº·çŠ¶æ€æ£€æŸ¥
åœ¨å‘é€å…·ä½“æ¨ç†è¯·æ±‚å‰ï¼Œå¯ä»¥å…ˆç¡®è®¤æœåŠ¡æ˜¯å¦å·²æˆåŠŸå¯åŠ¨å¹¶å‡†å¤‡å°±ç»ªã€‚
vLLM å¥åº·æ£€æŸ¥
curl -v http://172.18.8.210:8000/health

Triton Server å¥åº·æ£€æŸ¥
curl -v http://172.18.8.208:8000/v2/health/ready

Ollama æ¨¡å‹åˆ—è¡¨æ£€æŸ¥
curl http://127.0.0.1:11434/api/tags


# æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯åŠ¨
curl -v http://172.18.8.208:8000/health

# æµ‹è¯•APIè¯·æ±‚
curl http://172.18.8.208:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Hello", "max_tokens": 5000}'  # æµ‹è¯•é•¿åºåˆ—

# æ¨¡å‹åˆ—è¡¨ - éªŒè¯æ¨¡å‹åŠ è½½ï¼ŒæŸ¥è¯¢æ¨¡å‹ä¿¡æ¯
curl http://172.18.8.208:8000/v1/models

# å¯¹è¯ï¼ˆChat Completionï¼‰â€‹ï¼Œè¿™ä¸ªéœ€è¦è®¾ç½®ï¼Œchat_template.jinjaï¼Œå¦åˆ™æ— æ³•è®¿é—®ã€‚
curl http://172.18.8.208:8000/v1/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "/model",
        "messages": [
        {"role": "user", "content": "ä½ å¥½ï¼Œä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"}
        ],
        "max_tokens": 200
    }'

# ç”ŸvLLM æ–‡æœ¬ç”Ÿæˆ (Completions API)
curl http://172.18.8.208:8000/v1/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "/model",
        "prompt": "å¦‚ä½•å­¦ä¹ æ·±åº¦å­¦ä¹ ï¼Ÿ",
        "max_tokens": 150,
        "temperature": 0.8
    }' | jq .  # ç”¨jqç¾åŒ–è¾“å‡º

# ä½¿ç”¨ vLLM APIè®¿é—®:
curl http://172.18.8.208:8000/v1/completions -H "Content-Type: application/json" -d '{
  "model": "/model",
  "prompt": "Hello world",
  "max_tokens": 128
}'


# é—®é¢˜é›†ï¼ŒåŠè§£å†³çº¿ç´¢:
# ä¼šå‡ºç°å¾ˆå¤šæŠ¥é”™ã€‚éœ€è¦ä¸æ–­è°ƒæ•´å‚æ•°ã€‚
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.25 GiB. GPU 0 has a total capacty of 15.77 GiB of which 281.12 MiB is free. 
Including non-PyTorch memory, this process has 15.49 GiB memory in use. 
Of the allocated memory 15.04 GiB is allocated by PyTorch, and 4.72 MiB is reserved by PyTorch but unallocated. 
If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  
See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
(RayWorkerVllm pid=1756) WARNING 09-11 02:01:17 custom_all_reduce.py:44] Custom allreduce is disabled because your platform lacks GPU P2P capability. 
To slience this warning, specifydisable_custom_all_reduce=True explicitly.

# GPU blocks: 0
ValueError: No available memory for the cache blocks.
â†’ æƒé‡æœ¬èº«å·²å æ»¡ 2Ã—V100-16GBï¼ŒKV-cache ä¸€å—éƒ½åˆ†ä¸åˆ°ï¼ˆ0 blocksï¼‰ã€‚

Qwen2.5-14B åŠç²¾åº¦æƒé‡ â‰ˆ 28 GBï¼Œ
2Ã—16 GB = 32 GB â†’ åªå‰© < 4 GB ç”¨äº KV-cacheï¼Œ
å³ä½¿ gpu_memory_utilization=0.98 ä¹Ÿ æ— æ³•å®¹çº³æœ€å° cacheã€‚

# V100-PCIE-16GB ä¸Šï¼Œæ— æ³•éƒ¨ç½² Qwen2.5-32B æ¨¡å‹ã€‚äº‘ä¸»æœºé…ç½®8Cï¼Œ32Gï¼Œ2ä¸ª V100-PCIE-16GBã€‚
# é”™è¯¯ä¿¡æ¯ã€‚
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacty of 15.77 GiB of which 1.76 GiB is free. 
Including non-PyTorch memory, this process has 14.00 GiB memory in use. 
Of the allocated memory 13.51 GiB is allocated by PyTorch, and 29.85 MiB is reserved by PyTorch but unallocated. 
If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
(RayWorkerVllm pid=100969) WARNING 09-10 05:13:58 custom_all_reduce.py:44] Custom allreduce is disabled because your platform lacks GPU P2P capability. To slience this warning, 
specifydisable_custom_all_reduce=True explicitly.

# é”™è¯¯ä¿¡æ¯ã€‚
ValueError: The checkpoint you are trying to load has model type `qwen3` but Transformers does not recognize this architecture. 
This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.

# é”™è¯¯ä¿¡æ¯ã€‚
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacty of 15.77 GiB of which 22.25 MiB is free. 
Including non-PyTorch memory, this process has 15.74 GiB memory in use. 
Of the allocated memory 15.29 GiB is allocated by PyTorch, and 6.66 MiB is reserved by PyTorch but unallocated. 
If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  
See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
(RayWorkerVllm pid=100129) WARNING 09-10 05:09:56 custom_all_reduce.py:44] Custom allreduce is disabled because your platform lacks GPU P2P capability. 
To slience this warning, specifydisable_custom_all_reduce=True explicitly.



æ¨¡å‹è®¿é—®æ§åˆ¶
åœ¨ Triton æˆ– vLLM ä¸­é›†æˆ API å¯†é’¥è®¤è¯ï¼Œä¾‹å¦‚ vllm å¯åŠ¨æ—¶æ·»åŠ ï¼š
vllm serve --model qwen3:14b --api-key my-secret-key




10. ä½¿ç”¨ vllm + ray é›†ç¾¤ï¼Œè¿›è¡Œå¤šæœºå¤šå¡çš„éƒ¨ç½²æµ‹è¯•
ä½¿ç”¨vllm+rayé›†ç¾¤è¿›è¡Œå¤šæœºå¤šå¡çš„éƒ¨ç½²æµ‹è¯•ã€‚3å°GPUæœåŠ¡å™¨ã€‚æ¯ä¸ªGPUæœåŠ¡å™¨é…ç½®ä¸º2ä¸ªV100-PCIE-16GBï¼ŒæœåŠ¡å™¨ä¹‹é—´çš„ç½‘ç»œä¸º10Gã€‚
# Qwen2.5-1.5Bä½¿ç”¨4å¡ï¼Œä¸èƒ½ä½¿ç”¨6å¡ï¼Œå› ä¸ºä¸èƒ½æ•´é™¤ã€‚å¤§çº¦5åˆ†é’Ÿå®ŒæˆåŠ è½½ã€‚
# Qwen2.5-7Bä½¿ç”¨4å¡ï¼Œæœªæµ‹è¯•ã€‚
# Qwen2.5-14Bä½¿ç”¨4å¡ï¼Œæ— æ³•å®Œæˆéƒ¨ç½²ï¼Œå¤§æ¦‚åŠ è½½æ¨¡å‹30åˆ†é’Ÿåã€‚ä¼šå‡ºé”™:ValueError: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine.
# V100-PCIE-16GBä¸Šï¼Œæ— æ³•éƒ¨ç½²Qwen2.5-14Bæ¨¡å‹ã€‚

# åœ¨å‘½ä»¤è¡Œé‡Œå¯åŠ¨ vllm çš„å‘½ä»¤ï¼Œå¯åŠ¨å‘½ä»¤å‚è€ƒ:
VLLM_ALLOW_LONG_MAX_MODEL_LEN=1 \
RAY_ADDRESS=172.18.8.210:6379 \
python3 -m vllm.entrypoints.openai.api_server \
  --model /model \
  --tensor-parallel-size 4 \
  --worker-use-ray \
  --host 0.0.0.0 \
  --dtype half \
  --swap-space 1 \
  --enforce-eager \
  --gpu-memory-utilization 0.8 \
  --max-num-batched-tokens 131072 \
  --trust-remote-code \
  --disable-custom-all-reduce


# åœ¨172.18.8.208è¿™ä¸ªèŠ‚ç‚¹ä¸Šå¯åŠ¨ï¼Œå¹¶è®¾ç½®172.18.8.208å®¹å™¨é‡Œçš„ ray ï¼Œä¸º head èŠ‚ç‚¹:
VLLM_ALLOW_LONG_MAX_MODEL_LEN=1 \
RAY_ADDRESS=172.18.8.208:6379 \
docker run --rm \
  --gpus all \
  --net=host --pid=host --ipc=host --privileged --shm-size=4G \
  -v /Data/llama-2-7b:/model \
  -e PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True \
  -e VLLM_SWAP_SPACE=1 \
  -e RAY_NODE_TYPE=head \
  -e RAY_HEAD_SERVICE_HOST=172.18.8.208 \
  -e RAY_HEAD_SERVICE_PORT=6379 \
  -e RAY_DASHBOARD_PORT=8265 \
  -e VLLM_TENSOR_PARALLEL_SIZE=2 \
  hub.rat.dev/vllm/vllm-openai:v0.6.3 \
  --model /model \
  --tensor-parallel-size 2 \
  --trust-remote-code \
  --host 0.0.0.0 \
  --dtype half \
  --swap-space 1 \
  --gpu-memory-utilization 0.8 \
  --max-num-batched-tokens 4096 \
  --disable-custom-all-reduce \
  --worker-use-ray \
  --enforce-eager


# åœ¨172.18.8.209èŠ‚ç‚¹ä¸Šå¯åŠ¨ï¼Œå¹¶è®¾ç½®172.18.8.209å®¹å™¨é‡Œçš„ ray ï¼Œä¸º work èŠ‚ç‚¹:
VLLM_ALLOW_LONG_MAX_MODEL_LEN=1 \
RAY_ADDRESS=172.18.8.208:6379 \
docker run --rm \
  --gpus all \
  --net=host --pid=host --ipc=host --privileged --shm-size=4G \
  -v /Data/llama-2-7b:/model \
  -e VLLM_HOST_IP=172.18.8.209 \
  -e RAY_HEAD_SERVICE_PORT=6379 \
  -e PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True \
  -e VLLM_SWAP_SPACE=1 \
  -e RAY_NODE_TYPE=worker \
  -e RAY_HEAD_SERVICE_HOST=172.18.8.208 \
  -e RAY_HEAD_SERVICE_PORT=6379 \
  -e RAY_DASHBOARD_PORT=8265 \
  -e VLLM_TENSOR_PARALLEL_SIZE=2 \
  -e RAY_ADDRESS=172.18.8.208:6379 \
  hub.rat.dev/vllm/vllm-openai:v0.6.3 \
  --model /model \
  --tensor-parallel-size 2 \
  --trust-remote-code \
  --host 0.0.0.0 \
  --dtype half \
  --swap-space 1 \
  --gpu-memory-utilization 0.8 \
  --max-num-batched-tokens 4096 \
  --disable-custom-all-reduce \
  --worker-use-ray \
  --enforce-eager


# åœ¨172.18.8.210èŠ‚ç‚¹ä¸Šå¯åŠ¨ï¼Œå¹¶è®¾ç½®172.18.8.210å®¹å™¨é‡Œçš„ ray ï¼Œä¸º work èŠ‚ç‚¹:
VLLM_ALLOW_LONG_MAX_MODEL_LEN=1 \
RAY_ADDRESS=172.18.8.208:6379 \
docker run --rm \
  --gpus all \
  --net=host --pid=host --ipc=host --privileged --shm-size=4G \
  -v /Data/llama-2-7b:/model \
  -e VLLM_HOST_IP=172.18.8.209 \
  -e RAY_HEAD_SERVICE_PORT=6379 \
  -e PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True \
  -e VLLM_SWAP_SPACE=1 \
  -e RAY_NODE_TYPE=worker \
  -e RAY_HEAD_SERVICE_HOST=172.18.8.208 \
  -e RAY_HEAD_SERVICE_PORT=6379 \
  -e RAY_DASHBOARD_PORT=8265 \
  -e VLLM_TENSOR_PARALLEL_SIZE=2 \
  -e RAY_ADDRESS=172.18.8.208:6379 \
  hub.rat.dev/vllm/vllm-openai:v0.6.3 \
  --model /model \
  --tensor-parallel-size 2 \
  --trust-remote-code \
  --host 0.0.0.0 \
  --dtype half \
  --swap-space 1 \
  --gpu-memory-utilization 0.8 \
  --max-num-batched-tokens 4096 \
  --disable-custom-all-reduce \
  --worker-use-ray \
  --enforce-eager




# å‘½ä»¤è¡Œæ€»ç»“
  --max-num-batched-tokens 131072 \ # ä¸éœ€è¦æ·»åŠ 
  --gpu-memory-utilization 0.8 \ 
  -e RAY_ADDRESS="172.18.8.208:10001" \
  -e RAY_REDIS_PASSWORD="your_password" \  # RAYå¯†ç 

vocab_size = 151936 æ— æ³•è¢« 6 æ•´é™¤ï¼Œè€Œä½ ç°åœ¨è¦æ±‚ --tensor-parallel-size 6ã€‚
vLLM çš„ VocabParallelEmbedding å¿…é¡»æ•´é™¤ï¼Œå¦åˆ™ä¼šç›´æ¥æ–­è¨€å¤±è´¥ã€‚

# ä¸æ”¯æŒçš„å‚æ•°
--distributed-executor-backend ray       # å‚æ•°åœ¨vllm-openai:v0.3.0ä¸­ä¸æ”¯æŒã€‚
--enable-chunked-prefill False           # å‚æ•°åœ¨vllm-openai:v0.3.0ä¸­ä¸æ”¯æŒã€‚
--ray-cluster-address 172.18.6.64:6379   # å‚æ•°åœ¨vllm-openai:v0.3.0ä¸­ä¸æ”¯æŒ







11. åˆ†å¸ƒå¼è®­ç»ƒä¸é›†ç¾¤é€šä¿¡ï¼Œæ£€æŸ¥å’Œæµ‹è¯• GPU çš„ nccl é€šä¿¡
åˆ†å¸ƒå¼é€šä¿¡æµ‹è¯• (NCCL)
åœ¨è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒå‰ï¼Œå¿…é¡»ç¡®ä¿èŠ‚ç‚¹é—´GPUçš„é€šä¿¡ï¼ˆç‰¹åˆ«æ˜¯é€šè¿‡10Gç½‘ç»œï¼‰æ˜¯å¥åº·å’Œé«˜æ•ˆçš„ã€‚

https://blog.csdn.net/rjc_lihui/article/details/146154987    nccl-tests è°ƒç”¨å‚æ•° (æ¥è‡ªdeepseek)
https://www.sensecore.cn/help/docs/cloud-foundation/compute/acp/acpBestPractices/Job-nccl_test    ã€NGC é•œåƒã€‘nccl-test é€šä¿¡åº“æ£€æµ‹æœ€ä½³å®è·µ
https://zhuanlan.zhihu.com/p/682530828                 å¤šæœºå¤šå¡è¿è¡Œnccl-testså’Œchannelè·å–
https://cloud.tencent.com/developer/article/2361710          nccl-test ä½¿ç”¨æŒ‡å¼•  

æ£€æŸ¥å’Œæµ‹è¯•GPUçš„ncclé€šä¿¡
æ–¹æ³•1. ä½¿ç”¨nccl-testsé¡¹ç›®æµ‹è¯• NCCL åŸºç¡€åŠŸèƒ½
æ–¹æ³•2. ä½¿ç”¨pythonçš„torch.distributedåº“

1. ä½¿ç”¨ nccl-tests
  åœ¨nvcr.io/nvidia/pytorch:23.10-py3ç­‰åŒ…å«å®Œæ•´CUDAå¼€å‘ç¯å¢ƒçš„å®¹å™¨ä¸­è¿›è¡Œã€‚
  å…‹éš†NVIDIAå®˜æ–¹çš„nccl-testsé¡¹ç›®ï¼Œç¼–è¯‘å¹¶è¿è¡Œæ€§èƒ½æµ‹è¯•è„šæœ¬ï¼Œå¦‚all_reduce_perfã€‚
  è§‚å¯Ÿè¾“å‡ºçš„å¸¦å®½ï¼ˆBus B/Wï¼‰ï¼Œè¯„ä¼°å…¶æ˜¯å¦æ¥è¿‘10Gç½‘ç»œçš„ç†è®ºä¸Šé™ã€‚

2. ä½¿ç”¨ PyTorch Distributed æµ‹è¯•
  ç¼–å†™Pythonè„šæœ¬ï¼Œåˆ©ç”¨torch.distributedåº“åœ¨å¤šä¸ªè¿›ç¨‹/èŠ‚ç‚¹é—´è¿›è¡Œå¼ é‡å¹¿æ’­ã€å½’çº¦ç­‰æ“ä½œã€‚
  è¿™å¯ä»¥æ›´è´´è¿‘å®é™…è®­ç»ƒåœºæ™¯ï¼ŒéªŒè¯PyTorchåˆ†å¸ƒå¼åç«¯çš„é€šä¿¡èƒ½åŠ›ã€‚

# ä½¿ç”¨çš„å®¹å™¨é•œåƒæ˜¯:pytorch:23.10-py3
docker run -it -d --shm-size=4G --gpus all --network host -v /Data:/Data nvcr.io/nvidia/pytorch:23.10-py3
docker run -it -d --shm-size=4G --gpus all --net=bridge -p 2222:22 -v /Data:/Data nvcr.io/nvidia/pytorch:23.10-py3   # éœ€è¦ä½¿ç”¨ç½‘æ¡¥æ–¹å¼ã€‚è®¾ç½®:--net=bridge

apt-get install -y openssh-server
cat >> /etc/ssh/sshd_config <<EOF
PermitRootLogin yes
EOF
/etc/init.d/ssh restart
passwd 


pip install -r requirements.txt

# å®‰è£… ansible.posix é›†åˆ
ansible-galaxy collection install ansible.posix

# å®‰è£…å…¶ä»–Kubesprayå¯èƒ½éœ€è¦çš„é›†åˆ
ansible-galaxy collection install community.general
ansible-galaxy collection install kubernetes.core

# å®‰è£… ansible.utils é›†åˆ
ansible-galaxy collection install ansible.utils

# åŒæ—¶å®‰è£…å…¶ä»–å¯èƒ½ç¼ºå°‘çš„é›†åˆ
ansible-galaxy collection install ansible.posix community.general kubernetes.core

# åœ¨ä¸»èŠ‚ç‚¹ç”Ÿæˆå¯†é’¥
ssh-keygen -t rsa

# å¤åˆ¶å…¬é’¥åˆ°æ‰€æœ‰èŠ‚ç‚¹ï¼ˆåŒ…æ‹¬è‡ªå·±ï¼‰
ssh-copy-id -o StrictHostKeyChecking=no root@172.18.8.208
ssh-copy-id -o StrictHostKeyChecking=no root@172.18.8.209
ssh-copy-id -o StrictHostKeyChecking=no root@172.18.8.210 

åœ¨ä¸»èŠ‚ç‚¹æµ‹è¯• hostfile ä¸­æ‰€æœ‰èŠ‚ç‚¹çš„è¿é€šæ€§:
# éå† hostfile ä¸­çš„èŠ‚ç‚¹ IPï¼Œæµ‹è¯• ping å’Œ SSH è¿æ¥
for ip in 172.18.8.208 172.18.8.209 172.18.8.210; do
  echo "Testing $ip..."
  ping -c 2 $ip  # æµ‹è¯•ç½‘ç»œè¿é€šæ€§
  ssh root@$ip "echo $ip is reachable"  # æµ‹è¯• SSH è¿é€šæ€§
done


# ä½¿ç”¨çš„å®¹å™¨é•œåƒæ˜¯:pytorch:23.10-py3ï¼Œå¹¶ç›´æ¥è¿›å…¥å®¹å™¨:
docker run -it --shm-size=4G --gpus all --network host -v /Data:/Data nvcr.io/nvidia/pytorch:23.10-py3 /bin/bash







11.1 ä½¿ç”¨ nccl-tests é¡¹ç›®æµ‹è¯• NCCL åŸºç¡€åŠŸèƒ½
# å¦‚æœå®¹å™¨ä¸­å·²å®‰è£…cuda
cat >> /etc/profile <<EOF
export PATH=/usr/local/cuda/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}
export LIBRARY_PATH=$LIBRARY_PATH:/usr/local/cuda/lib64
EOF

# ä½¿ç”¨ nccl-tests é¡¹ç›®æµ‹è¯• NCCL åŸºç¡€åŠŸèƒ½
https://github.com/NVIDIA/nccl-tests 
git clone https://github.com/NVIDIA/nccl-tests.git
cd /Data/DEMO/CODE/NCCL/nccl-tests
make -j
./build/all_reduce_perf -b 8 -e 128M -f 2
./build/all_reduce_perf -b 8 -e 128M -f 2 -g 2 -c 0
./build/broadcast_perf -b 128M -e 1G -f 2 -g 2 -c 1

# å¦‚æœå‡ºé”™ï¼Œéœ€è¦é‡æ–°ç¼–è¯‘
make clean

nccl-tests é¡¹ç›®ä¸­çš„å¸¸ç”¨å‚æ•°
å‚æ•°	è¯´æ˜
-b	èµ·å§‹æ•°æ®å¤§å°ï¼ˆä¾‹å¦‚ 128M è¡¨ç¤º 128 MBï¼‰ã€‚
-e	ç»“æŸæ•°æ®å¤§å°ï¼ˆä¾‹å¦‚ 1G è¡¨ç¤º 1 GBï¼‰ã€‚
-f	æ•°æ®å¤§å°çš„å¢é•¿å› å­ï¼ˆä¾‹å¦‚ 2 è¡¨ç¤ºæ¯æ¬¡æµ‹è¯•æ•°æ®å¤§å°ç¿»å€ï¼‰ã€‚
-g	ä½¿ç”¨çš„ GPU æ•°é‡ã€‚
-c	æ£€æŸ¥ç»“æœçš„æ­£ç¡®æ€§ï¼ˆå¯ç”¨æ•°æ®éªŒè¯ï¼‰ã€‚
-n	è¿­ä»£æ¬¡æ•°ï¼ˆé»˜è®¤ 100ï¼‰ã€‚
-w	é¢„çƒ­æ¬¡æ•°ï¼ˆé»˜è®¤ 10ï¼‰ã€‚
-o	é›†åˆæ“ä½œç±»å‹ï¼ˆä¾‹å¦‚ all_reduceã€broadcastã€reduce ç­‰ï¼‰ã€‚
-d	æ•°æ®ç±»å‹ï¼ˆä¾‹å¦‚ floatã€doubleã€int ç­‰ï¼‰ã€‚
-t	çº¿ç¨‹æ¨¡å¼ï¼ˆ0 è¡¨ç¤ºå•çº¿ç¨‹ï¼Œ1 è¡¨ç¤ºå¤šçº¿ç¨‹ï¼‰ã€‚
-a	èšåˆæ¨¡å¼ï¼ˆ0 è¡¨ç¤ºç¦ç”¨ï¼Œ1 è¡¨ç¤ºå¯ç”¨ï¼‰ã€‚
-m	æ¶ˆæ¯å¯¹é½ï¼ˆé»˜è®¤ 0ï¼‰ã€‚
-p	æ‰“å°æ€§èƒ½ç»“æœï¼ˆé»˜è®¤å¯ç”¨ï¼‰ã€‚
-l	æŒ‡å®š GPU åˆ—è¡¨ï¼ˆä¾‹å¦‚ 0,1,2,3 è¡¨ç¤ºä½¿ç”¨ GPU 0ã€1ã€2ã€3ï¼‰ã€‚
-r	æŒ‡å®š rank çš„æ•°é‡ï¼ˆå¤šèŠ‚ç‚¹æµ‹è¯•æ—¶ä½¿ç”¨ï¼‰ã€‚
-s	æŒ‡å®šèŠ‚ç‚¹æ•°é‡ï¼ˆå¤šèŠ‚ç‚¹æµ‹è¯•æ—¶ä½¿ç”¨ï¼‰ã€‚

# æœ¬æ¬¡å®éªŒçš„ç›®å½•:/Data/DEMO/CODE/NCCL/nccl-tests/
cd /Data/DEMO/CODE/NCCL/nccl-tests/
mpirun --allow-run-as-root ./build/all_reduce_perf -b 8 -e 128M -f 2  # ä½¿ç”¨mpirunè¿è¡Œã€‚
mpirun --allow-run-as-root ./build/broadcast_perf -b 128M -e 1G -f 2 -g 2 -c 1  # V100-PCIE-16GB GPUå†…å­˜ä¸è¶³ï¼Œä¼šæŠ¥é”™ã€‚
mpirun --allow-run-as-root ./build/broadcast_perf -b 128M -e 512M -f 2 -g 2 -c 1  # å¯ä»¥å®Œæˆ
mpirun --allow-run-as-root -np 64 -N 8 ./build/all_reduce_perf -b 8 -e 8G -f 2 -g 1 # æœªé€šè¿‡ï¼Œä¼šæŠ¥é”™ã€‚
mpirun --allow-run-as-root -np 3 -N 3 ./build/all_reduce_perf -b 8 -e 8G -f 2 -g 1 # æœªé€šè¿‡ï¼Œä¼šæŠ¥é”™ã€‚

# å…ˆåœ¨åŒä¸€èŠ‚ç‚¹æµ‹è¯•æ˜¯å¦èƒ½è¿è¡Œ
mpirun --allow-run-as-root \
 -np 2 \
 -x NCCL_DEBUG=INFO \
 -x CUDA_VISIBLE_DEVICES=0,1 \
 ./build/all_reduce_perf -b 8 -e 128M -f 2 -g 2 -c 0

# å¤šè¿­ä»£æµ‹è¯•â€‹
# å¢åŠ è¿­ä»£æ¬¡æ•°ä»¥è·å¾—æ›´ç¨³å®šçš„æ€§èƒ½æ•°æ®
mpirun --allow-run-as-root -np 1 ./build/all_reduce_perf -b 1M -e 64M -f 2 -g 2 -i 100
mpirun --allow-run-as-root -np 2 ./build/all_reduce_perf -b 1M -e 64M -f 2 -g 2 -i 100
mpirun --allow-run-as-root ./build/all_reduce_perf -b 1M -e 4M -f 2 -g 2 -i 10

# å‡å°‘é¢„çƒ­è¿­ä»£
mpirun --allow-run-as-root ./build/broadcast_perf -b 1M -e 64M -f 2 -g 2 -w 0 -i 50

# â€‹æ•…éšœæ’é™¤ä¸“ç”¨æµ‹è¯•
# æœ€å°æ•°æ®é‡æµ‹è¯•ï¼ˆæ’é™¤å†…å­˜é—®é¢˜ï¼‰
mpirun --allow-run-as-root ./build/all_reduce_perf -b 1 -e 1 -f 1 -g 2

# å•å­—èŠ‚æµ‹è¯•
mpirun --allow-run-as-root -np 1 ./build/broadcast_perf -b 1 -e 1 -f 1 -g 2
mpirun --allow-run-as-root -np 1 ./build/broadcast_perf -b 1M -e 64M -f 2 -g 2 -w 0 -i 50

mpirun --allow-run-as-root --mca plm_base_verbose 10 -H "192.168.1.11" -np 2 true

mpirun --allow-run-as-root --mca plm_base_verbose 10 -H "192.168.1.12" -np 2 printenv

mpirun --allow-run-as-root --mca plm_base_verbose 10 -H "192.168.1.11,192.168.1.12" -np 2 printenv


# 24.2. mpirun é€‰é¡¹
https://docs.redhat.com/zh-cn/documentation/red_hat_enterprise_linux/8/html/building_running_and_managing_containers/con_the-mpirun-options_assembly_using-podman-in-hpc-environment

ä»¥ä¸‹ mpirun é€‰é¡¹ç”¨äºå¯åŠ¨å®¹å™¨:
--mca orte_tmpdir_base /tmp/podman-mpirun line å‘Šè¯‰ Open MPI åœ¨ /tmp/podman-mpirun ä¸­åˆ›å»ºæ‰€æœ‰ä¸´æ—¶æ–‡ä»¶ï¼Œè€Œä¸æ˜¯åœ¨ /tmp ä¸­åˆ›å»ºã€‚
å¦‚æœä½¿ç”¨å¤šä¸ªèŠ‚ç‚¹ï¼Œåˆ™åœ¨å…¶ä»–èŠ‚ç‚¹ä¸Šè¿™ä¸ªç›®å½•çš„åç§°ä¼šä¸åŒã€‚è¿™éœ€è¦å°†å®Œæ•´çš„ /tmp ç›®å½•æŒ‚è½½åˆ°å®¹å™¨ä¸­ï¼Œè€Œè¿™æ›´ä¸ºå¤æ‚ã€‚

mpirun å‘½ä»¤æŒ‡å®šè¦å¯åŠ¨çš„å‘½ä»¤ï¼ˆ podman å‘½ä»¤ï¼‰ã€‚ä»¥ä¸‹ podman é€‰é¡¹ç”¨äºå¯åŠ¨å®¹å™¨:
run å‘½ä»¤è¿è¡Œå®¹å™¨ã€‚
--env-host é€‰é¡¹å°†ä¸»æœºä¸­çš„æ‰€æœ‰ç¯å¢ƒå˜é‡å¤åˆ¶åˆ°å®¹å™¨ä¸­ã€‚
-v /tmp/podman-mpirun:/tmp/podman-mpirun è¡Œå‘Šè¯‰ Podman æŒ‚è½½ç›®å½•ï¼ŒOpen MPI åœ¨è¯¥ç›®å½•ä¸­åˆ›å»ºå®¹å™¨ä¸­å¯ç”¨çš„ä¸´æ—¶ç›®å½•å’Œæ–‡ä»¶ã€‚
--userns=keep-id è¡Œç¡®ä¿å®¹å™¨å†…éƒ¨å’Œå¤–éƒ¨çš„ç”¨æˆ· ID æ˜ å°„ã€‚
--net=host --pid=host --ipc=host è¡Œè®¾ç½®åŒæ ·çš„ç½‘ç»œã€PID å’Œ IPC å‘½åç©ºé—´ã€‚
mpi-ring æ˜¯å®¹å™¨çš„åç§°ã€‚
/home/ring æ˜¯å®¹å™¨ä¸­çš„ MPI ç¨‹åºã€‚


# å¤šèŠ‚ç‚¹å¯åŠ¨
# ä½¿ç”¨mpirunå¤šèŠ‚ç‚¹è¿è¡Œnccl-testsï¼Œå› ä¸ºç«¯å£é€šè®¯å’Œä¸»æœºåçš„é—®é¢˜æœªå®Œæˆã€‚
1. æŒ‡å®šåŸºç¡€ç«¯å£
mpirun --allow-run-as-root \
  -np 6 \
  -hostfile hostfile \
  -mca btl_tcp_if_include eth0 \
  -x NCCL_SOCKET_IFNAME=eth0 \
  -x NCCL_DEBUG=INFO \
  -x NCCL_PORT=15000 \
  -x CUDA_VISIBLE_DEVICES=0,1 \
  ./build/all_reduce_perf -b 8 -e 128M -f 2 -g 2 -c 0

mpirun --allow-run-as-root \
  -np 6 \
  -hostfile hostfile \
  -mca btl_tcp_if_include enp4s1 \
  -x NCCL_SOCKET_IFNAME=enp4s1 \
  -x NCCL_DEBUG=INFO \
  -x NCCL_PORT=15000 \
  -x CUDA_VISIBLE_DEVICES=0,1 \
  ./build/all_reduce_perf -b 8 -e 128M -f 2 -g 2 -c 0

2. æŒ‡å®šç«¯å£èŒƒå›´
mpirun --allow-run-as-root \
  -np 6 \
  -hostfile hostfile \
  -mca btl_tcp_if_include eth0 \
  -x NCCL_SOCKET_IFNAME=eth0 \
  -x NCCL_DEBUG=INFO \
  -x NCCL_PORT=15000 \              # åŸºç¡€ç«¯å£
  -x NCCL_MIN_PORT=15000 \          # æœ€å°ç«¯å£
  -x NCCL_MAX_PORT=15100 \          # æœ€å¤§ç«¯å£
  -x CUDA_VISIBLE_DEVICES=0,1 \
  ./build/all_reduce_perf -b 8 -e 128M -f 2 -g 2 -c 0

3. åŒæ—¶æŒ‡å®šå¤šä¸ªç›¸å…³ç«¯å£
mpirun --allow-run-as-root \
  -np 6 \
  -hostfile hostfile \
  -mca btl_tcp_if_include eth0 \
  -x NCCL_SOCKET_IFNAME=eth0 \
  -x NCCL_DEBUG=INFO \
  -x NCCL_PORT=15000 \
  -x NCCL_SOCKET_SEND_RECV_PREFIX=tcp \
  -x CUDA_VISIBLE_DEVICES=0,1 \
  ./build/all_reduce_perf -b 8 -e 128M -f 2 -g 2 -c 0

mpirun --allow-run-as-root \
  -np 6 \
  --hostfile hostfile \
  -mca btl_tcp_if_include eth0 \
  -mca btl_tcp_port_min_v4 30000 \
  -mca btl_tcp_port_range_v4 100 \
  -x NCCL_SOCKET_IFNAME=eth0 \
  -x NCCL_DEBUG=INFO \
  -x NCCL_IGNORE_DISABLED_P2P=1 \
  ./build/all_reduce_perf -b 8 -e 128M -f 2 -g 1







11.2 ä½¿ç”¨ python çš„ torch.distributed åº“æµ‹è¯• NCCL åŸºç¡€åŠŸèƒ½
# å¦‚æœæ˜¯dockerå®¹å™¨è¿è¡Œï¼Œéœ€è¦åœ¨ docker å®¹å™¨å¯åŠ¨æ—¶ï¼Œä½¿ç”¨--network host
# 5ä¸ªpythonç¨‹åºï¼Œæµ‹è¯• NCCL åŸºç¡€åŠŸèƒ½
ddp_test.py
ddp_test_0.py
cuda_p2p_test.py
multi_node_nccl_test.py
advanced_nccl_test_0.py

11.2.1 ddp_test.py
python3 ddp_test.py


11.2.2 ddp_test_0.py
python3 ddp_test_0.py


11.2.3 cuda_p2p_test.py
python3 cuda_p2p_test.py


11.2.4 multi_node_nccl_test.py
# å•æœºæµ‹è¯•å•ä¸ª GPU
python3 multi_node_nccl_test.py 0 1 172.18.8.209 29500

# å•æœºæµ‹è¯•å¤šä¸ª GPUï¼ˆ2ä¸ªGPUï¼‰
# ä¸€ä¸ªç»ˆç«¯è¿è¡Œ
python3 multi_node_nccl_test.py 0 2 172.18.8.209 29500
# å¦ä¸€ä¸ªç»ˆç«¯è¿è¡Œ
python3 multi_node_nccl_test.py 1 2 172.18.8.209 29500


11.2.5 advanced_nccl_test_0.py
# å•æœºæµ‹è¯•å•ä¸ª GPU
python3 advanced_nccl_test_0.py 0 1 localhost 12355

# å•æœºæµ‹è¯•å¤šä¸ª GPUï¼ˆ2ä¸ªGPUï¼‰
# ä¸€ä¸ªç»ˆç«¯è¿è¡Œ
python3 advanced_nccl_test_0.py 0 2 localhost 12355
# å¦ä¸€ä¸ªç»ˆç«¯è¿è¡Œ
python3 advanced_nccl_test_0.py 1 2 localhost 12355

# å¤šèŠ‚ç‚¹å¯åŠ¨
# å¤šæœºå¤š GPU æµ‹è¯•ï¼ˆ3èŠ‚ç‚¹ï¼Œæ¯èŠ‚ç‚¹2GPUï¼‰:
èŠ‚ç‚¹1 (172.18.8.208)â€‹â€‹:
python3 advanced_nccl_test_0.py 0 6 172.18.8.208 12355
python3 advanced_nccl_test_0.py 1 6 172.18.8.208 12355

èŠ‚ç‚¹2 (172.18.8.209)â€‹â€‹:
python3 advanced_nccl_test_0.py 2 6 172.18.8.208 12355
python3 advanced_nccl_test_0.py 3 6 172.18.8.208 12355

èŠ‚ç‚¹3 (172.18.8.210)â€‹â€‹:
python3 advanced_nccl_test_0.py 4 6 172.18.8.208 12355
python3 advanced_nccl_test_0.py 5 6 172.18.8.208 12355


NCCL è°ƒä¼˜
è®¾ç½® NCCL_DEBUGã€NCCL_IB_DISABLEã€NCCL_P2P_DISABLEã€NCCL_SOCKET_IFNAME ç­‰å‚æ•°çš„ä½œç”¨ã€‚







11.3. ä½¿ç”¨openmpiçš„ç¼–è¯‘å™¨ï¼Œç¼–è¯‘å’Œè¿è¡Œncclå®˜ç½‘ä¸Šçš„3ä¸ªç¤ºä¾‹
https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/examples.html
https://scc.ustc.edu.cn/zlsc/user_doc/html/mpi-application/mpi-application.html    MPIå¹¶è¡Œç¨‹åºç¼–è¯‘åŠè¿è¡Œ    
https://xflops.sjtu.edu.cn/hpc-start-guide/parallel-computing/mpi/    HPCå…¥é—¨æŒ‡å—


# å®‰è£… openmpi
apt-get update
apt-get install -y infiniband-diags ibverbs-utils libibverbs-dev libfabric1 libfabric-dev libpsm2-dev
apt-get install -y openmpi-bin openmpi-common libopenmpi-dev libgtk2.0-dev
apt-get install -y librdmacm-dev # ä¸éœ€è¦å®‰è£…

åœ¨pytorch:23.10-py3å®¹å™¨é•œåƒå†…ï¼Œåœ¨/opt/hpcx/ompi/ä¸‹å·²ç»å®‰è£…å¥½äº†openmpiã€‚ä¸éœ€è¦è¦æ‰§è¡Œä»¥ä¸Šæ­¥éª¤ã€‚

# openmpi çš„ä¸€äº›å·¥å…·:
mpirun:
mpirunæ˜¯openmpiçš„å‘½ä»¤è¡Œå·¥å…·ï¼Œå®ƒæä¾›äº†ä¸€ç§ç®€å•çš„æ–¹å¼æ¥å¹¶è¡Œå¯åŠ¨åº”ç”¨ç¨‹åºï¼Œä½†æ˜¯å¿…é¡»ä¾èµ–openmpiç¯å¢ƒã€‚
å®ƒå…è®¸åœ¨å¤šä¸ªèŠ‚ç‚¹ä¸ŠåŒæ—¶å¯åŠ¨å¤šä¸ªå¹¶è¡Œåº”ç”¨ç¨‹åºï¼Œæ¯ä¸ªåº”ç”¨ç¨‹åºéƒ½æ˜¯ä»¥è¿›ç¨‹çš„æ–¹å¼è¿è¡Œï¼Œè€Œä¸æ˜¯çº¿ç¨‹ã€‚å¦å¤–ï¼Œmpirunå’Œmpiexecæ˜¯åŒä¸€ä¸ªå·¥å…·ï¼Œç”¨æ³•ç›¸åŒã€‚

mpicc

oshmem_info æ˜¯ OpenSHMEM åº“æä¾›çš„ä¸€ä¸ªå·¥å…·ï¼Œç”¨äºæ˜¾ç¤º SHMEM (Scalable Hierarchical Memory) ç¼–ç¨‹ç¯å¢ƒçš„ä¿¡æ¯ã€‚


# å¯èƒ½å‡ºç°çš„é”™è¯¯æç¤º:
mpirun: symbol lookup error: mpirun: undefined symbol: opal_libevent2022_event_base_loop    # 

æµ‹è¯•ç¼–è¯‘ä¸ªè¿è¡Œhello_mpi.c:
// hello_mpi.c
// check env in terminal: mpicc --version; mpirun --version
// æ£€æŸ¥åº“:ldconfig -p | grep libmpi
// apt-get install openmpi-bin openmpi-common libopenmpi-dev libgtk2.0-dev
// gcc hello_mpi.c -o hello_mpi -I/usr/lib/x86_64-linux-gnu/openmpi/include -L/usr/lib/x86_64-linux-gnu/openmpi/lib -Wl,-rpath,/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi -lm

// å¼ºçƒˆæ¨èä½¿ç”¨æ–¹æ¡ˆ3ï¼ˆmpiccï¼‰ï¼Œå› ä¸ºå®ƒæ˜¯MPIæ ‡å‡†æä¾›çš„ç¼–è¯‘å™¨åŒ…è£…å™¨ï¼Œä¼šè‡ªåŠ¨å¤„ç†æ‰€æœ‰å¿…è¦çš„åŒ…å«è·¯å¾„å’Œåº“é“¾æ¥ã€‚
// apt-get install openmpi-bin libopenmpi-dev
// mpicc hello_mpi.c -o hello_mpi

// mpirun -np 2 --allow-run-as-root ./hello_mpi


https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/examples.html

1. Example 1: Single Process, Single Thread, Multiple Devices
nccl-official-example-1.cu 
# ä¿®æ”¹ä»£ç ä¸­çš„ nDev = 1ï¼Œdevs[0] = {0}
nvcc nccl-official-example-1.cu -o nccl-official-example-1 -lnccl

2. Example 2: One Device per Process or Thread
è¿™æ˜¯ä¸€ä¸ªç»“åˆäº†MPIå’ŒNCCLçš„åˆ†å¸ƒå¼ç¨‹åº:
nccl-official-example-2.cu
nvcc nccl-official-example-2.cu -o nccl-official-example-2 -lnccl -lmpi -L /usr/lib/x86_64-linux-gnu/openmpi/lib -I /usr/lib/x86_64-linux-gnu/openmpi/include   # å‡ºé”™:fatal error: mpi.h: No such file or directory
nvcc nccl-official-example-2.cpp -o nccl-official-example-2 -lnccl -lmpi -L /usr/lib/x86_64-linux-gnu/openmpi/lib -I /usr/lib/x86_64-linux-gnu/openmpi/include  # å‡ºé”™:fatal error: mpi.h: No such file or directory

# æ–¹æ³•1:åŒæ—¶é“¾æ¥ MPI å’Œ NCCL âœ… 
mpicxx -o nccl-official-example-2 nccl-official-example-2.cpp -I/usr/local/cuda/include -L/usr/local/cuda/lib64 -lnccl -lcudart # å¯æ‰§è¡ŒæˆåŠŸ

# æ–¹æ³•2:å¦‚æœä½¿ç”¨ç³»ç»Ÿé»˜è®¤è·¯å¾„
mpicxx -o nccl-official-example-2 nccl-official-example-2.cpp -lnccl -lcudart # å‡ºé”™ï¼Œfatal error: cuda_runtime.h: No such file or directory

# æ–¹æ³•3:æŒ‡å®š CUDA toolkit è·¯å¾„
mpicxx -o nccl-official-example-2 nccl-official-example-2.cpp -I/usr/local/cuda/include -L/usr/local/cuda/lib64 -lnccl -lcudart -L/usr/lib/x86_64-linux-gnu/openmpi/lib -I/usr/lib/x86_64-linux-gnu/openmpi/include # å¯æ‰§è¡ŒæˆåŠŸ


3. Example 3: Multiple Devices per Thread
nvcc nccl-official-example-3.cu -o nccl-official-example-3  -lnccl  -lmpi -L /usr/lib64/mpich-3.2/lib/ -I /usr/include/mpich-3.2-x86_64 # å‡ºé”™ï¼Œfatal error: mpi.h: No such file or directory

mpicxx -o 22.out 22.c -lnccl  -lmpi -L /usr/lib64/mpich-3.2/lib/ -I /usr/include/mpich-3.2-x86_64 -I/usr/local/cuda/include -L/usr/local/cuda/lib64 -lcudart
mpicxx nccl-official-example-2.cu -o nccl-official-example-2 -lnccl  -lmpi -L /usr/lib64/mpich-3.2/lib/ -I /usr/include/mpich-3.2-x86_64 -I/usr/local/cuda/include -L/usr/local/cuda/lib64 -lcudart


mpicxx nccl-official-example-2.cpp -o nccl-official-example-2 \
  -lnccl -lmpi \
  -L/usr/lib64/mpich-3.2/lib -I/usr/include/mpich-3.2-x86_64 \
  -I/usr/local/cuda/include -L/usr/local/cuda/lib64 -lcudart

mpicxx nccl-official-example-3.cpp -o nccl-official-example-3 \
  -lnccl -lmpi \
  -L/usr/lib64/mpich-3.2/lib -I/usr/include/mpich-3.2-x86_64 \
  -I/usr/local/cuda/include -L/usr/local/cuda/lib64 -lcudart

mpicxx nccl-official-example-3.cpp -o nccl-official-example-3 \
  -I/usr/local/cuda/include \
  -I/usr/local/nccl/include \
  -L/usr/local/cuda/lib64 -lcuda -lcudart \
  -L/usr/local/nccl/lib -lnccl \
  -pthread

nvcc nccl-official-example-3.cpp -o nccl-official-example-3 \
  -I/usr/include/openmpi-x86_64 \
  -L/usr/lib64/openmpi/lib \
  -lmpi -lnccl -lcudart -O2




12. V100-PCIE-16GBä¸Šï¼ŒResNet-152 å¯¹ CIFAR-10 æ•°æ®é›†è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ
å‚è€ƒæ–‡æ¡£:
https://blog.csdn.net/qq_41185868/article/details/82793025   Datasetä¹‹CIFAR-10:CIFAR-10æ•°æ®é›†çš„ç®€ä»‹ã€ä¸‹è½½ã€ä½¿ç”¨æ–¹æ³•ä¹‹è¯¦ç»†æ”»ç•¥
https://www.cnblogs.com/mengtao-wang/p/18888373    Pytorchå®æˆ˜-CIFAR-10å›¾åƒåˆ†ç±»
https://zhuanlan.zhihu.com/p/72679537     æ·±åº¦å­¦ä¹ ä¹‹16â€”â€”æ®‹å·®ç½‘ç»œ(ResNet)
https://zhuanlan.zhihu.com/p/353235794?s_r=0      ResNet50ç½‘ç»œç»“æ„å›¾åŠç»“æ„è¯¦è§£
https://blog.csdn.net/Chuck0415/article/details/146167848     ResNet50æ·±åº¦è§£æ:åŸç†ã€ç»“æ„ä¸PyTorchå®ç°

æ®‹å·®ç½‘ç»œ(ResNet)
æ®‹å·®ç½‘ç»œåœ¨è®¾è®¡ä¹‹åˆï¼Œä¸»è¦æ˜¯æœåŠ¡äºå·ç§¯ç¥ç»ç½‘ç»œ(CNN)ï¼Œåœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸåº”ç”¨è¾ƒå¤šï¼Œä½†æ˜¯éšç€CNNç»“æ„çš„å‘å±•ï¼Œåœ¨å¾ˆå¤šæ–‡æœ¬å¤„ç†ï¼Œæ–‡æœ¬åˆ†ç±»é‡Œé¢(n-gram)ï¼Œ
ä¹ŸåŒæ ·å±•ç°å‡ºæ¥å¾ˆå¥½çš„æ•ˆæœã€‚

åˆ†å¸ƒå¼è®­ç»ƒå®æˆ˜:ResNet-152ï¼ŒCIFAR-10
å®Œæ•´çš„ç«¯åˆ°ç«¯ç¤ºä¾‹ï¼Œåœ¨3å°æœåŠ¡å™¨ï¼ˆå…±6ä¸ªGPUï¼‰ä¸Šå¯¹CIFAR-10æ•°æ®é›†è¿›è¡Œé•¿æ—¶é—´çš„åˆ†å¸ƒå¼è®­ç»ƒã€‚

æ¨¡å‹å¼€å‘ä¸ä¼˜åŒ–
æ¨¡å‹ç²¾åº¦ä¸é€Ÿåº¦ä¼˜åŒ–
æ··åˆç²¾åº¦è®­ç»ƒï¼ˆAMPï¼‰
Gradient checkpointing
ZeRO / FSDPï¼ˆåˆ†å¸ƒå¼å†…å­˜ä¼˜åŒ–ï¼‰

åœ¨3å°GPUæœåŠ¡å™¨ä¸Šï¼Œæ¯ä¸ªGPUæœåŠ¡å™¨é…ç½®ä¸º2ä¸ªV100-PCIE-16GBï¼ŒæœåŠ¡å™¨ä¹‹é—´çš„ç½‘ç»œä¸º10Gã€‚å¸®æˆ‘è®¾è®¡ä¸€ä¸ªéœ€è¦24å°æ—¶æ‰èƒ½å®Œæˆçš„åˆ†å¸ƒå¼æ¨¡å‹è®­ç»ƒã€‚ç»™å‡ºæ‰€æœ‰å…·ä½“çš„ç¨‹åºã€‚
ä¸‹é¢æ˜¯ä¸€ä¸ªå…·ä½“çš„åˆ†å¸ƒå¼æ¨¡å‹è®­ç»ƒç¨‹åºç¤ºä¾‹ï¼Œä½¿ç”¨PyTorchä½œä¸ºæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå¹¶ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒã€‚è¿™ä¸ªç¨‹åºå°†å¸®åŠ©æ‚¨åœ¨3å°GPUæœåŠ¡å™¨ä¸Šè¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒã€‚
CIFAR-10æ˜¯ä¸€ä¸ªæ›´æ¥è¿‘æ™®é€‚ç‰©ä½“çš„å½©è‰²å›¾åƒæ•°æ®é›†ã€‚CIFAR-10 æ˜¯ç”±Hinton çš„å­¦ç”ŸAlex Krizhevsky å’ŒIlya Sutskever æ•´ç†çš„ä¸€ä¸ªç”¨äºè¯†åˆ«æ™®é€‚ç‰©ä½“çš„å°å‹æ•°æ®é›†ã€‚ä¸€å…±åŒ…å«10 ä¸ªç±»åˆ«çš„RGB 
å½©è‰²å›¾ç‰‡:é£æœºï¼ˆ airplane ï¼‰ã€æ±½è½¦ï¼ˆ automobile ï¼‰ã€é¸Ÿç±»ï¼ˆ bird ï¼‰ã€çŒ«ï¼ˆ cat ï¼‰ã€é¹¿ï¼ˆ deer ï¼‰ã€ç‹—ï¼ˆ dog ï¼‰ã€è›™ç±»ï¼ˆ frog ï¼‰ã€é©¬ï¼ˆ horse ï¼‰ã€èˆ¹ï¼ˆ ship ï¼‰å’Œå¡è½¦ï¼ˆ truck ï¼‰ã€‚
æ¯ä¸ªå›¾ç‰‡çš„å°ºå¯¸ä¸º32 Ã— 32 ï¼Œæ¯ä¸ªç±»åˆ«æœ‰6000ä¸ªå›¾åƒï¼Œæ•°æ®é›†ä¸­ä¸€å…±æœ‰50000 å¼ è®­ç»ƒå›¾ç‰‡å’Œ10000 å¼ æµ‹è¯•å›¾ç‰‡ã€‚


# ä½¿ç”¨çš„å®¹å™¨é•œåƒæ˜¯:pytorch:23.10-py3
docker run -it -d --shm-size=4G --gpus all --network host -v /Data:/Data nvcr.io/nvidia/pytorch:23.10-py3

# ç¨‹åºæ–‡ä»¶:
cifar10_utils.pyâ€‹
cifar10_train.pyâ€‹
run.sh


ä½¿ç”¨run.shæ¥è¿è¡Œï¼Œå†…å®¹å¦‚ä¸‹:
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
#!/bin/bash
# run.sh

MASTER_ADDR="172.18.8.208" # åœ¨MASTER_ADDRçš„rank0ä¸Šåˆ›å»ºç›®å½•å’Œæ—¥å¿—ï¼Œåªåœ¨rank0è¿›ç¨‹ä¸Šæ‰“å°æ±‡æ€»ç»“æœï¼Œæ¯ä¸ªepochç»“æŸæ—¶ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆåªåœ¨rank0ä¸Šæ‰§è¡Œï¼‰ï¼Œ # è®­ç»ƒç»“æŸåä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼ˆåªåœ¨rank0ä¸Šæ‰§è¡Œï¼‰
MASTER_PORT="29500"
WORLD_SIZE=6
GPU_PER_NODE=2
RANK=$1

# å¯é€‰:è®¾ç½® NCCL å‚æ•°ä¼˜åŒ– 10G ç½‘ç»œ    
export NCCL_SOCKET_IFNAME=enp4s1
export NCCL_DEBUG=INFO

cd /Data/DEMO/CODE/RESNET/;
python3 cifar10_train.py \
  --rank $RANK \
  --world_size $WORLD_SIZE \
  --master_addr $MASTER_ADDR \
  --epochs 100 \
  --batch_size 32 \
  --accumulation_steps 4 \
  --data_dir ./data \
  --save_dir ./checkpoints \
  --log_dir ./logs
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

å¤šèŠ‚ç‚¹å¯åŠ¨:
åœ¨172.18.8.208:
bash /Data/DEMO/CODE/RESNET/run.sh 0 &
bash /Data/DEMO/CODE/RESNET/run.sh 1 &

åœ¨172.18.8.209:
bash /Data/DEMO/CODE/RESNET/run.sh 2 &
bash /Data/DEMO/CODE/RESNET/run.sh 3 &

åœ¨172.18.8.210:
bash /Data/DEMO/CODE/RESNET/run.sh 4 &
bash /Data/DEMO/CODE/RESNET/run.sh 5 &

# æ¯éš”2ç§’åˆ·æ–°ä¸€æ¬¡ï¼Œæ¯æ¬¡åªåœ¨å›ºå®šä½ç½®åˆ·æ–° âœ… 
watch -n 5 -d nvidia-smi

https://www.cnblogs.com/mengtao-wang/p/18888373    Pytorchå®æˆ˜-CIFAR-10å›¾åƒåˆ†ç±»
è®­ç»ƒç­–ç•¥ä¸è¶…å‚æ•°å‚è€ƒ:
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

æŸå¤±å‡½æ•°:äº¤å‰ç†µæŸå¤±æ˜¯å¤šåˆ†ç±»ä»»åŠ¡çš„æ ‡å‡†é€‰æ‹©ï¼Œå¯è¡¡é‡æ¨¡å‹è¾“å‡ºçš„æ¦‚ç‡åˆ†å¸ƒä¸çœŸå®æ ‡ç­¾åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ï¼›
ä¼˜åŒ–å™¨:SGDå…·æœ‰è‰¯å¥½çš„ç¨³å®šæ€§å’Œå¯è§£é‡Šæ€§ï¼›
å­¦ä¹ ç‡è°ƒåº¦:æ¯ 20 ä¸ª epoch è¡°å‡ 10 å€ï¼›
åŠ¨é‡:momentum=0.9 èƒ½åŠ é€Ÿæ”¶æ•›å¹¶æŠ‘åˆ¶éœ‡è¡ã€‚


æ‰©å±•çŸ¥è¯†:
----------------------------------------------------
å“ªäº›æ¨¡å‹è®­ç»ƒæ¡†æ¶æ˜¯ä¸“é—¨ç”¨äºè®­ç»ƒå¤§è¯­è¨€æ¨¡å‹çš„ã€‚
ä»¥ä¸‹æ˜¯ä¸“é—¨ç”¨äºè®­ç»ƒå¤§è¯­è¨€æ¨¡å‹çš„ä¸»è¦æ¡†æ¶:
1. ä¸»æµä¸“ç”¨æ¡†æ¶
Megatron-LM (NVIDIA)  # NVIDIA å¼€å‘çš„é«˜æ•ˆ Transformerè®­ç»ƒæ¡†æ¶
DeepSpeed (Microsoft) # Microsoft çš„æ·±åº¦ä¼˜åŒ–è®­ç»ƒåº“
FairScale (Meta)      # Meta çš„å¯æ‰©å±•è®­ç»ƒåº“
2. ç»¼åˆAIæ¡†æ¶ä¸­çš„LLMæ”¯æŒ
Hugging Face Transformers + Accelerate  # Hugging Face ç”Ÿæ€ç³»ç»Ÿçš„è®­ç»ƒå·¥å…·
PyTorch Lightning                       # ç®€åŒ– PyTorch è®­ç»ƒçš„é«˜çº§æ¡†æ¶
3. ä¸“ä¸šè®­ç»ƒå¹³å°
Colossal-AI   # ä¸­å›½çš„å¼€æºå¤§æ¨¡å‹è®­ç»ƒæ¡†æ¶
Alpa          # Googleå’ŒUC Berkeley åˆä½œçš„è‡ªåŠ¨å¹¶è¡Œè®­ç»ƒç³»ç»Ÿ
4. äº‘å¹³å°ä¸“ç”¨è§£å†³æ–¹æ¡ˆ
AWS SageMaker Model Parallelism    # AWSçš„æ¨¡å‹å¹¶è¡Œè®­ç»ƒæœåŠ¡
Google Vertex AI                   # Google Cloudçš„AIè®­ç»ƒå¹³å°
5. æ–°å…´æ¡†æ¶
FastMoE           # ä¸“æ³¨äºMoEï¼ˆMixture of Expertsï¼‰çš„æ¡†æ¶
Petals            # å»ä¸­å¿ƒåŒ–çš„LLMè®­ç»ƒå’Œæ¨ç†
6. æ¡†æ¶é€‰æ‹©å»ºè®®
åˆå­¦è€…æ¨è:
# Hugging Face + Accelerate
# ä¼˜ç‚¹:æ–‡æ¡£ä¸°å¯Œï¼Œç¤¾åŒºæ´»è·ƒï¼Œæ˜“ä¸Šæ‰‹
pip install transformers accelerate datasets
ä¼ä¸šçº§ç”Ÿäº§ç¯å¢ƒ:
# DeepSpeed æˆ– Megatron-LM
# ä¼˜ç‚¹:æ€§èƒ½ä¼˜åŒ–æè‡´ï¼Œæ”¯æŒè¶…å¤§è§„æ¨¡æ¨¡å‹
# ç¼ºç‚¹:å­¦ä¹ æ›²çº¿é™¡å³­
ç ”ç©¶ç”¨é€”:
# FairScale + PyTorch Lightning
# ä¼˜ç‚¹:çµæ´»æ€§é«˜ï¼Œå®éªŒæ–¹ä¾¿
# ç¼ºç‚¹:éœ€è¦æ›´å¤šæ‰‹åŠ¨è°ƒä¼˜
----------------------------------------------------

æ‰©å±•çŸ¥è¯†:
----------------------------------------------------
å“ªäº›å¸¸ç”¨çš„æ¨¡å‹è®­ç»ƒæ¡†æ¶æ˜¯ç”¨äºè®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ä¹‹å¤–çš„æ¨¡å‹çš„ã€‚
è¿™äº›æ¡†æ¶è¦†ç›–äº†æœºå™¨å­¦ä¹ çš„ä¸»è¦åº”ç”¨é¢†åŸŸ:
è§†è§‰ä»»åŠ¡: Detectron2, MMDetection, YOLO
è¯­éŸ³ä»»åŠ¡: ESPnet, SpeechBrain, Kaldi
æ¨èç³»ç»Ÿ: DeepCTR, RecBole, TFRS
å›¾å­¦ä¹ : PyG, DGL, GraphVite
å¼ºåŒ–å­¦ä¹ : SB3, RLlib, Acme
æ—¶é—´åºåˆ—: GluonTS, TSFresh
å¤šæ¨¡æ€: OpenMMLabç³»åˆ—, MONAI
AutoML: AutoGluon, H2O.ai
é€‰æ‹©æ¡†æ¶æ—¶éœ€è¦è€ƒè™‘ä»»åŠ¡ç±»å‹ã€å›¢é˜ŸæŠ€æœ¯æ ˆã€æ€§èƒ½è¦æ±‚ç­‰å› ç´ ã€‚
----------------------------------------------------






13. V100-PCIE-16GBä¸Šï¼Œä½¿ç”¨ BERT æ¨¡å‹å¯¹ bert-base-uncased æ•°æ®é›†è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ
æ ¸å¿ƒç›®æ ‡
ä»»åŠ¡ç±»å‹:BERT æ¨¡å‹çš„é¢„è®­ç»ƒï¼ˆPretrainingï¼‰ã€‚
æ”¯æŒçš„ä»»åŠ¡:
æ©ç è¯­è¨€å»ºæ¨¡ (MLM):é¢„æµ‹è¢«é®è”½çš„å•è¯ã€‚
åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸï¼ŒBERTï¼ˆBidirectional Encoder Representations from Transformersï¼‰æ¨¡å‹å·²ç»æˆä¸ºä¸€ä¸ªé‡è¦çš„é‡Œç¨‹ç¢‘ã€‚BERT-base-uncasedæ¨¡å‹æ˜¯BERTç³»åˆ—ä¸­çš„ä¸€ä¸ªåŸºç¡€ç‰ˆæœ¬ï¼Œé€‚ç”¨äºè‹±æ–‡æ–‡æœ¬å¤„ç†




13.1 ä½¿ç”¨ BERT æ¨¡å‹å¯¹ bert-base-uncased æ•°æ®é›†è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ
# ä½¿ç”¨çš„å®¹å™¨é•œåƒæ˜¯:pytorch:23.10-py3
docker run -it -d --shm-size=4G --gpus all --ipc=host --network host -v /Data:/Data nvcr.io/nvidia/pytorch:23.10-py3

pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple;
mkdir -p /root/.config/pip;
cat > /root/.config/pip/pip.conf <<EOF
[global]
index-url = https://pypi.tuna.tsinghua.edu.cn/simple
EOF

# æœ¬ç¨‹åºä¾èµ–çš„ç‰ˆæœ¬
pip list | grep -E 'torch|transformers'
pip install transformers==4.30.0 huggingface_hub safetensors==0.6.2 --no-dependencies --find-links=/Data/IMAGES/whl   # åªéœ€è¦å®‰è£…transformersç­‰è¿™å‡ ä¸ªã€‚

# å®‰è£…å®Œæˆä¹‹å:
pip list | grep -E 'torch|transformers'
pytorch-quantization      2.1.2
torch                     2.1.0a0+32f93b1
torch-tensorrt            0.0.0
torchdata                 0.7.0a0
torchtext                 0.16.0a0
torchvision               0.16.0a0
transformers              4.30.0

# ä¿å­˜pythonåŒ…çš„ç‰ˆæœ¬å’Œä¾èµ–:
apt install pipreqs
pipreqs ./ --savepath ../bert-requirements.txt

cat ./bert-requirements.txt # ä¼šå‘ç°torchç‰ˆæœ¬æœ‰å·®å¼‚ï¼Œä½†æ˜¯å¯ä»¥å¿½ç•¥
pytorch-quantization==2.1.2
torch==2.3.1
torch-tensorrt==0.0.0
torchdata==0.7.0a0
torchtext==0.16.0a0
torchvision==0.16.0a0
transformers==4.30.0


# ä¸‹è¼‰æ¨¡å‹æ–‡ä»¶åˆ°æŒ‡å®šç›®éŒ„ï¼Œæ¯ä¸ªä¸»æœºéƒ½è¦æœ‰ä¸€ä»½
export HF_ENDPOINT=https://hf-mirror.com
huggingface-cli download bert-base-uncased --local-dir /Data/DEMO/MODEL/bert-base-uncased

# å¦‚æœå‡ºç°:
âš ï¸  Warning: 'huggingface-cli download' is deprecated. Use 'hf download' instead.

# ä¸‹è½½æ¨¡å‹æ–‡ä»¶
wget https://hf-mirror.com/bert-base-uncased/resolve/main/pytorch_model.bin -P /Data/DEMO/MODEL/bert-base-uncased
wget https://hf-mirror.com/bert-base-uncased/resolve/main/config.json -P /Data/DEMO/MODEL/bert-base-uncased
wget https://hf-mirror.com/bert-base-uncased/resolve/main/vocab.txt -P /Data/DEMO/MODEL/bert-base-uncased

# bert_train.pyä¸­ï¼Œä»æœ¬åœ°åŠ è½½
tokenizer = BertTokenizer.from_pretrained("./models/bert-base-uncased")
model = BertModel.from_pretrained("./models/bert-base-uncased")

scp -r * 172.18.8.209:/Data/DEMO/CODE/BERT/
scp -r * 172.18.8.210:/Data/DEMO/CODE/BERT/

# å‚è€ƒæ–‡æ¡£
https://pytorch.ac.cn/docs/2.5/elastic/run.html#google_vignette    # torchrun çš„æ–‡æ¡£
torchrun æ˜¯ä¸€ä¸ª python æ§åˆ¶å°è„šæœ¬ï¼ŒæŒ‡å‘ torch.distributed.run çš„ä¸»æ¨¡å—ï¼Œè¯¥æ¨¡å—åœ¨ setup.py ä¸­çš„ entry_points é…ç½®ä¸­å£°æ˜ã€‚å®ƒç­‰æ•ˆäºè°ƒç”¨ python -m torch.distributed.runã€‚
https://blog.csdn.net/u013172930/article/details/148519788    # ã€PyTorchã€‘torchrun:åˆ†å¸ƒå¼è®­ç»ƒçš„å¯åŠ¨å’Œç®¡ç†å‘½ä»¤è¡Œå·¥å…·
https://blog.csdn.net/Komach/article/details/130765773        #  å…³äºé›†ç¾¤åˆ†å¸ƒå¼torchrunå‘½ä»¤è¸©å‘è®°å½•ï¼ˆè‡ªç”¨ï¼‰


# æ¨¡å‹æ–‡ä»¶å’Œç¨‹åº:
bert_dataset.py
bert_model.py
bert_train.py


# ä¸»èŠ‚ç‚¹ä¸Šæµ‹è¯•è¿è¡Œ:172.18.8.208
export OMP_NUM_THREADS=4
export NCCL_DEBUG=INFO
export TORCH_DISTRIBUTED_DEBUG=DETAIL
torchrun \
    --nnodes=1 \
    --nproc_per_node=2 \
    --rdzv_id=100 \
    --rdzv_backend=static \
    --master_addr=172.18.8.208 \
    --master_port=29500 \
    bert_train.py --config config.yaml


# å¤šèŠ‚ç‚¹å¯åŠ¨ (åœ¨ä¸»èŠ‚ç‚¹è¿è¡Œ) 172.18.8.208
torchrun \
  --nnodes=3 \
  --nproc_per_node=2 \
  --node_rank=0 \
  --master_addr=172.18.8.208 \
  --master_port=29500 \
  bert_train.py \
  --config config.yaml

# å¤šèŠ‚ç‚¹å¯åŠ¨ï¼Œ172.18.8.209
torchrun \
  --nnodes=3 \
  --nproc_per_node=2 \
  --node_rank=1 \
  --master_addr=172.18.8.208 \
  --master_port=29500 \
  bert_train.py \
  --config config.yaml

# å¤šèŠ‚ç‚¹å¯åŠ¨ï¼Œ172.18.8.210
torchrun \
  --nnodes=3 \
  --nproc_per_node=2 \
  --node_rank=2 \
  --master_addr=172.18.8.208 \
  --master_port=29500 \
  bert_train.py \
  --config config.yaml




13.2 ä½¿ç”¨ BERT æ¨¡å‹å¯¹ bert-base-uncased æ•°æ®é›†è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒï¼Œæ·»åŠ  TensorBoard
ä½¿ç”¨ TensorBoard å¯è§†åŒ–ä½ çš„ BERT åˆ†å¸ƒå¼è®­ç»ƒè¿‡ç¨‹ï¼Œéœ€é€šè¿‡ TensorBoard æ—¥å¿—å†™å…¥ã€åˆ†å¸ƒå¼ç¯å¢ƒé€‚é…ï¼ˆä»…ä¸»èŠ‚ç‚¹å†™æ—¥å¿—ï¼‰ã€å…³é”®æŒ‡æ ‡è¿½è¸ªï¼ˆæŸå¤±ã€å­¦ä¹ ç‡ã€GPU å†…å­˜ç­‰ï¼‰ä¸‰ä¸ªæ ¸å¿ƒæ­¥éª¤å®ç°ã€‚
ç¡®ä¿ tensorboard å’Œ torch.utils.tensorboard å·²å®‰è£…:
pip install tensorboard  # TensorBoard æ ¸å¿ƒåº“
# PyTorch 2.1.0 å·²å†…ç½® torch.utils.tensorboardï¼Œæ— éœ€é¢å¤–å®‰è£…,âœ… 

# æ¨¡å‹æ–‡ä»¶å’Œç¨‹åº:
bert_dataset.py
bert_model.py
bert_train.py

ï¼ˆ1ï¼‰å¯¼å…¥ TensorBoard ç›¸å…³æ¨¡å—
åœ¨æ–‡ä»¶é¡¶éƒ¨å¯¼å…¥ SummaryWriterï¼ˆTensorBoard æ—¥å¿—å†™å…¥å·¥å…·ï¼‰:
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
import torch
import torch.nn as nn
# ... å…¶ä»–åŸæœ‰å¯¼å…¥ ...
from torch.utils.tensorboard import SummaryWriter  # æ–°å¢:å¯¼å…¥ TensorBoard å†™å…¥å™¨
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

ï¼ˆ2ï¼‰åˆå§‹åŒ– TensorBoard æ—¥å¿—å™¨ï¼ˆä»…ä¸»èŠ‚ç‚¹
åœ¨ main() å‡½æ•°ä¸­ï¼Œä¸»èŠ‚ç‚¹ï¼ˆrank 0ï¼‰åˆ›å»º SummaryWriterï¼ˆæŒ‡å®šæ—¥å¿—ä¿å­˜è·¯å¾„ï¼‰ï¼Œä»èŠ‚ç‚¹ä¸åˆ›å»ºï¼ˆé¿å…å†²çªï¼‰ã€‚ä¿®æ”¹ä½ç½®åœ¨ã€Œè‡ªåŠ¨åˆ›å»ºä¿å­˜ç›®å½•ã€ä¹‹å:
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
def main():
    # ... åŸæœ‰ä»£ç :è§£æå‚æ•°ã€åŠ è½½ configã€åˆå§‹åŒ–åˆ†å¸ƒå¼ã€åˆ›å»ºä¿å­˜ç›®å½• ...

    # æ–°å¢:åˆå§‹åŒ– TensorBoard æ—¥å¿—å™¨ï¼ˆä»…ä¸»èŠ‚ç‚¹ï¼‰
    writer = None  # åˆå§‹åŒ– writer ä¸º None
    if rank == 0:
        # æ—¥å¿—ä¿å­˜è·¯å¾„:./tensorboard_logs/[å½“å‰æ—¶é—´]ï¼ˆé¿å…å¤šæ¬¡è®­ç»ƒæ—¥å¿—æ··æ·†ï¼‰
        tensorboard_log_dir = os.path.join("./tensorboard_logs", datetime.now().strftime("%Y%m%d_%H%M%S"))
        os.makedirs(tensorboard_log_dir, exist_ok=True)
        writer = SummaryWriter(log_dir=tensorboard_log_dir)  # åˆ›å»ºæ—¥å¿—å†™å…¥å™¨
        logger.info(f"TensorBoard æ—¥å¿—å·²ä¿å­˜è‡³:{tensorboard_log_dir}")

    # ... åç»­ä»£ç :åˆå§‹åŒ–æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€æ•°æ®åŠ è½½ ...
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

ï¼ˆ3ï¼‰è®­ç»ƒè¿‡ç¨‹ä¸­å†™å…¥å…³é”®æŒ‡æ ‡
åœ¨ train_epoch() å‡½æ•°ä¸­ï¼Œæ¯æ¬¡è¿­ä»£ / æ¯ N æ­¥ å°†å…³é”®æŒ‡æ ‡å†™å…¥ TensorBoardã€‚éœ€ä¿®æ”¹ train_epoch() å‡½æ•°ï¼Œæ·»åŠ  writer å’Œ global_stepï¼ˆå…¨å±€æ­¥æ•°ï¼Œç”¨äº TensorBoard æ¨ªåæ ‡ï¼‰å‚æ•°:
æ­¥éª¤ 3.1:ä¿®æ”¹ train_epoch() å‡½æ•°å®šä¹‰
å¢åŠ  writer å’Œ global_step å‚æ•°:
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
def train_epoch(
    model, dataloader, optimizer, scheduler, criterion, scaler, 
    epoch, rank, world_size, logger, accumulation_steps,
    writer=None, global_step=None  # æ–°å¢:TensorBoard ç›¸å…³å‚æ•°
):
    model.train()
    total_loss = 0.0
    total_steps = len(dataloader)
    # ... åŸæœ‰ä»£ç :åˆå§‹åŒ–è¿›åº¦æ¡ã€ä¼˜åŒ–å™¨æ¸…é›¶ ...
æ­¥éª¤ 3.2:è¿­ä»£ä¸­å†™å…¥æŒ‡æ ‡ï¼ˆæ¯ 50 æ­¥ï¼Œä¸è¿›åº¦æ¡æ›´æ–°åŒæ­¥ï¼‰
åœ¨ train_epoch() å‡½æ•°çš„è¿­ä»£å¾ªç¯ä¸­ï¼Œæ·»åŠ æŒ‡æ ‡å†™å…¥é€»è¾‘ï¼ˆä»…ä¸»èŠ‚ç‚¹æ‰§è¡Œï¼‰:
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
def train_epoch(...):
    # ... åŸæœ‰ä»£ç :è¿­ä»£éå† dataloader ...

    for step, batch in enumerate(pbar):
        # ... åŸæœ‰ä»£ç :æ•°æ®è¿ç§»åˆ° GPUã€å‰å‘è®¡ç®—ã€æŸå¤±è®¡ç®— ...

        # åŸæœ‰ä»£ç :è¿›åº¦æ¡æ›´æ–°ï¼ˆrank 0ï¼‰
        if rank == 0 and step % 50 == 0:
            current_lr = optimizer.param_groups[0]['lr']
            mem_used = torch.cuda.max_memory_allocated() / (1024 ** 3)  # GPU å†…å­˜ï¼ˆGBï¼‰
            pbar.set_postfix({
                'mlm_loss': f"{mlm_loss.item():.4f}",
                'nsp_loss': f"{nsp_loss.item():.4f}" if nsp_loss != 0 else "N/A",
                'lr': f"{current_lr:.2e}",
                'mem_used': f"{mem_used:.2f}GB"
            })

            # æ–°å¢:å†™å…¥ TensorBoard æŒ‡æ ‡ï¼ˆä»…ä¸»èŠ‚ç‚¹ï¼Œæ¯ 50 æ­¥ï¼‰
            if writer is not None and global_step is not None:
                writer.add_scalar("Train/MLM_Loss", mlm_loss.item(), global_step)  # MLM æŸå¤±
                writer.add_scalar("Train/NSP_Loss", nsp_loss.item(), global_step)  # NSP æŸå¤±
                writer.add_scalar("Train/Total_Loss", (mlm_loss + nsp_loss).item(), global_step)  # æ€»æŸå¤±
                writer.add_scalar("Train/Learning_Rate", current_lr, global_step)  # å­¦ä¹ ç‡
                writer.add_scalar("Train/GPU_Memory_GB", mem_used, global_step)  # GPU å†…å­˜ä½¿ç”¨
                global_step += 1  # å…¨å±€æ­¥æ•°é€’å¢

    # ... åŸæœ‰ä»£ç :è®¡ç®—å¹³å‡æŸå¤±ã€åˆ†å¸ƒå¼æŸå¤±åŒæ­¥ ...

    # æ–°å¢:æ¯ä¸ª Epoch ç»“æŸåï¼Œå†™å…¥å¹³å‡æŸå¤±ï¼ˆå¯é€‰ï¼Œè¡¥å…… Epoch çº§æŒ‡æ ‡ï¼‰
    if rank == 0 and writer is not None:
        writer.add_scalar("Train/Average_Loss_Per_Epoch", avg_loss, epoch)
    
    return avg_loss, global_step  # æ–°å¢:è¿”å›æ›´æ–°åçš„ global_step


ï¼ˆ4ï¼‰åœ¨ main() ä¸­è°ƒç”¨ train_epoch() å¹¶ä¼ é€’å‚æ•°
åœ¨ main() å‡½æ•°çš„è®­ç»ƒå¾ªç¯ä¸­ï¼Œåˆå§‹åŒ– global_stepï¼ˆå…¨å±€æ­¥æ•°ï¼Œåˆå§‹ä¸º 0ï¼‰ï¼Œå¹¶å°† writer å’Œ global_step ä¼ é€’ç»™ train_epoch():
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
def main():
    # ... åŸæœ‰ä»£ç :åˆå§‹åŒ–æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€æ•°æ®åŠ è½½ ...

    # æ–°å¢:åˆå§‹åŒ–å…¨å±€æ­¥æ•°ï¼ˆç”¨äº TensorBoard æ¨ªåæ ‡ï¼Œæ¯ 50 æ­¥é€’å¢ï¼‰
    global_step = 0
    best_loss = float('inf')

    # è®­ç»ƒå¾ªç¯
    for epoch in range(config['training']['epochs']):
        sampler.set_epoch(epoch)
        
        # ä¿®æ”¹:è°ƒç”¨ train_epoch() æ—¶ä¼ é€’ writer å’Œ global_stepï¼Œå¹¶æ¥æ”¶æ›´æ–°åçš„ global_step
        avg_loss, global_step = train_epoch(
            model, dataloader, optimizer, scheduler, criterion, scaler,
            epoch, rank, world_size, logger, config['training']['accumulation_steps'],
            writer=writer, global_step=global_step  # ä¼ é€’ TensorBoard å‚æ•°
        )

        # ... åŸæœ‰ä»£ç :ä¿å­˜æ¨¡å‹ã€æ—¥å¿—æ‰“å° ...

    # æ–°å¢:è®­ç»ƒç»“æŸåå…³é—­ TensorBoard å†™å…¥å™¨ï¼ˆé‡Šæ”¾èµ„æºï¼‰
    if rank == 0 and writer is not None:
        writer.close()
        logger.info("TensorBoard æ—¥å¿—å™¨å·²å…³é—­")

    # ... åŸæœ‰ä»£ç :æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ ...
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++



# å¤šèŠ‚ç‚¹å¯åŠ¨ (åœ¨ä¸»èŠ‚ç‚¹è¿è¡Œ) 172.18.8.208
torchrun \
  --nnodes=3 \
  --nproc_per_node=2 \
  --node_rank=0 \
  --master_addr=172.18.8.208 \
  --master_port=29500 \
  bert_train_tensorboard.py \
  --config config.yaml

# å¤šèŠ‚ç‚¹å¯åŠ¨ï¼Œ172.18.8.209
torchrun \
  --nnodes=3 \
  --nproc_per_node=2 \
  --node_rank=1 \
  --master_addr=172.18.8.208 \
  --master_port=29500 \
  bert_train_tensorboard.py \
  --config config.yaml

# å¤šèŠ‚ç‚¹å¯åŠ¨ï¼Œ172.18.8.210
torchrun \
  --nnodes=3 \
  --nproc_per_node=2 \
  --node_rank=2 \
  --master_addr=172.18.8.208 \
  --master_port=29500 \
  bert_train_tensorboard.py \
  --config config.yaml


å¯åŠ¨ TensorBoard æœåŠ¡
# æ ¼å¼:tensorboard --logdir=æ—¥å¿—ä¿å­˜è·¯å¾„ --port=ç«¯å£å·ï¼ˆé¿å…ç«¯å£å†²çªï¼‰
export TENSORBOARD_HOST=0.0.0.0
tensorboard --logdir=./tensorboard_logs --port=6006

# å¤§çº¦7å°æ—¶å®Œæˆæ¨¡å‹è®­ç»ƒã€‚
anhua208:6888:44995 [0] NCCL INFO comm 0x555e22e0ddc0 rank 1 nranks 6 cudaDev 1 busId d0 - Abort COMPLETE
[2025-09-30 22:04:05] __main__ INFO: [Rank 0] ä¿å­˜æœ€ä½³æ¨¡å‹ - æŸå¤±: 10.8564
[2025-09-30 22:04:05] __main__ INFO: [Rank 0] è®­ç»ƒå®Œæˆï¼
[2025-09-30 22:04:05] __main__ INFO: [Rank 0] æœ€ä½³æŸå¤±: 10.8564
[2025-09-30 22:04:05] __main__ INFO: [Rank 0] TensorBoard æ—¥å¿—å™¨å·²å…³é—­
anhua208:6887:6917 [0] NCCL INFO [Service thread] Connection closed by localRank 0
anhua208:6887:45022 [0] NCCL INFO comm 0x560b56bc7700 rank 0 nranks 6 cudaDev 0 busId c0 - Abort COMPLETE



æ‰©å±•çŸ¥è¯†:
----------------------------------------------------
é™¤äº†TensorBoardï¼Œè¿˜æœ‰å¾ˆå¤šå…¶ä»–æ–¹æ³•å¯ä»¥å¯¹æ¨¡å‹è®­ç»ƒè¿‡ç¨‹è¿›è¡Œå¯è§†åŒ–å’Œç›‘æ§:
1. Weights & Biases (W&B)
2. MLflow
3. Comet.ml
4. Neptune.ai
5. è‡ªå®šä¹‰Matplotlibå®æ—¶ç»˜å›¾
6. Plotly + Dash å®æ—¶ç›‘æ§é¢æ¿
7. Visdom (Facebook)
8. è‡ªå®šä¹‰æ—¥å¿—æ–‡ä»¶ + å¤–éƒ¨å·¥å…·
9. ä½¿ç”¨Pandas + Jupyterè¿›è¡Œäº¤äº’å¼åˆ†æ
10. Prometheus + Grafana (ç”Ÿäº§ç¯å¢ƒ)
11. Sacred + Omniboard
12. ç®€å•çš„è¿›åº¦æ¡ + æŒ‡æ ‡æ˜¾ç¤º

é€‰æ‹©å»ºè®®:
ç ”ç©¶/å®éªŒé˜¶æ®µ: W&B, TensorBoard, MLflow
ç”Ÿäº§ç¯å¢ƒ: Prometheus + Grafana, è‡ªå®šä¹‰ç›‘æ§é¢æ¿
å¿«é€ŸåŸå‹: Matplotlibå®æ—¶ç»˜å›¾, tqdmè¿›åº¦æ¡
å›¢é˜Ÿåä½œ: W&B, MLflow, Neptune
èµ„æºå—é™: ç®€å•çš„æ—¥å¿—è®°å½• + å¤–éƒ¨åˆ†æ
----------------------------------------------------






14. triton-inference-server éƒ¨ç½²
https://blog.csdn.net/u013171226/article/details/148792425    Triton serverçš„éƒ¨ç½²ã€æ„å»ºã€backendæ’ä»¶æœºåˆ¶æ•´ä½“ä»‹ç»
https://zhuanlan.zhihu.com/p/634444666                        æ¨¡å‹æ¨ç†æœåŠ¡åŒ–æ¡†æ¶Tritonä¿å§†å¼æ•™ç¨‹ï¼ˆä¸‰ï¼‰:å¼€å‘å®è·µ
https://www.jianshu.com/p/e4ff723b101a                        AIæ¨¡å‹éƒ¨ç½²:ä¸€æ–‡æå®šTriton Inference Serverçš„å¸¸ç”¨åŸºç¡€é…ç½®å’ŒåŠŸèƒ½ç‰¹æ€§
https://www.jianshu.com/p/b59860ce0fe9                        saved_model è½¬ tensorrt çš„ plan æ¨¡å‹

æ‰©å±•çŸ¥è¯†:
----------------------------------------------------
NVIDIA Triton Inference Serveræ˜¯ä¸ºå¤§è§„æ¨¡éƒ¨ç½²è®¾è®¡çš„é«˜æ€§èƒ½æ¨ç†æœåŠ¡å™¨
V100é€‚é…çš„Tritonå®¹å™¨ç‰ˆæœ¬ä¸ºnvcr.io/nvidia/tritonserver:23.09-py3ï¼Œå…¶å†…ç½®TensorRT 8.6.1ã€‚
TensorRT åŠ é€Ÿï¼Œåœ¨ V100 ä¸Šç”¨ TensorRT å¯¹ Stable Diffusionã€Whisperã€LLaMA ç­‰æ¨¡å‹åšæ¨ç†åŠ é€Ÿã€‚
å¯åŠ¨Triton SDKå®¹å™¨nvcr.io/nvidia/tritonserver:23.09-py3-sdkç”¨äºå‘é€å®¢æˆ·ç«¯è¯·æ±‚ã€‚

GPUå‹å·ä¸º:V100-PCIE-16GB
åˆ™ç®—åŠ›ä¸º:7.0
åˆ™é€‚é…çš„CUDAç‰ˆæœ¬ä¸º:cuda_11.8.0_520.61.05_linux.run    # è¿™ä¸ªå®‰è£…åŒ…é‡Œå¸¦GPUé©±åŠ¨ã€‚
åˆ™TensorRTç‰ˆæœ¬ä¸º:TensorRT version: 8.6.1ã€‚
trtexecç‰ˆæœ¬ä¸º:TensorRT v8601
tritonserverç‰ˆæœ¬ä¸º:server_version:2.39.0

# è®©å¤§è¯­è¨€æ¨¡å‹æé—®éƒ¨ç½²ç»™å‡ºéƒ¨ç½²æ–¹æ¡ˆå’Œç¨‹åº:
åœ¨3å° GPU æœåŠ¡å™¨ï¼Œæ¯ä¸ª GPU æœåŠ¡å™¨é…ç½®ä¸º2ä¸ª V100-PCIE-16GB ï¼Œå¦‚ä½•ä½¿ç”¨ tensorrt å’Œ triton å¯¹ yolov13x.pt æ¨¡å‹è¿›è¡Œåˆ†å¸ƒå¼éƒ¨ç½²ã€‚
----------------------------------------------------

14.1. ç¯å¢ƒå‡†å¤‡
éœ€è¦å®¹å™¨é•œåƒä¸º:nvcr.io/nvidia/tritonserver:23.09-py3
docker run -it -d --shm-size=4G --name tritonserver --gpus all --network host -v /Data:/Data nvcr.io/nvidia/tritonserver:23.09-py3
èƒ½åœ¨å®¹å™¨ä¸­ç›´æ¥è¿è¡Œ:tritonserver

# triton-inference-server åœ¨ github ä¸Šçš„å®˜ç½‘ã€‚
https://github.com/triton-inference-server/server
Release 2.38.0 corresponding to NGC container 23.09  # è¯´æ˜triton-inference-serverå¯¹åº”çš„å®¹å™¨é•œåƒç‰ˆæœ¬ï¼Œtritonserver:23.09-py3
https://github.com/triton-inference-server/server/archive/refs/tags/v2.38.0.tar.gz  # ä¸‹è½½é“¾æ¥

https://www.cnblogs.com/zzk0/p/15932542.html    # æˆ‘ä¸ä¼šç”¨ Triton ç³»åˆ—:å‘½ä»¤è¡Œå‚æ•°ç®€è¦ä»‹ç»
https://zhuanlan.zhihu.com/p/574146311          # æ·±åº¦å­¦ä¹ éƒ¨ç½²ç¥å™¨â€”â€”triton inference serverå…¥é—¨æ•™ç¨‹æŒ‡åŒ—
https://zhuanlan.zhihu.com/p/21172600328        # ä½¿ç”¨ triton éƒ¨ç½²æ¨¡å‹
https://blog.csdn.net/qq_41664845/article/details/125529197    # Tritonéƒ¨ç½²Torchå’ŒOnnxæ¨¡å‹ï¼Œé›†æˆæ•°æ®é¢„å¤„ç†

# è¿™é‡Œæ˜¯å®˜æ–¹çš„æ­¥éª¤:
++++++++++++++++++++++++++++++++++++++++++++++++++++++++ 
# Step 1: Create the example model repository
git clone -b r25.08 https://github.com/triton-inference-server/server.git  # å®˜æ–¹æ•™ç¨‹ï¼Œæœ¬æ¬¡éƒ¨ç½²ä¸ä½¿ç”¨è¿™ä¸ªç‰ˆæœ¬ã€‚
cd server/docs/examples
./fetch_models.sh

server-2.38.0 # å®é™…ç”¨çš„æ˜¯è¿™ä¸ªç‰ˆæœ¬:
éœ€è¦ä¸‹è½½2ä¸ªæ–‡ä»¶:
https://storage.googleapis.com/download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz # è¿™ä¸ªèƒ½ç›´æ¥ä¸‹ã€‚
mv /tmp/inception_v3_2016_08_28_frozen.pb model_repository/inception_graphdef/1/model.graphdef
https://contentmamluswest001.blob.core.windows.net/content/14b2744cf8d6418c87ffddc3f3127242/9502630827244d60a1214f250e3bbca7/08aed7327d694b8dbaee2c97b8d0fcba/densenet121-1.2.onnx # è¿™ä¸ªä¸‹è½½ä¸äº†ã€‚
å¾—åˆ°:model_repository/inception_graphdef/1/model.graphdef

https://github.com/onnx/models # æ›¿ä»£æ–¹æ³•ï¼Œä»è¿™é‡Œæ‰¾ densenet121 å¯¹åº”çš„ç‰ˆæœ¬ã€‚
https://github.com/onnx/models/tree/main/validated/vision/classification/densenet-121   # ä¸‹è½½åœ°å€
https://github.com/onnx/models/blob/main/validated/vision/classification/densenet-121/model/densenet-7.onnx
cp densenet-7.onnx densenet121-1.2.onnx;
å¾—åˆ°:model_repository/densenet_onnx/1/model.onnx
++++++++++++++++++++++++++++++++++++++++++++++++++++++++

++++++++++++++++++++++++++++++++++++++++++++++++++++++++   è¿™é‡Œæ˜¯å®˜ç½‘æ–‡æ¡£è¯´æ˜:
# Step 2: Launch triton from the NGC Triton container
docker run --gpus=1 --rm --net=host -v ${PWD}/model_repository:/models nvcr.io/nvidia/tritonserver:25.08-py3 tritonserver --model-repository=/models --model-control-mode explicit --load-model densenet_onnx

# Step 3: Sending an Inference Request
# In a separate console, launch the image_client example from the NGC Triton SDK container
docker run -it --rm --net=host nvcr.io/nvidia/tritonserver:23.09-py3-sdk /workspace/install/bin/image_client -m densenet_onnx -c 3 -s INCEPTION /workspace/images/mug.jpg

# æµ‹è¯•è®¿é—®æœåŠ¡
# è¿è¡Œå‘½ä»¤:
/workspace/install/bin/image_client -m densenet_onnx -c 3 -s INCEPTION -u http://172.18.8.208:8000 /workspace/images/mug.jpg

# Inference should return the following
Image '/workspace/images/mug.jpg':
    15.346230 (504) = COFFEE MUG
    13.224326 (968) = CUP
    10.422965 (505) = COFFEEPOT
++++++++++++++++++++++++++++++++++++++++++++++++++++++++ è¿™é‡Œæ˜¯å®˜ç½‘æ–‡æ¡£è¯´æ˜


# è¿™é‡Œæ˜¯æœ¬æ¬¡è¯•éªŒçš„å®é™…æ­¥éª¤:
++++++++++++++++++++++++++++++++++++++++++++++++++++++++    è¿™é‡Œæ˜¯æœ¬æ¬¡è¯•éªŒçš„å®é™…æ­¥éª¤:
cd /Data/DEMO/CODE/server-2.38.0/docs/examples;
# åœ¨172.18.8.208çš„tritonserver:23.09-py3å®¹å™¨å†…å¯ä»¥è¿è¡Œï¼Œä¾èµ–/opt/tritonserverä¸‹çš„å¾ˆå¤šåº“ã€‚
tritonserver --model-repository=./model_repository --model-control-mode explicit --load-model densenet_onnx


curl -v 172.18.8.208:8000/v2/health/ready # éªŒè¯tritonserverå·²æ­£å¸¸è¿è¡Œ
curl -v 172.18.8.209:8000/v2/health/ready

# æ£€æŸ¥æ¨¡å‹å…ƒæ•°æ®
curl http://172.18.8.208:8000/v2/models/densenet_onnx

# ç”¨åŸå§‹ HTTP è¯·æ±‚:æœªæµ‹é€šã€‚
curl -X POST http://172.18.8.208:8000/v2/models/densenet_onnx/infer \
  -H "Content-Type: application/json" \
  -d '{
    "inputs": [{
      "name": "input_0",
      "shape": [1, 3, 224, 224],
      "datatype": "FP32",
      "data": [1.jpg]
    }]
  }'


# ä½¿ç”¨: nvcr.io/nvidia/tritonserver:23.09-py3-sdk æµ‹è¯•è®¿é—®:
# åœ¨GPUæœåŠ¡å™¨ä¸Šè¿è¡Œï¼Œå¯¹åº”çš„å®¹å™¨ç‰ˆæœ¬:
docker run -it -d --shm-size=4G --name tritonserver-sdk --gpus all --network host -v /Data:/Data nvcr.io/nvidia/tritonserver:23.09-py3-sdk

# è¿›å…¥å®¹å™¨ï¼Œè¿è¡Œå‘½ä»¤:
/workspace/install/bin/image_client -m densenet_onnx -c 3 -s INCEPTION -u http://172.18.8.208:8000 /workspace/images/mug.jpg

# å¯ä»¥æ‰¾ä¸€ä¸ª CPU æœåŠ¡å™¨è¿è¡Œ:
docker run -it -d --shm-size=4G --name tritonserver-sdk --network host -v /Data:/Data nvcr.io/nvidia/tritonserver:23.09-py3-sdk

# éªŒè¯æœåŠ¡æ˜¯å¦å¯åŠ¨æˆåŠŸï¼ˆæŸ¥çœ‹æ—¥å¿—æ˜¯å¦æœ‰"Successfully loaded model"ï¼‰
curl http://172.18.8.210:8000/v2/models/sd_pipeline                # è‹¥è¿”å›æ¨¡å‹ä¿¡æ¯ï¼Œè¯´æ˜æœåŠ¡æ­£å¸¸
curl -v http://172.18.8.210:8000/v2/health/ready                   # è‹¥è¿”å›æ¨¡å‹ä¿¡æ¯ï¼Œè¯´æ˜æœåŠ¡æ­£å¸¸

curl http://172.18.8.208:8000/v2/models/yolov13                    # è‹¥è¿”å›æ¨¡å‹ä¿¡æ¯ï¼Œè¯´æ˜æœåŠ¡æ­£å¸¸
curl -v http://172.18.8.208:8000/v2/health/ready                   # è‹¥è¿”å›æ¨¡å‹ä¿¡æ¯ï¼Œè¯´æ˜æœåŠ¡æ­£å¸¸

# ä¸‹è½½è‹±ä¼Ÿè¾¾å®˜æ–¹é•œåƒ
https://catalog.ngc.nvidia.com/



triton-inference-server éƒ¨ç½² yolov13x
è¿›è¡Œæ¨¡å‹è½¬æ¢ï¼Œ
pytorch æ ¼å¼çš„æ¨¡å‹ï¼Œè½¬æ¢æˆ onnx æ ¼å¼ï¼Œå†è½¬æ¢æˆ

python3 touch2onnx.py
# æç¤º:ModuleNotFoundError: No module named 'ultralytics' 
# è§£å†³æ–¹æ³•:
pip install ultralytics

# æç¤º:AttributeError: module 'cv2.dnn' has no attribute 'DictValue'
# è§£å†³æ–¹æ³•:
pip install opencv-python opencv-contrib-python opencv-python-headless

pip install onnxruntime==1.15.1 ultralytics==8.3.213
pip install opencv-python==4.9.0.80 opencv-contrib-python==4.9.0.80 opencv-python-headless==4.9.0.80
pip install opencv-python==4.12.0.88 opencv-contrib-python==4.12.0.88 opencv-python-headless==4.12.0.88
pip install opencv==4.8.1 opencv-python==4.8.1.78 opencv-contrib-python==4.8.1.78 opencv-python-headless==4.8.1.78

# æç¤º:ImportError: numpy.core.multiarray failed to import
pip install numpy==1.26.4

pip list |grep -E 'transformers|ultralytics|numpy|opencv|polars|polars_runtime|onnxruntime|ultralytics'
numpy                     1.26.4
opencv                    4.7.0
opencv-contrib-python     4.12.0.88
opencv-python             4.12.0.88
opencv-python-headless    4.12.0.88
polars                    1.34.0
polars-runtime-32         1.34.0
transformers              4.30.0
ultralytics               8.3.213
ultralytics-thop          2.0.17

pip install opencv-python==4.10.0.84
pip install opencv-python-headless







15. Milvus
1. Milvus
Milvus Milvus æ˜¯ä¸€ä¸ªä¼ä¸šçº§å‘é‡æ•°æ®åº“ï¼Œæ”¯æŒå¤§è§„æ¨¡æ•°æ®é›†å’Œé«˜å¹¶å‘åœºæ™¯ã€‚å®ƒé‡‡ç”¨åˆ†å¸ƒå¼æ¶æ„ï¼Œæ”¯æŒå¤šç§ç´¢å¼•ï¼ˆå¦‚ HNSW å’Œ IVFï¼‰ï¼Œå¹¶æä¾›æ¯«ç§’çº§å»¶è¿Ÿã€‚é€‚åˆéœ€è¦é«˜æ€§èƒ½å’Œæ‰©å±•æ€§çš„åº”ç”¨ã€‚
https://hub.docker.com/r/milvusdb/milvus/   # hub.docker.comä¸Šçš„åœ°å€
docker pull milvusdb/milvus:v2.6.3-gpu  # è¿™ä¸ªæ˜¯GPUç‰ˆçš„ï¼Œ3.04 GB
docker pull hub.rat.dev/milvusdb/milvus:v2.6.3      # è¿™ä¸ªæ˜¯CPUç‰ˆçš„ï¼Œ1.02 GB
docker pull hub.rat.dev/milvusdb/milvus:v2.5.15      # è¿™ä¸ªæ˜¯CPUç‰ˆçš„ï¼Œ1.02 GB   

Milvus å®˜ç½‘è¯´æ˜åœ°å€:
https://milvus.io/docs/zh/install_standalone-docker.md

Milvus è¿›è¡Œå®¹å™¨éƒ¨ç½²æ—¶ï¼Œéƒ¨ç½²åœ¨GPUä¸Šæœ‰ä»€ä¹ˆå¥½å¤„ï¼Ÿ # åœ¨å¤§è¯­è¨€æ¨¡å‹é‡Œæé—®ã€‚

# å‚è€ƒèµ„æ–™:
https://blog.csdn.net/lsb2002/article/details/132222947    ä¸ºAIè€Œç”Ÿçš„æ•°æ®åº“:Milvusè¯¦è§£åŠå®æˆ˜
https://zhuanlan.zhihu.com/p/634255317   Milvus å®Œæ•´æŒ‡å—:å¼€æºå‘é‡æ•°æ®åº“ï¼ŒAI åº”ç”¨å¼€å‘çš„åŸºç¡€è®¾æ–½ï¼ˆé€è¡Œè§£é‡Šä»£ç ï¼Œå°ç™½é€‚ç”¨ï¼‰
https://www.milvus-io.com/getstarted/standalone/install_standalone-helm   ç”¨ Kubernetes å®‰è£…ç‹¬ç«‹è¿è¡Œçš„ Milvus


wget https://github.com/milvus-io/milvus/releases/download/v2.5.15/milvus-standalone-docker-compose.yml -O docker-compose.yml        # æœ¬æ¬¡æµ‹è¯•ä½¿ç”¨è¿™ä¸ªç‰ˆæœ¬
wget https://github.com/milvus-io/milvus/releases/download/v2.6.0-rc1/milvus-standalone-docker-compose-gpu.yml -O docker-compose.yml # è¿™ä¸ªæ˜¯ç›®å‰æœ€æ–°ç‰ˆæœ¬

# éœ€è¦ä½¿ç”¨ docker-compose
apt install -y docker-compose
docker-compose up -d
docker-compose ps

# è®¾ç½®ä»£ç†ä¸‹è½½:
export http_proxy=http://192.168.1.2:7890;
export https_proxy=https://192.168.1.2:7890;

# docker é•œåƒ

docker pull minio/minio:RELEASE.2023-03-20T20-16-18Z
docker pull milvusdb/milvus:v2.2.11

docker pull dhub.kubesre.xyz/milvusdb/milvus:v2.2.11
docker pull dhub.kubesre.xyz/minio/minio:RELEASE.2023-03-20T20-16-18Z

docker tag dhub.kubesre.xyz/milvusdb/milvus:v2.2.11 milvusdb/milvus:v2.2.11
docker tag dhub.kubesre.xyz/minio/minio:RELEASE.2023-03-20T20-16-18Z minio/minio:RELEASE.2023-03-20T20-16-18Z

WARN[0000] /Data/BBC/Milvus/docker-compose.yml: `version` is obsolete
docker ps 
CONTAINER ID   IMAGE                                                  COMMAND                  CREATED             STATUS                       PORTS                                                                                          NAMES
69deaf344a2a   hub.rat.dev/milvusdb/milvus:v2.5.15                    "/tini -- milvus runâ€¦"   About an hour ago   Up About an hour (healthy)   0.0.0.0:9091->9091/tcp, [::]:9091->9091/tcp, 0.0.0.0:19530->19530/tcp, [::]:19530->19530/tcp   milvus-standalone
217102be5b70   hub.rat.dev/minio/minio:RELEASE.2024-05-28T17-19-04Z   "/usr/bin/docker-entâ€¦"   About an hour ago   Up About an hour (healthy)   0.0.0.0:9000-9001->9000-9001/tcp, [::]:9000-9001->9000-9001/tcp                                milvus-minio
e5634dc58a3a   quay.io/coreos/etcd:v3.5.18                            "etcd -advertise-cliâ€¦"   About an hour ago   Up About an hour (healthy)   2379-2380/tcp                                                                                  milvus-etcd

# æµ‹è¯• Milvus æ•°æ®åº“çš„è¿æ¥å’Œè¯»å†™:
python3 hello_milvus.py

# Python å¯¼å…¥ Milvus å‘é‡æ•°æ®:
pip install pymilvus sentence_transformers




æ‰©å±•çŸ¥è¯†:
----------------------------------------------------
ä½¿ç”¨ Milvus Operator åœ¨ Kubernetes ä¸­è¿è¡Œ Milvus:
https://milvus.io/docs/zh/install_cluster-milvusoperator.md?tab=helm

----------------------------------------------------



2. ä½¿ç”¨ attu è®¿é—® Milvus
# ä½¿ç”¨attuè®¿é—®Milvus
docker pull docker.chenby.cn/zilliz/attu
docker pull dhub.kubesre.xyz/zilliz/attu:v2.2.6

docker tag dhub.kubesre.xyz/zilliz/attu:v2.2.6 zilliz/attu:v2.2.6
docker pull dhub.kubesre.xyz/zilliz/attu:v2.4.4


# è¿è¡Œ Attu å®¹å™¨çš„å‚è€ƒå‘½ä»¤:
docker run -d \
  --name attu \
  -p 18000:3000 \
  -e MILVUS_URL=host.docker.internal:19530 \
  --add-host=host.docker.internal:host-gateway \
  zilliz/attu:v2.3.8

# å¦‚æœæ— æ³•ç›´æ¥ä¸‹è½½å®¹å™¨é•œåƒï¼Œä¹Ÿå¯ä»¥é€šè¿‡æ·»åŠ  daemon.json æ–‡ä»¶é‡Œçš„ registry-mirrors ï¼Œæ¥è§£å†³ç›´æ¥ä¸‹è½½çš„é—®é¢˜:
mkdir -p /etc/docker
tee /etc/docker/daemon.json <<EOF
{
    "registry-mirrors": [
        "https://docker.anyhub.us.kg",
        "https://dockerhub.icu",
        "https://docker.awsl9527.cn"
    ]
}
EOF
systemctl daemon-reload
systemctl restart docker

http://192.168.1.103:8000/
zilliz:zilliz

æ‰©å±•çŸ¥è¯†:
----------------------------------------------------
Milvus ä¸ Weaviate è¯¦ç»†æ¯”è¾ƒæŠ¥å‘Š    https://www.syndataworks.cn/blog/blog/2024-10-29-milvus-vs-weaviate/
Docker:Dockeréƒ¨ç½²Milvuså‘é‡æ•°æ®åº“ä¸å¯è§†åŒ–ç•Œé¢Attuï¼ˆå¯¹æ¥æœ¬åœ°MinioæœåŠ¡ï¼‰    https://www.cnblogs.com/nhdlb/p/18839349
å¿«é€Ÿé–‹å§‹ä½¿ç”¨ Attu Desktop    https://milvus.io/docs/zh-hant/quickstart_with_attu.md
ä½¿ç”¨Pythonè¿è¡ŒMilvus         https://zhuanlan.zhihu.com/p/617972545
ä½¿ç”¨ Python è¿è¡ŒMilvus    https://geekdaxue.co/read/milvus-docs-v2/site-zh-CN-getstarted-example_code.md
----------------------------------------------------



16. Weaviate

Weaviate Weaviate æ”¯æŒæ··åˆæœç´¢ï¼ˆå‘é‡+å…³é”®è¯ï¼‰å’Œæ¨¡å—åŒ–æ‰©å±•ï¼Œé€‚åˆä¸­å°è§„æ¨¡æ•°æ®é›†ã€‚å®ƒæä¾› GraphQL æŸ¥è¯¢æ¥å£ï¼Œæ”¯æŒæ°´å¹³æ‰©å±•å’Œå¤šç§Ÿæˆ·æ¶æ„ã€‚
Weaviate å®˜ç½‘è¯´æ˜åœ°å€:
https://docs.weaviate.io/deploy/installation-guides/docker-installation       # åœ¨ Docker ä¸­è¿è¡Œ Milvus (Linux)

docker run -p 8080:8080 -p 50051:50051 cr.weaviate.io/semitechnologies/weaviate:1.33.0    # å®˜æ–¹æ¨èçš„å‘½ä»¤è¡Œ

docker pull cr.weaviate.io/semitechnologies/weaviate:1.33.0   # å®˜ç½‘æ˜¾ç¤ºçš„è¿æ¥ï¼Œä½†æ— æ³•ä¸‹è½½
docker pull cr.weaviate.io/semitechnologies/weaviate:1.25.0   # å®˜ç½‘æ˜¾ç¤ºçš„è¿æ¥ï¼Œä½†æ— æ³•ä¸‹è½½

docker pull hub.rat.dev/cr.weaviate.io/semitechnologies/weaviate:1.25.0   # æ— æ³•ä¸‹è½½

docker pull hub.rat.dev/semitechnologies/weaviate:1.25.0    # å¯ä»¥ä¸‹è½½ï¼Œé•œåƒå¾ˆå°åªæœ‰ï¼Œ120Må¤§å°
docker pull hub.rat.dev/semitechnologies/weaviate:1.33.0    # å¯ä»¥ä¸‹è½½ï¼Œé•œåƒå¾ˆå°åªæœ‰ï¼Œ120Må¤§å°

docker run -it -d --shm-size=4G --ipc=host --network host -v /Data:/Data hub.rat.dev/semitechnologies/weaviate:1.25.0   # å¯ä»¥å¯åŠ¨ï¼Œä½†æ˜¯ï¼Œæ–°ç‰ˆçš„weaviate.Clientä¸æ”¯æŒã€‚
docker run -it -d --shm-size=4G --ipc=host --network host -v /Data:/Data hub.rat.dev/semitechnologies/weaviate:1.33.0   # ä½¿ç”¨è¿™ä¸ªç‰ˆæœ¬çš„weaviateå®¹å™¨é•œåƒã€‚

docker exec -it 4203bb50eb91f88a082ed22c09e862b7bc705f6137e0b9908097cdb14ae1edac /bin/sh  # Weaviate çš„ç»ˆç«¯æ˜¯ /bin/sh

# ç¼–å†™ python ç¨‹åºï¼Œ Weaviate_Client_v4.py æµ‹è¯• Weaviate çš„å®¹å™¨å¯åŠ¨æ˜¯å¦æ­£å¸¸:
# æç¤º:ModuleNotFoundError: No module named 'weaviate'
pip install weaviate-client numpy
pip list |grep -E 'weaviate-client|numpy'
numpy                 2.3.4
weaviate-client       4.17.0

# ç®€å•æµ‹è¯• Weaviate æ˜¯å¦å¯è¾¾:
python3 Weaviate_Client_v4.py

# æµ‹è¯•è¯»å†™æ•°æ®:
python3 Weaviate_test_v4_fixed.py

# ä»¥ä¸‹æ˜¯åŠ å…¥äº† â€‹â€‹æ›´å¤šæµ‹è¯•ç”¨ä¾‹â€‹â€‹ å’Œ â€‹â€‹æ€§èƒ½è®¡æ—¶åŠŸèƒ½â€‹â€‹ çš„ â€‹â€‹å®Œæ•´ç¨‹åºä»£ç â€‹â€‹ï¼ŒåŸºäºæ‚¨æä¾›çš„ Weaviate_test_v4_fixed.py è¿›è¡Œäº†æ‰©å±•ã€‚
# æˆ‘ä»¬åœ¨æ¯ä¸ªå…³é”®æ“ä½œï¼ˆå¦‚æ’å…¥ã€æŸ¥è¯¢ã€æ›´æ–°ã€æ‰¹é‡æ“ä½œã€å‘é‡æœç´¢ç­‰ï¼‰å‰åæ·»åŠ äº† time.time()è®¡æ—¶ï¼Œå¹¶è¾“å‡ºè€—æ—¶åŠç›¸å…³çš„æ€§èƒ½æŒ‡æ ‡ï¼ˆå¦‚æ¯ç§’æ“ä½œæ•°ã€æŸ¥è¯¢å“åº”æ—¶é—´ç­‰ï¼‰ã€‚
python3 Weaviate_test_v4_with_performance.py

æ‰©å±•çŸ¥è¯†:
----------------------------------------------------
Weaviate è¿›è¡Œå®¹å™¨éƒ¨ç½²æ—¶ï¼Œéƒ¨ç½²åœ¨GPUä¸Šæœ‰ä»€ä¹ˆå¥½å¤„ï¼Ÿ    # åœ¨å¤§è¯­è¨€æ¨¡å‹é‡Œæé—®ã€‚

Triton + TensorRT Ensemble
æ¼”ç¤ºå¦‚ä½•æŠŠå¤šä¸ªæ¨¡å‹ï¼ˆå‰å¤„ç† â†’ ä¸»æ¨¡å‹ â†’ åå¤„ç†ï¼‰ä¸²èµ·æ¥ã€‚

æ¨¡å‹çƒ­æ›´æ–°ä¸A/Bæµ‹è¯•
åœ¨ Triton æˆ– vLLM ä¸­çƒ­æ›´æ–°æƒé‡ï¼Œåšå¯¹æ¯”æµ‹è¯•ã€‚

æ¨¡å‹å‹ç¼©ä¸é‡åŒ–
INT8 / FP16 æ¨ç†ã€‚
----------------------------------------------------





17. NVIDIA Nsight Compute å’Œ NVIDIA Nsight Systems



NVIDIA Nsight Computeï¼Œcuda-toolkit é™„å¸¦çš„å·¥å…·ã€‚
# æŸ¥çœ‹Nsight-Computeæ”¯æŒçš„sections
ncu --list-sections

NVIDIA Nsight Systemsï¼Œcuda-toolkit é™„å¸¦çš„å·¥å…·ã€‚
# åˆ—å‡ºå½“å‰æ‰€æœ‰æ´»åŠ¨çš„æ€§èƒ½åˆ†æä¼šè¯ã€‚
nsys sessions list

# å«å¤šç§å¤æ‚æ¨¡å¼:å¤šæ ¸æ ¸å‡½æ•°ã€å…±äº«å†…å­˜çš„çŸ©é˜µä¹˜æ³•ã€å¹¶è¡Œå½’çº¦ã€åŸå­æ“ä½œã€æµï¼ˆstreamsï¼‰ä¸å¼‚æ­¥æ‹·è´ã€é¡µé”å®šä¸»æœºå†…å­˜ã€NVTX åŒºæ®µæ ‡æ³¨ç­‰
# ä½¿ç”¨ nsight-systems (nsys) ä¸ nsight-compute (ncu) çš„å‘½ä»¤è¡Œé‡‡æ ·/åˆ†æå‘½ä»¤ï¼ˆå«å¸¸ç”¨é€‰é¡¹ã€å¯¼å‡º CSV/HTML çš„æ–¹å¼ï¼‰
# å¦‚ä½•ä» nsys æ—¶é—´çº¿ä¸ ncu æ€§èƒ½æ•°æ®ä¸­æ‰¾å‡ºæ€§èƒ½ç“¶é¢ˆçš„å…·ä½“æ£€æŸ¥ç‚¹ä¸ç¤ºä¾‹è§£é‡Š
# æ³¨:ä¸‹é¢çš„ç¨‹åºè‡ªè¶³ï¼ˆåªä¾èµ– CUDA SDKï¼‰ã€‚è¦å¯ç”¨ NVTX æ ‡æ³¨ï¼Œéœ€è¦ CUDA çš„ nvToolsExtï¼ˆé€šå¸¸éš CUDA toolkit ä¸€èµ·æä¾›ï¼‰ã€‚å¦‚æœä½ çš„ç¯å¢ƒæ²¡æœ‰ nvToolsExtï¼Œå¯æŠŠ NVTX ç›¸å…³è¡Œæ³¨é‡Šæ‰ï¼Œç¨‹åºä»èƒ½å·¥ä½œã€‚
ç¨‹åºæ–‡ä»¶:
complex_cuda.cu
complex_cuda_0.cu
matrix_add_profiling.cu 

1. ç¼–è¯‘:

# ç¤ºä¾‹ç¼–è¯‘ï¼ˆå¦‚æœè¦å¯ç”¨ NVTXï¼‰:
make NVCCFLAGS="-O3 -arch=sm_80 -lineinfo -DUSE_NVTX"    # ç¼–è¯‘ä¸é€šè¿‡ã€‚
# åœ¨ nvcr.io/nvidia/pytorch:23.10-py3 å®¹å™¨ä¸­çš„æŠ¥é”™:
# é”™è¯¯æç¤º:tmpxft_00001304_00000000-6_complex_cuda.cudafe1.cpp:(.text.startup+0x1e1): undefined reference to `nvtxRangePushA'

# åœ¨ nvcr.io/nvidia/pytorch:25.09-py3 å®¹å™¨ä¸­çš„æŠ¥é”™:
# complex_cuda.cu:9:10: fatal error: nvToolsExt.h: No such file or directory
find /usr -name "nvToolsExt.h" 2>/dev/null

# ä½¿ç”¨å®é™…è·¯å¾„æ›¿æ¢
nvcc -O3 -arch=sm_80 -lineinfo \
-I/usr/local/cuda-13.0/targets/x86_64-linux/include/nvtx3/nvToolsExt.h \
-L/usr/local/cuda/lib64 \
-DUSE_NVTX -o complex_cuda complex_cuda.cu -lnvToolsExt

# ä½¿ç”¨å®é™…è·¯å¾„æ›¿æ¢
nvcc -O3 -arch=sm_80 -lineinfo \
-I/usr/local/cuda-13.0/targets/x86_64-linux/include/nvtx3/nvToolsExt.h \
-L/usr/local/cuda/lib64 \
-DUSE_NVTX -o complex_cuda_0 complex_cuda_0.cu -lnvToolsExt


make NVCCFLAGS="-O3 -arch=sm_80 -lineinfo"  # ç¼–è¯‘é€šè¿‡ï¼Œä½†æ˜¯ç¼ºå°‘ NVTX ã€‚

# nvcc é“¾æ¥ NVTX çš„æ–¹å¼ï¼ˆåŠ ä¸Š -lnvToolsExtï¼‰:
nvcc -O3 -arch=sm_80 -lineinfo -DUSE_NVTX -o complex_cuda complex_cuda.cu -lnvToolsExt     # ç¼–è¯‘é€šè¿‡
nvcc -O3 -arch=sm_80 -lineinfo -DUSE_NVTX -o complex_cuda_0 complex_cuda_0.cu -lnvToolsExt     # ç¼–è¯‘é€šè¿‡

-lnvToolsExt:é“¾æ¥ NVTX åº“ã€‚
-O3:å¯ç”¨ä¼˜åŒ–ï¼Œä½¿æ€§èƒ½åˆ†ææœ‰æ„ä¹‰ã€‚
-lineinfo:ç”Ÿæˆè¡Œä¿¡æ¯ï¼Œä»¥ä¾¿ nsight-compute å¯ä»¥å°†æŒ‡æ ‡æ˜ å°„å›æºä»£ç è¡Œã€‚

# è°ƒè¯•ï¼Œå¦‚éœ€è¿›ä¸€æ­¥æ’æŸ¥ libnvToolsExt.so è·¯å¾„ï¼Œå¯ä»¥æ‰§è¡Œ:
ls /usr/local/cuda/lib64/libnvToolsExt*

# å¸¸è§é—®é¢˜ä¸æ³¨æ„äº‹é¡¹:
nsys ä¸ ncu éƒ½ä¼šå¼•å…¥ä¸€å®šçš„å¼€é”€ï¼Œåˆ†æç»“æœå—å½±å“ï¼Œè¯·åœ¨åˆ†ææ—¶è€ƒè™‘æœ€ä½³åœºæ™¯ï¼ˆå¤šæ¬¡è¿è¡Œå–å¹³å‡ï¼‰ã€‚
ncu --set=full è¾“å‡ºéå¸¸å¤šæŒ‡æ ‡ï¼›å…ˆç”¨ summary / top-k kernels å†æ·±å…¥å•ä¸ª kernelã€‚
å¦‚æœç¨‹åºä½¿ç”¨å¤šä¸ªè¿›ç¨‹æˆ– CUDA MPSï¼Œä¼šå¯¹é‡‡é›†é€ æˆä¸åŒè¡¨ç°ï¼›--target-processes=all å¸®åŠ©é‡‡é›†å­è¿›ç¨‹ã€‚
å»ºè®®å°† -lineinfo ç¼–è¯‘é€‰é¡¹æ‰“å¼€ä»¥ä¾¿ ncu æŠŠæŒ‡æ ‡æ˜ å°„åˆ°æºç è¡Œï¼ˆMakefile å·²åŠ å…¥ -lineinfoï¼‰ã€‚


2. ä½¿ç”¨ nsight-systems (nsys) å‘½ä»¤è¡Œåˆ†æ:

# åŸºç¡€è¿½è¸ª:
nsys profile --duration <ç§’æ•°> ./my_app  # é™åˆ¶è¿½è¸ªæ—¶é•¿ï¼ˆé¿å…æ—¥å¿—è¿‡å¤§ï¼‰
nsys profile --duration 100 ./complex_cuda

# ç”Ÿæˆ .qdrep æŠ¥å‘Šæ–‡ä»¶ï¼Œç”Ÿæˆnsys_report.nsys-repæ–‡ä»¶:
nsys profile --output <æŠ¥å‘Šæ–‡ä»¶å> ./my_app  # ä¿å­˜ç»“æœä¸º.nsys-repæ–‡ä»¶
# ç¤ºä¾‹å‘½ä»¤ï¼ˆç”Ÿæˆ .qdrep æŠ¥å‘Šæ–‡ä»¶ï¼‰:
# åŸºç¡€é‡‡é›†å‘½ä»¤ï¼ˆå¯¹æ•´ä¸ªè¿›ç¨‹ï¼‰:
nsys profile --output=nsys_report --trace=cuda,osrt,nvtx --sample=cpu ./complex_cuda    # å¾—åˆ°åŒç›®å½•ä¸‹ï¼Œ nsys_report.nsys-rep æ–‡ä»¶ã€‚

# å¯¼å‡ºç«ç„°å›¾ / timeline è½¬æˆ HTMLï¼ˆæ›´ç›´è§‚ï¼‰:
nsys-ui nsys_report.qdrep
# æˆ–è€…å¯¼å‡º timeline: (éœ€è¦ GUI é€šå¸¸)


# å¯¼å‡ºæ¦‚è¦ï¼ˆæ–‡æœ¬ï¼‰:
nsys stats --report=summary nsys_report.nsys-rep > nsys_summary.txt      # å¾—åˆ°åŒç›®å½•ä¸‹ï¼Œ nsys_summary.txtp æ–‡ä»¶ã€‚

æ‰©å±•çŸ¥è¯†:
----------------------------------------------------
åœ¨GPUèŠ‚ç‚¹ä¸Šï¼Œå®‰è£… cuda_11.8.0_520.61.05_linux.run ï¼Œæ–‡ä»¶ä¼šå®‰è£…åœ¨ /usr/local/cuda-11.8/ ç›®å½•ä¸­ï¼Œ
åœ¨ /usr/local/cuda-11.8/nsight-compute-2022.3.0/ ä¸­ï¼Œæœ‰ ncu ncu-ui nv-nsight-cu nv-nsight-cu-cli å‘½ä»¤è¡Œå·¥å…·ã€‚
åœ¨ /usr/local/cuda-11.8/nsight-systems-2022.4.2/ ä¸­ï¼Œæœ‰ nsys nsys-ui å‘½ä»¤è¡Œå·¥å…·ã€‚

åœ¨ nvcr.io/nvidia/pytorch:23.10-py3å®¹å™¨ä¸­:
åœ¨ /opt/nvidia/nsight-compute/2023.2.2/ä¸­ï¼Œæœ‰ ncu ncu-ui å‘½ä»¤è¡Œå·¥å…·ã€‚å®¹å™¨ä¸­æ‰€å¸¦çš„å‘½ä»¤è¡Œå·¥å…·ï¼Œæ¯” run æ–‡ä»¶å®‰è£…çš„è¦å°‘ã€‚
åœ¨ /etc/alternatives/cuda-12/NsightSystems-cli-2023.3.1/ ä¸­ï¼Œæœ‰ nsys å‘½ä»¤è¡Œå·¥å…·ã€‚

åœ¨ nvcr.io/nvidia/pytorch:25.09-py3 å®¹å™¨ä¸­:
åœ¨ /opt/nvidia/nsight-compute/2025.3.1 ä¸­ï¼Œæœ‰ ncu ncu-ui å‘½ä»¤è¡Œå·¥å…·ã€‚å®¹å™¨ä¸­æ‰€å¸¦çš„å‘½ä»¤è¡Œå·¥å…·ï¼Œæ¯” run æ–‡ä»¶å®‰è£…çš„è¦å°‘ã€‚
åœ¨ /usr/local/cuda-13.0/NsightSystems-cli-2025.5.1/target-linux-x64/ ä¸­ï¼Œæœ‰ nsys å‘½ä»¤è¡Œå·¥å…·ã€‚
----------------------------------------------------



3. ä½¿ç”¨ nsight-compute (ncu) å‘½ä»¤è¡Œåˆ†æ:
é€šè¿‡ NVIDIA Nsight Compute (ncu) åˆ†æ GPU æŒ‡ä»¤æµæ•ˆç‡ï¼ˆInstruction Flow Efficiencyï¼‰ æ—¶ï¼Œå…³é”®æ˜¯ç†è§£æŒ‡ä»¤æ‰§è¡Œçš„ç“¶é¢ˆæ¥æºã€SMï¼ˆStreaming Multiprocessorï¼‰åˆ©ç”¨ç‡ä»¥åŠåˆ†æ”¯ã€è°ƒåº¦å’Œä¾èµ–å…³ç³»ç­‰å½±å“å› ç´ ã€‚

# å¯¼å‡º CSVï¼ˆåªç¤ºä¾‹å‡ ä¸ªé‡è¦ metricï¼‰:
ncu -o ncu_report_csv --csv --metrics achieved_occupancy,sm_efficiency,sm__throughput.avg.pct_of_peak_sustained_elapsed,dram_read_throughput,dram_write_throughput,inst_executed ./complex_cuda > metrics.csv
ncu -o ncu_report_csv --csv --metrics achieved_occupancy,sm_efficiency,sm__throughput.avg.pct_of_peak_sustained_elapsed,dram_read_throughput,dram_write_throughput,inst_executed ./complex_cuda_0 > metrics.csv

# å¯¼å‡º HTML:
ncu --export report.html ./complex_cuda
ncu --export report.html ./complex_cuda_0

# åŸºæœ¬æ€§èƒ½åˆ†æ
ncu -o profile --metrics smsp__cycles_active.avg.pct_of_peak_sustained ./complex_cuda
ncu -o profile --metrics smsp__cycles_active.avg.pct_of_peak_sustained ./complex_cuda_0

ncu --set=full --target-processes=all -o ncu_report ./complex_cuda
ncu --set full --target-processes all -o ncu_report ./complex_cuda

# åªé‡‡é›†æŒ‡ä»¤æµç›¸å…³æŒ‡æ ‡ï¼š
ncu --metrics sm__inst_executed.sum,sm__inst_executed_per_issue_active.avg.pct_of_peak_sustained_active,smsp__warp_issue_stalled_long_scoreboard_per_warp_active.avg.pct ./complex_cuda

# ç”Ÿæˆ .ncu-rep æ–‡ä»¶åå¯åœ¨ GUI ä¸­æ‰“å¼€ï¼š
ncu-ui report.ncu-rep



4. openmpi æµ‹è¯•åˆ†æç¨‹åº:
# openmpi å‚è€ƒæ–‡æ¡£:
https://docs.open-mpi.org/en/v5.0.x/man-openmpi/man1/mpirun.1.html#


mpirun --allow-run-as-root -np 2 --bind-to core ./complex_cuda
mpirun --allow-run-as-root -np 2 --bind-to core -H aa,aa,bb ./complex_cuda

mpirun --np 4 --report-bindings --map-by core --bind-to core --allow-run-as-root ./complex_cuda
mpirun --np 4 --report-bindings --map-by core --bind-to core --allow-run-as-root ./complex_cuda_0
mpirun --np 4 --report-bindings --map-by core --bind-to core --allow-run-as-root ./matrix_add_profiling

mpirun --allow-run-as-root --np 4 --report-bindings --map-by slot:PE=2 --use-hwthread-cpus ./matrix_add_profilin


æ‰©å±•çŸ¥è¯†:
----------------------------------------------------
åœ¨ gemini æé—®:
ç¼–å†™ä¸€ä¸ªæœ€å¤æ‚çš„ï¼Œæœ€å…¨é¢çš„CUDAç¨‹åºï¼Œè¦æ±‚å¸¦åç½®çš„çŸ©é˜µåŠ æ³• kernel å’Œæµ‹è¯•ï¼Œ
æ¯ä¸ª kernel ç”¨ NVTX åˆ†åŒºåŒ…è£¹ï¼Œæ–¹ä¾¿åˆ†æï¼Œå¯æ ¹æ®éœ€è¦æ‰©å±•æ›´å¤šæµ‹è¯•ï¼Œå¦‚ä¸åŒ streamã€ä¸åŒ block sizeã€ä¸åŒæ•°æ®è§„æ¨¡ç­‰ã€‚
å¹¶ä½¿ç”¨nsight-computeå’Œnsight-systemsçš„å‘½ä»¤è¡Œå·¥å…·åˆ†æè¿™ä¸ªç¨‹åºçš„è¿è¡Œè¿‡ç¨‹ã€‚

matrix_add_profiling.cu   # å¤§è¯­è¨€æ¨¡å‹è¾“å‡ºç¨‹åºæ–‡ä»¶

nvcc -o matrix_add_profiling matrix_add_profiling.cu -O3 -lineinfo -lnvToolsExt

# éœ€è¦å¢åŠ ç¼–è¯‘å‚æ•°:
nvcc -o matrix_add_profiling matrix_add_profiling.cu -O3 -lineinf -lnvToolsExt -gencode arch=compute_70,code=sm_70   # æŒ‡å®š GPU æ¶æ„ï¼Œä¸”æŒ‡å®šæ­£ç¡®çš„ GPU æ¶æ„

ä¸ºä»€ä¹ˆè¿™ä¸ªå‘½ä»¤æœ‰æ•ˆï¼Ÿ
-gencode arch=compute_70,code=sm_70:
arch=compute_70:å‘Šè¯‰ç¼–è¯‘å™¨ç”Ÿæˆè®¡ç®—èƒ½åŠ› 7.0 çš„ PTXï¼ˆä¸€ç§ä¸­é—´ä»£ç ï¼Œç”¨äºæœªæ¥çš„ JIT ç¼–è¯‘ï¼‰ã€‚
code=sm_70:å‘Šè¯‰ç¼–è¯‘å™¨ç›´æ¥ç”Ÿæˆè®¡ç®—èƒ½åŠ› 7.0 çš„æœ¬æœºäºŒè¿›åˆ¶ä»£ç  (SASS)ã€‚

nvcc -O3 -arch=sm_80 -lineinfo -DUSE_NVTX

----------------------------------------------------




æ‰©å±•çŸ¥è¯†:
----------------------------------------------------
docker pull nvcr.io/nvidia/pytorch:25.09-py3   # "Ubuntu 24.04.3 LTS"
docker run -it -d --gpus all --cap-add SYS_ADMIN --net=host --pid=host --ipc=host --privileged -v /Data:/Data nvcr.io/nvidia/pytorch:25.09-py3 


docker pull pytorch/manylinux2_28-builder:cuda12.9
docker pull hub.rat.dev/pytorch/manylinux2_28-builder:cuda12.9   # ä½¿ç”¨ä»£ç†æ‰èƒ½ pull ä¸‹æ¥
docker run -it -d --gpus all --cap-add SYS_ADMIN --net=host --pid=host --ipc=host --privileged -v /Data:/Data hub.rat.dev/pytorch/manylinux2_28-builder:cuda12.9

AlmaLinux release 8.10 
Linux anhua209 5.15.0-153-generic #163-Ubuntu SMP Thu Aug 7 16:37:18 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux
----------------------------------------------------



# è¿˜æœªæ•´ç†:


ResNet50

14. åˆ†å¸ƒå¼è®­ç»ƒä¼˜åŒ–ä¸å¸¸è§é—®é¢˜æ’æŸ¥ï¼ˆNCCLè°ƒä¼˜ã€AMPã€FSDPï¼‰
1. NCCLè°ƒä¼˜
NCCLåŸºç¡€é…ç½®ä¼˜åŒ–
NCCLæ€§èƒ½ç›‘æ§
2. Automatic Mixed Precision (AMP) ä¼˜åŒ–
AMPåŸºç¡€ä½¿ç”¨
AMPæ€§èƒ½ç›‘æ§
3. FSDP (Fully Sharded Data Parallel) ä¼˜åŒ–
FSDPåŸºç¡€é…ç½®
FSDPæ€§èƒ½ä¼˜åŒ–
4. å¸¸è§é—®é¢˜æ’æŸ¥
å†…å­˜ç›¸å…³é—®é¢˜

ResNet50

15. TensorRT éƒ¨ç½²ä¸ Triton é›†æˆï¼ˆåŠ é€Ÿ Stable Diffusion ç­‰ï¼‰


âŒğŸ‰âœ…ğŸ”ğŸ“„â„¹ï¸ğŸ“šğŸš€ğŸ“ŠğŸ‰ğŸšªğŸ“¦ğŸ“Š ğŸ”§ ğŸ§ªğŸ“¢ğŸ“ˆğŸ›ğŸ¡   â›µï¸


```
