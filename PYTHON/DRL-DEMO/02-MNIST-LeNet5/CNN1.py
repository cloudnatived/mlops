# æ„å»ºäº†ä¸€ä¸ªåŸºæœ¬çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¥è®­ç»ƒ MNIST æ‰‹å†™æ•°å­—è¯†åˆ«ä»»åŠ¡ï¼Œå¹¶å¸¦æœ‰è®­ç»ƒæ›²çº¿å¯è§†åŒ–ã€‚
# ä¸‹é¢æ˜¯åŸå§‹ CNN æ¨¡å‹çš„ TensorFlow 2.x / Keras é£æ ¼é‡å†™ç‰ˆæœ¬ï¼Œä¿æŒäº†åŸæœ‰ç»“æ„ï¼ˆä¸¤å±‚å·ç§¯ + Dropout + å…¨è¿æ¥ï¼‰ï¼Œæ”¯æŒå¯è§†åŒ–å‡†ç¡®ç‡å’ŒæŸå¤±æ›²çº¿ï¼Œå¹¶ä½¿ç”¨ He åˆå§‹åŒ–å’Œæ ‡å‡†åŒ–è¾“å…¥
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import tensorflow_datasets as tfds
import os

# è¶…å‚æ•°é…ç½®
INPUT_SHAPE = (28, 28, 1)           # è¾“å…¥çš„åˆ—æ•°
NUM_CLASSES = 10                    
BATCH_SIZE = 128                    # è®­ç»ƒé›†æ¯ä¸€æ‰¹æ¬¡çš„ç…§ç‰‡
MAX_EPOCHS = 50                     # è¿­ä»£çš„æ¬¡æ•°
DROPOUT_RATE = 0.15                 # Dropoutæ¯”ç‡
MODEL_DIR = "CNN1"

# æ•°æ®é¢„å¤„ç†å‡½æ•°
def normalize_img(image, label):
    image = tf.cast(image, tf.float32) / 255.0
    mean, var = tf.nn.moments(image, axes=[0, 1, 2])
    std = tf.sqrt(var)
    image = (image - mean) / (std + 1e-6)
    return tf.expand_dims(image, -1), tf.one_hot(label, NUM_CLASSES)

# æ•°æ®åŠ è½½
def load_datasets():
    (ds_train, ds_test), _ = tfds.load(
        'mnist',
        split=['train', 'test'],
        shuffle_files=True,
        as_supervised=True,
        with_info=True
    )
    ds_train = ds_train.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
    ds_train = ds_train.cache().shuffle(1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

    ds_test = ds_test.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)
    ds_test = ds_test.batch(BATCH_SIZE).cache().prefetch(tf.data.AUTOTUNE)

    return ds_train, ds_test

# æ¨¡å‹æ„å»º
def build_optimized_model():
    model = tf.keras.Sequential([
        tf.keras.layers.Input(shape=INPUT_SHAPE),   # ğŸ‘ˆ æ˜ç¡®è¾“å…¥å½¢çŠ¶

        # æ•°æ®å¢å¼ºå±‚
        tf.keras.layers.RandomRotation(0.1),
        tf.keras.layers.RandomTranslation(0.1, 0.1),
        tf.keras.layers.RandomZoom(0.1),

        # ç¬¬ä¸€å±‚å·ç§¯å—
        tf.keras.layers.Conv2D(32, kernel_size=3, activation='relu', padding='same',
                               kernel_initializer='he_normal', input_shape=INPUT_SHAPE),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.MaxPooling2D(pool_size=2),
        tf.keras.layers.Dropout(DROPOUT_RATE),

        # ç¬¬äºŒå±‚å·ç§¯å—
        tf.keras.layers.Conv2D(64, kernel_size=3, activation='relu', padding='same',
                               kernel_initializer='he_normal'),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.MaxPooling2D(pool_size=2),
        tf.keras.layers.Dropout(DROPOUT_RATE),

        # å…¨è¿æ¥å±‚
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(512, activation='relu', kernel_initializer='he_normal'),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dropout(DROPOUT_RATE+0.1),
        tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')
    ])

    # å­¦ä¹ ç‡è¡°å‡
    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
        initial_learning_rate=0.001,
        decay_steps=10000,
        decay_rate=0.9,
        staircase=True
    )

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    return model

# å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
def plot_metrics(history):
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Train Accuracy')
    plt.plot(history.history['val_accuracy'], label='Test Accuracy')
    plt.title('Accuracy Curve')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Test Loss')
    plt.title('Loss Curve')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.tight_layout()
    plt.savefig('CNN1/training_metrics.png')  # ä¿å­˜å›¾åƒ
    plt.show()

# ä¸»å‡½æ•°
def main():
    ds_train, ds_test = load_datasets()
    model = build_optimized_model()

    # å®šä¹‰å›è°ƒå‡½æ•°
    callbacks = [
        tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),
        tf.keras.callbacks.ModelCheckpoint(
            os.path.join(MODEL_DIR, 'best_model.h5'),
            save_best_only=True,
            monitor='val_accuracy',
            mode='max'
        ),
        tf.keras.callbacks.ReduceLROnPlateau(
            factor=0.5,
            patience=3,
            min_lr=1e-6,
            monitor='val_loss',
            mode='min'
        ),
        tf.keras.callbacks.TensorBoard(log_dir=os.path.join('CNN1/logs', 'mnist_cnn'))
    ]

    # ç¡®ä¿æ¨¡å‹ä¿å­˜ç›®å½•å­˜åœ¨
    if not os.path.exists(MODEL_DIR):
        os.makedirs(MODEL_DIR)

    history = model.fit(
        ds_train,
        epochs=MAX_EPOCHS,
        validation_data=ds_test,
        callbacks=callbacks,
        verbose=1
    )

    # å¯è§†åŒ–
    plot_metrics(history)

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    model.save(os.path.join(MODEL_DIR, 'final_model.keras'))
    model.save(os.path.join(MODEL_DIR, 'final_model.h5'))
    model.save(os.path.join(MODEL_DIR, 'saved_model'))

    print(f"æ¨¡å‹å·²ä¿å­˜åˆ° {MODEL_DIR}")

# å…¥å£ç‚¹
if __name__ == '__main__':
    main()
