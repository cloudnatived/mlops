#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# cifar10_utils.py
import torch
import torch.distributed as dist
from torch.utils.data import DataLoader
from torch.utils.data.distributed import DistributedSampler
from torchvision import datasets, transforms
import os

# CIFAR-10 å®˜æ–¹æ•°æ®é›† URL å’Œæ–‡ä»¶åï¼ˆç”¨äºæ£€æŸ¥ï¼‰
_CIFAR10_URL = "https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz"
_CIFAR10_FILENAME = "cifar-10-python.tar.gz"
_CIFAR10_EXTRACTED_DIR = "cifar-10-batches-py"


def setup_distributed(rank, world_size, master_addr, port="29500"):
    """åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ"""
    os.environ['MASTER_ADDR'] = master_addr
    os.environ['MASTER_PORT'] = port
    dist.init_process_group(
        backend='nccl',
        world_size=world_size,
        rank=rank
    )
    torch.cuda.set_device(rank % torch.cuda.device_count())


def cleanup():
    dist.destroy_process_group()


def _check_cifar10_manual_download(data_dir):
    """
    æ£€æŸ¥ data_dir ç›®å½•ä¸‹æ˜¯å¦å­˜åœ¨ cifar-10-python.tar.gz æ–‡ä»¶
    å¦‚æœå­˜åœ¨ï¼Œè¿”å› Trueï¼ˆè¡¨ç¤ºä¸éœ€è¦ä¸‹è½½ï¼‰
    """
    tar_path = os.path.join(data_dir, _CIFAR10_FILENAME)
    extracted_path = os.path.join(data_dir, _CIFAR10_EXTRACTED_DIR)
    # å¦‚æœ .tar.gz å­˜åœ¨ æˆ– å·²è§£å‹çš„ç›®å½•å­˜åœ¨ï¼Œéƒ½è®¤ä¸ºæ•°æ®å·²å‡†å¤‡
    return os.path.isfile(tar_path) or os.path.isdir(extracted_path)


def get_dataloaders(batch_size, data_dir='./data'):
    """
    è·å– CIFAR-10 æ•°æ®é›†
    :param batch_size: æ¯å¡ batch size
    :param data_dir: æ•°æ®é›†ä¿å­˜è·¯å¾„ï¼ˆç›¸å¯¹æˆ–ç»å¯¹è·¯å¾„ï¼‰
    """
    data_dir = os.path.abspath(data_dir)
    os.makedirs(data_dir, exist_ok=True)

    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])

    # ======== åˆ†å¸ƒå¼å®‰å…¨ï¼šæ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨ ========
    is_main_process = (dist.get_rank() == 0)
    world_size = dist.get_world_size()

    # ä¸»è¿›ç¨‹æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸‹è½½
    should_download = False
    if is_main_process:
        if not _check_cifar10_manual_download(data_dir):
            should_download = True
            print(f"ğŸ” Rank 0: {_CIFAR10_FILENAME} not found. Will download from {_CIFAR10_URL}")
        else:
            print(f"âœ… Rank 0: {_CIFAR10_FILENAME} or extracted data found. Skip download.")
    
    # å¹¿æ’­ should_download æ ‡å¿—ç»™æ‰€æœ‰è¿›ç¨‹
    should_download_tensor = torch.tensor(int(should_download), dtype=torch.uint8, device='cuda')
    dist.broadcast(should_download_tensor, src=0)
    should_download = bool(should_download_tensor.item())

    # æ‰€æœ‰è¿›ç¨‹ç­‰å¾…ä¸»è¿›ç¨‹å®Œæˆæ£€æŸ¥
    dist.barrier()

    # ======== åŠ è½½æ•°æ®é›† ========
    # æ— è®ºæ˜¯å¦ä¸‹è½½ï¼Œæ‰€æœ‰è¿›ç¨‹éƒ½ç”¨ download=should_download
    # PyTorch å†…éƒ¨ä¼šå¤„ç†ï¼šå¦‚æœå·²è§£å‹ï¼Œå³ä½¿ download=True ä¹Ÿä¸ä¼šé‡å¤ä¸‹è½½
    train_set = datasets.CIFAR10(
        root=data_dir,
        train=True,
        download=should_download,
        transform=transform_train
    )
    test_set = datasets.CIFAR10(
        root=data_dir,
        train=False,
        download=should_download,
        transform=transform_test
    )

    # å†æ¬¡åŒæ­¥ï¼Œç¡®ä¿æ‰€æœ‰è¿›ç¨‹å®Œæˆæ•°æ®åŠ è½½
    dist.barrier()

    # åˆ†å¸ƒå¼é‡‡æ ·å™¨
    train_sampler = DistributedSampler(train_set, shuffle=True)
    test_sampler = DistributedSampler(test_set, shuffle=False)

    # DataLoader
    train_loader = DataLoader(
        train_set,
        batch_size=batch_size,
        sampler=train_sampler,
        num_workers=4,
        pin_memory=True,
        persistent_workers=True
    )
    test_loader = DataLoader(
        test_set,
        batch_size=batch_size,
        sampler=test_sampler,
        num_workers=4,
        pin_memory=True,
        persistent_workers=True
    )

    return train_loader, test_loader, train_sampler
