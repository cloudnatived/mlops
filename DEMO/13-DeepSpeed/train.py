#!/usr/bin/env python3
import os
import torch
import deepspeed
import argparse
import json
import time
from tqdm import tqdm
from torch.utils.tensorboard import SummaryWriter

from models.simple_transformer import create_model
from data.dummy_dataset import create_data_loader

def setup_training():
    """è®¾ç½®è®­ç»ƒå‚æ•°"""
    parser = argparse.ArgumentParser(description='DeepSpeed Demo Training')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--vocab_size', type=int, default=50257)
    parser.add_argument('--hidden_size', type=int, default=768)
    parser.add_argument('--num_layers', type=int, default=12)
    parser.add_argument('--num_heads', type=int, default=12)
    parser.add_argument('--max_seq_length', type=int, default=1024)
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--epochs', type=int, default=10)
    parser.add_argument('--batch_size', type=int, default=8)
    parser.add_argument('--learning_rate', type=float, default=3e-4)
    parser.add_argument('--num_samples', type=int, default=10000)
    parser.add_argument('--num_workers', type=int, default=4)
    parser.add_argument('--save_interval', type=int, default=1000)
    
    # DeepSpeedå‚æ•°
    parser.add_argument('--local_rank', type=int, default=-1)
    parser.add_argument('--deepspeed_config', type=str, default='configs/ds_config.json')
    
    # åˆ†å¸ƒå¼å‚æ•°
    parser.add_argument('--master_addr', type=str, default='localhost')
    parser.add_argument('--master_port', type=str, default='29500')
    
    return parser.parse_args()

def initialize_distributed(args):
    """åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ"""
    torch.cuda.set_device(args.local_rank)
    deepspeed.init_distributed()
    
    args.world_size = torch.distributed.get_world_size()
    args.global_rank = torch.distributed.get_rank()
    
    print(f"ğŸš€ åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ: rank {args.global_rank}/{args.world_size}")
    
    # è®¾ç½®ä¸»èŠ‚ç‚¹åœ°å€å’Œç«¯å£
    os.environ['MASTER_ADDR'] = args.master_addr
    os.environ['MASTER_PORT'] = args.master_port

def train_epoch(model, dataloader, sampler, epoch, global_step, writer, args):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    sampler.set_epoch(epoch)
    
    total_loss = 0
    num_batches = len(dataloader)
    
    progress_bar = tqdm(dataloader, desc=f'Epoch {epoch}', disable=args.global_rank != 0)
    
    for batch_idx, batch in enumerate(progress_bar):
        # å°†æ•°æ®ç§»åŠ¨åˆ°GPU
        input_ids = batch['input_ids'].cuda()
        attention_mask = batch['attention_mask'].cuda()
        labels = batch['labels'].cuda()
        
        # å‰å‘ä¼ æ’­å’ŒæŸå¤±è®¡ç®—
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs['loss']
        
        # åå‘ä¼ æ’­å’Œä¼˜åŒ–
        model.backward(loss)
        model.step()
        
        # è®°å½•æŸå¤±
        total_loss += loss.item()
        
        # è®°å½•åˆ°TensorBoardï¼ˆåªåœ¨rank 0ä¸Šè®°å½•ï¼‰
        if args.global_rank == 0 and global_step % 10 == 0:
            writer.add_scalar('train/loss', loss.item(), global_step)
            writer.add_scalar('train/learning_rate', 
                             model.get_lr()[0] if hasattr(model, 'get_lr') else args.learning_rate, 
                             global_step)
        
        # æ‰“å°è®­ç»ƒä¿¡æ¯
        if global_step % 100 == 0 and args.global_rank == 0:
            print(f'Step {global_step}: Loss = {loss.item():.4f}')
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        if global_step % args.save_interval == 0 and args.global_rank == 0:
            save_checkpoint(model, global_step, args)
        
        global_step += 1
    
    avg_loss = total_loss / num_batches
    return avg_loss, global_step

def save_checkpoint(model, step, args):
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    checkpoint_dir = f'checkpoints/step_{step}'
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    model.save_checkpoint(checkpoint_dir)
    print(f"âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_dir}")

def evaluate_model(model, dataloader, args):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    total_loss = 0
    num_batches = len(dataloader)
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc='Evaluating', disable=args.global_rank != 0):
            input_ids = batch['input_ids'].cuda()
            attention_mask = batch['attention_mask'].cuda()
            labels = batch['labels'].cuda()
            
            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs['loss']
            total_loss += loss.item()
    
    avg_loss = total_loss / num_batches
    
    # åœ¨æ‰€æœ‰rankä¸ŠåŒæ­¥æŸå¤±
    if args.world_size > 1:
        avg_loss_tensor = torch.tensor(avg_loss).cuda()
        torch.distributed.all_reduce(avg_loss_tensor)
        avg_loss = avg_loss_tensor.item() / args.world_size
    
    return avg_loss

def main():
    # è®¾ç½®å‚æ•°
    args = setup_training()
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ
    initialize_distributed(args)
    
    # åªåœ¨rank 0ä¸Šåˆ›å»ºTensorBoard writer
    if args.global_rank == 0:
        writer = SummaryWriter('runs/deepspeed_demo')
        print("ğŸ“Š TensorBoardæ—¥å¿—ç›®å½•: runs/deepspeed_demo")
    else:
        writer = None
    
    # åˆ›å»ºæ¨¡å‹
    model = create_model(args)
    
    # åŠ è½½DeepSpeedé…ç½®
    with open(args.deepspeed_config, 'r') as f:
        ds_config = json.load(f)
    
    # åˆå§‹åŒ–DeepSpeed
    model_engine, optimizer, _, _ = deepspeed.initialize(
        args=args,
        model=model,
        model_parameters=model.parameters(),
        config=ds_config
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataloader, sampler = create_data_loader(args, args.global_rank, args.world_size)
    
    print(f"ğŸ¯ å¼€å§‹è®­ç»ƒ: {args.world_size}ä¸ªGPU, {len(dataloader)}ä¸ªæ‰¹æ¬¡/epoch")
    
    # è®­ç»ƒå¾ªç¯
    global_step = 0
    for epoch in range(args.epochs):
        if args.global_rank == 0:
            print(f"\nğŸ“ˆ Epoch {epoch+1}/{args.epochs}")
        
        # è®­ç»ƒä¸€ä¸ªepoch
        start_time = time.time()
        avg_loss, global_step = train_epoch(
            model_engine, dataloader, sampler, epoch, global_step, writer, args
        )
        epoch_time = time.time() - start_time
        
        # è¯„ä¼°æ¨¡å‹
        eval_loss = evaluate_model(model_engine, dataloader, args)
        
        # æ‰“å°è®­ç»ƒç»“æœï¼ˆåªåœ¨rank 0ä¸Šï¼‰
        if args.global_rank == 0:
            print(f"âœ… Epoch {epoch+1} å®Œæˆ:")
            print(f"   è®­ç»ƒæŸå¤±: {avg_loss:.4f}")
            print(f"   è¯„ä¼°æŸå¤±: {eval_loss:.4f}")
            print(f"   æ—¶é—´: {epoch_time:.2f}ç§’")
            print(f"   å…¨å±€æ­¥æ•°: {global_step}")
            
            # è®°å½•åˆ°TensorBoard
            if writer:
                writer.add_scalar('train/epoch_loss', avg_loss, epoch)
                writer.add_scalar('eval/loss', eval_loss, epoch)
                writer.add_scalar('train/epoch_time', epoch_time, epoch)
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    if args.global_rank == 0:
        save_checkpoint(model_engine, global_step, args)
        print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    
    # æ¸…ç†
    if writer:
        writer.close()

if __name__ == "__main__":
    main()
